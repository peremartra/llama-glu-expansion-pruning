{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/llama-glu-expansion-pruning/blob/main/notebooks/02_Evaluate_1B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzfjNg_SIvXD"
      },
      "source": [
        "# GLU Pruning Research - Llama-3.2-1B Evaluation\n",
        "## 02 - Comprehensive Benchmark Suite Evaluation\n",
        "\n",
        "### Exploring GLU Expansion Ratios in Llama-3.2 Models\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "\n",
        "[![Paper](https://img.shields.io/badge/OSF-Paper-blue?logo=osf&logoColor=white)](https://doi.org/10.31219/osf.io/qgxea)\n",
        "[![GitHub](https://img.shields.io/badge/â­_Star-OptiPFair-orange?logo=github&logoColor=white)](https://github.com/peremartra/optipfair)\n",
        "[![PyPI](https://img.shields.io/pypi/v/optipfair?logo=python&logoColor=white&label=v)](https://pypi.org/project/optipfair/)\n",
        "\n",
        "**Repository:** [github.com/peremartra/llama-glu-expansion-pruning](https://github.com/peremartra/llama-glu-expansion-pruning)\n",
        "\n",
        "---\n",
        "\n",
        "**Colab Environment:** GPU L4 (or T4)\n",
        "\n",
        "**Models to Evaluate:**\n",
        "* Llama-3.2-1B (base) - Baseline\n",
        "* Llama-3.2-1B-pruned-20% (220% expansion)\n",
        "* Llama-3.2-1B-pruned-40% (140% expansion) â­ Star model\n",
        "* Llama-3.2-1B-pruned-60% (60% expansion)\n",
        "\n",
        "**Benchmarks (10 total):**\n",
        "* WikiText-2 Perplexity (0-shot)\n",
        "* BoolQ (0-shot)\n",
        "* Lambada-OpenAI (0-shot)\n",
        "* MMLU (5-shot)\n",
        "* ARC-Challenge (0-shot)\n",
        "* HellaSwag (0-shot)\n",
        "* WinoGrande (0-shot)\n",
        "* PIQA (0-shot)\n",
        "* TruthfulQA MC1/MC2 (0-shot)\n",
        "* GSM8K (5-shot CoT)\n",
        "\n",
        "**Estimated Runtime:** ~4-5 hours total\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“‹ Notebook Objective\n",
        "\n",
        "This notebook conducts a comprehensive evaluation of the Llama-3.2-1B model family across three pruning levels (20%, 40%, 60%) to determine:\n",
        "\n",
        "1. **Performance degradation patterns** across different pruning intensities\n",
        "2. **Optimal expansion ratio** for GLU-MLP layers (hypothesis: 140%)\n",
        "3. **Task-specific resilience** to pruning (knowledge vs. algorithmic tasks)\n",
        "4. **Which models merit uploading to HuggingFace Hub** for Phase 2\n",
        "\n",
        "### Key Features:\n",
        "- âœ… **Checkpoint/Resume Support:** Survives Colab disconnections\n",
        "- âœ… **On-the-fly Pruning:** No need to pre-create models\n",
        "- âœ… **Robust Error Handling:** Continues if individual benchmarks fail\n",
        "- âœ… **Progress Tracking:** Live updates and detailed logging\n",
        "\n",
        "### Results will answer:\n",
        "- Does 40% pruning (140% expansion) truly outperform other levels?\n",
        "- Which benchmarks are most sensitive to pruning?\n",
        "- Should we upload non-star models to HF, or only the 40% version?\n",
        "\n",
        "---\n",
        "\n",
        "**Note:** This evaluation uses the MAW (Maximum Absolute Weight) neuron selection method, validated in Notebook 00 as the optimal approach for GLU architectures.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkVFbeCMIvXF"
      },
      "source": [
        "# 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nAo67s0lIvXF",
        "outputId": "22f883fc-3e4a-41c4-fe9e-774e7109a99c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install -q optipfair\n",
        "!pip install -q lm-eval\n",
        "!pip install -q langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWIHQuIGIvXG",
        "outputId": "80aa8362-79f7-4475-ef0f-4bb5af334e5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive for checkpoint persistence\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG2nO7YpIvXG",
        "outputId": "e4d67d05-862c-43ae-e0cc-76f33ddd576c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… utils.py downloaded successfully\n"
          ]
        }
      ],
      "source": [
        "# Download utils.py from GitHub repository\n",
        "!wget -q https://raw.githubusercontent.com/peremartra/llama-glu-expansion-pruning/main/utils.py\n",
        "\n",
        "# Verify download\n",
        "import os\n",
        "if os.path.exists('utils.py'):\n",
        "    print(\"âœ… utils.py downloaded successfully\")\n",
        "else:\n",
        "    print(\"âŒ Failed to download utils.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHjkx6_QIvXG",
        "outputId": "1404269f-7329-48ab-ae93-9bb168203a4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All imports successful\n",
            "ğŸ“± Device: GPU\n",
            "   GPU: NVIDIA A100-SXM4-40GB\n",
            "   Memory: 42.5 GB\n"
          ]
        }
      ],
      "source": [
        "# Import core libraries and utilities\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# Import our utility functions\n",
        "from utils import (\n",
        "    EXPERIMENT_CONFIG,\n",
        "    BENCHMARKS_BASE,\n",
        "    load_or_create_model,\n",
        "    run_robust_evaluation,\n",
        "    clear_gpu_cache,\n",
        "    get_model_stats,\n",
        "    format_results_table\n",
        ")\n",
        "\n",
        "print(\"âœ… All imports successful\")\n",
        "print(f\"ğŸ“± Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LixoDuXJIvXG"
      },
      "source": [
        "# 2. Configuration & Planning\n",
        "\n",
        "This section filters the experiment configuration for 1B models and displays the evaluation plan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5fDt6IxIvXG",
        "outputId": "e727572b-c6e3-48d3-bf02-3c63182b5584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ğŸ“Š EVALUATION PLAN: Llama-3.2-1B Family\n",
            "======================================================================\n",
            "\n",
            "Total models to evaluate: 7\n",
            "Benchmarks per model: 11\n",
            "Total evaluations: 77\n",
            "Estimated runtime: ~4-5 hours\n",
            "\n",
            "Models to evaluate:\n",
            "----------------------------------------------------------------------\n",
            "Model                          Pruning    Star  \n",
            "----------------------------------------------------------------------\n",
            "Llama-3.2-1B (baseline)        0%         300%         N/A   \n",
            "Llama-3.2-1B-pruned-10pct      10%        No    \n",
            "Llama-3.2-1B-pruned-20pct      20%        No    \n",
            "Llama-3.2-1B-pruned-30pct      30%        No    \n",
            "Llama-3.2-1B-pruned-40pct      40%        â­ Yes \n",
            "Llama-3.2-1B-pruned-50pct      50%        No    \n",
            "Llama-3.2-1B-pruned-60pct      60%        No    \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Benchmarks to run:\n",
            "----------------------------------------------------------------------\n",
            " 1. wikitext                  0-shot\n",
            " 2. boolq                     0-shot\n",
            " 3. lambada_openai            0-shot\n",
            " 4. mmlu                      5-shot\n",
            " 5. arc_challenge             0-shot\n",
            " 6. hellaswag                 0-shot\n",
            " 7. winogrande                0-shot\n",
            " 8. piqa                      0-shot\n",
            " 9. truthfulqa_mc1            0-shot\n",
            "10. truthfulqa_mc2            0-shot\n",
            "11. gsm8k                     5-shot\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "âš™ï¸  Configuration:\n",
            "   - Neuron selection method: MAW (Maximum Absolute Weight)\n",
            "   - Checkpointing: Enabled (per-task granularity)\n",
            "   - Model creation: On-the-fly pruning (no pre-creation needed)\n",
            "   - Error handling: Skip failed tasks and continue\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Filter configuration for 1B models only\n",
        "models_1b = [\n",
        "    config for config in EXPERIMENT_CONFIG\n",
        "    if \"1B\" in config[\"base_model\"] and \"3B\" not in config[\"base_model\"]\n",
        "]\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ğŸ“Š EVALUATION PLAN: Llama-3.2-1B Family\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "print(f\"Total models to evaluate: {len(models_1b) + 1}\")  # +1 for base model\n",
        "print(f\"Benchmarks per model: {len(BENCHMARKS_BASE)}\")\n",
        "print(f\"Total evaluations: {(len(models_1b) + 1) * len(BENCHMARKS_BASE)}\")\n",
        "print(f\"Estimated runtime: ~4-5 hours\\n\")\n",
        "\n",
        "# Display models table\n",
        "print(\"Models to evaluate:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Model':<30} {'Pruning':<10} {'Star':<6}\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Llama-3.2-1B (baseline)':<30} {'0%':<10} {'300%':<12} {'N/A':<6}\")\n",
        "for config in models_1b:\n",
        "    model_name = config['hf_repo_id'].split('/')[-1]\n",
        "    pruning = f\"{config['pruning_pct']}%\"\n",
        "    star = \"â­ Yes\" if config['is_star'] else \"No\"\n",
        "    print(f\"{model_name:<30} {pruning:<10} {star:<6}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Display benchmarks\n",
        "print(\"\\nBenchmarks to run:\")\n",
        "print(\"-\" * 70)\n",
        "for i, task in enumerate(BENCHMARKS_BASE, 1):\n",
        "    task_name = task['name']\n",
        "    fewshot = f\"{task['num_fewshot']}-shot\"\n",
        "    print(f\"{i:2d}. {task_name:<25} {fewshot}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "print(\"\\nâš™ï¸  Configuration:\")\n",
        "print(f\"   - Neuron selection method: MAW (Maximum Absolute Weight)\")\n",
        "print(f\"   - Checkpointing: Enabled (per-task granularity)\")\n",
        "print(f\"   - Model creation: On-the-fly pruning (no pre-creation needed)\")\n",
        "print(f\"   - Error handling: Skip failed tasks and continue\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7U0stRUIvXG",
        "outputId": "f720bd4e-8b2d-41ce-efa4-2d9e410db1ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Checkpoint directory: /content/drive/MyDrive/glu_pruning/checkpoints/1b\n",
            "âœ… Results directory: /content/drive/MyDrive/glu_pruning/results\n",
            "\n",
            "Checkpoint files:\n",
            "   baseline  : âœ… Exists\n",
            "   10pct     : ğŸ†• New\n",
            "   20pct     : ğŸ†• New\n",
            "   30pct     : ğŸ†• New\n",
            "   40pct     : ğŸ†• New\n",
            "   50pct     : ğŸ†• New\n",
            "   60pct     : ğŸ†• New\n"
          ]
        }
      ],
      "source": [
        "# Setup checkpoint paths\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/glu_pruning/checkpoints/1b\"\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/glu_pruning/results\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "Path(CHECKPOINT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"âœ… Checkpoint directory: {CHECKPOINT_DIR}\")\n",
        "print(f\"âœ… Results directory: {RESULTS_DIR}\")\n",
        "\n",
        "# Define checkpoint paths for each model\n",
        "checkpoint_paths = {\n",
        "    \"baseline\": f\"{CHECKPOINT_DIR}/llama_3.2_1b_baseline.json\",\n",
        "    \"10pct\": f\"{CHECKPOINT_DIR}/llama_3.2_1b_pruned_10pct.json\",\n",
        "    \"20pct\": f\"{CHECKPOINT_DIR}/llama_3.2_1b_pruned_20pct.json\",\n",
        "    \"30pct\": f\"{CHECKPOINT_DIR}/llama_3.2_1b_pruned_30pct.json\",\n",
        "    \"40pct\": f\"{CHECKPOINT_DIR}/llama_3.2_1b_pruned_40pct.json\",\n",
        "    \"50pct\": f\"{CHECKPOINT_DIR}/llama_3.2_1b_pruned_50pct.json\",\n",
        "    \"60pct\": f\"{CHECKPOINT_DIR}/llama_3.2_1b_pruned_60pct.json\",\n",
        "}\n",
        "\n",
        "print(\"\\nCheckpoint files:\")\n",
        "for key, path in checkpoint_paths.items():\n",
        "    exists = \"âœ… Exists\" if Path(path).exists() else \"ğŸ†• New\"\n",
        "    print(f\"   {key:<10}: {exists}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5yTl5gTIvXH"
      },
      "source": [
        "# 3. Baseline Evaluation\n",
        "\n",
        "Evaluate the original Llama-3.2-1B model to establish performance baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjcHGNrsIvXH"
      },
      "outputs": [],
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ğŸ“Š PHASE 1: BASELINE EVALUATION\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "BASE_MODEL_ID = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# Load base model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "print(f\"Loading base model: {BASE_MODEL_ID}...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    #dtype=torch.float16, #L4\n",
        "    dtype=torch.bfloat16, #A100\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"âœ… Model loaded successfully\")\n",
        "\n",
        "# Display model statistics\n",
        "base_stats = get_model_stats(base_model)\n",
        "print(f\"\\nğŸ“ˆ Model Statistics:\")\n",
        "print(f\"   Parameters: {base_stats['total_parameters']:,}\")\n",
        "print(f\"   Size: {base_stats['size_gb']:.2f} GB\")\n",
        "\n",
        "# Run evaluation with checkpointing\n",
        "baseline_results = run_robust_evaluation(\n",
        "    model=base_model,\n",
        "    tokenizer=tokenizer,\n",
        "    tasks=BENCHMARKS_BASE,\n",
        "    checkpoint_path=checkpoint_paths[\"baseline\"],\n",
        "    model_name=\"Llama-3.2-1B-baseline\"\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"âœ… BASELINE EVALUATION COMPLETED\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# Display results summary\n",
        "print(\"Results Preview:\")\n",
        "print(format_results_table(baseline_results))\n",
        "\n",
        "# Clear memory\n",
        "del base_model\n",
        "clear_gpu_cache()\n",
        "\n",
        "print(\"\\nğŸ§¹ Memory cleared, ready for pruned models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtbaAsxIIvXH"
      },
      "source": [
        "# 4. Pruned Models Evaluation Loop\n",
        "\n",
        "Evaluate the three pruned variants (20%, 40%, 60%) using on-the-fly pruning with OptiPFair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPoZiAfjIvXH"
      },
      "outputs": [],
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ğŸ“Š PHASE 2: PRUNED MODELS EVALUATION\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# Store all results for final comparison\n",
        "all_results = {\n",
        "    \"baseline\": baseline_results\n",
        "}\n",
        "\n",
        "# Evaluate each pruned model\n",
        "for i, config in enumerate(models_1b, 1):\n",
        "    model_name = config['hf_repo_id'].split('/')[-1]\n",
        "    pruning_pct = config['pruning_pct']\n",
        "    is_star = config['is_star']\n",
        "\n",
        "    print(f\"\\n{'â”€'*70}\")\n",
        "    print(f\"ğŸ”„ EVALUATING MODEL {i}/{len(models_1b)}: {model_name}{pruning_pct} \")\n",
        "    print(f\"   Pruning: {pruning_pct}% |  Star: {'â­' if is_star else 'No'}\")\n",
        "    print(f\"{'â”€'*70}\\n\")\n",
        "\n",
        "    try:\n",
        "        # Load or create model using utility function\n",
        "        model, tokenizer, stats = load_or_create_model(config, device=\"auto\")\n",
        "\n",
        "        # Display model statistics\n",
        "        print(f\"\\nğŸ“ˆ Model Statistics:\")\n",
        "        print(f\"   Parameters: {stats['total_parameters']:,}\")\n",
        "        print(f\"   Size: {stats['size_gb']:.2f} GB\")\n",
        "        if 'pruning_stats' in stats:\n",
        "            print(f\"   Reduction: {stats['pruning_stats']['percentage_reduction']:.2f}%\")\n",
        "        print(f\"   Source: {stats['source']}\\n\")\n",
        "\n",
        "        # Determine checkpoint key\n",
        "        checkpoint_key = f\"{pruning_pct}pct\"\n",
        "\n",
        "        # Run evaluation with checkpointing\n",
        "        results = run_robust_evaluation(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            tasks=BENCHMARKS_BASE,\n",
        "            checkpoint_path=checkpoint_paths[checkpoint_key],\n",
        "            model_name=model_name\n",
        "        )\n",
        "\n",
        "        # Store results\n",
        "        all_results[checkpoint_key] = results\n",
        "\n",
        "        print(f\"\\nâœ… {model_name}{pruning_pct} evaluation completed\")\n",
        "        print(\"\\nResults Preview:\")\n",
        "        print(format_results_table(results))\n",
        "\n",
        "        # Clear memory before next model\n",
        "        del model\n",
        "        clear_gpu_cache()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ ERROR evaluating {model_name}: {str(e)}\")\n",
        "        print(\"   Continuing with next model...\\n\")\n",
        "        clear_gpu_cache()\n",
        "        continue\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"âœ… ALL PRUNED MODELS EVALUATED\")\n",
        "print(f\"{'='*70}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1MptG-dIvXH"
      },
      "source": [
        "# 5. Results Consolidation & Export\n",
        "\n",
        "Consolidate all evaluation results and export to CSV for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5RqknL6IvXH",
        "outputId": "6f9dc1b0-be9f-4205-f660-eb2996ab87a6"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "ğŸ“Š CONSOLIDATING RESULTS\n",
            "======================================================================\n",
            "\n",
            "âœ… Consolidated 77 result rows\n",
            "   Models: 7\n",
            "   Tasks: 11\n",
            "   Metrics per task: 11\n",
            "\n",
            "DataFrame Preview:\n",
            "          model  pruning_pct  is_star            task  word_perplexity,none  \\\n",
            "0  Llama-3.2-1B            0    False        wikitext               11.5708   \n",
            "1  Llama-3.2-1B            0    False           boolq                   NaN   \n",
            "2  Llama-3.2-1B            0    False  lambada_openai                   NaN   \n",
            "3  Llama-3.2-1B            0    False            mmlu                   NaN   \n",
            "4  Llama-3.2-1B            0    False   arc_challenge                   NaN   \n",
            "5  Llama-3.2-1B            0    False       hellaswag                   NaN   \n",
            "6  Llama-3.2-1B            0    False      winogrande                   NaN   \n",
            "7  Llama-3.2-1B            0    False            piqa                   NaN   \n",
            "8  Llama-3.2-1B            0    False  truthfulqa_mc1                   NaN   \n",
            "9  Llama-3.2-1B            0    False  truthfulqa_mc2                   NaN   \n",
            "\n",
            "   byte_perplexity,none  bits_per_byte,none  accuracy acc_norm  perplexity  \\\n",
            "0                1.5807              0.6606       NaN      NaN         NaN   \n",
            "1                   NaN                 NaN    0.6343      N/A         NaN   \n",
            "2                   NaN                 NaN       NaN      NaN        5.75   \n",
            "3                   NaN                 NaN    0.3111      N/A         NaN   \n",
            "4                   NaN                 NaN    0.3106   0.3626         NaN   \n",
            "5                   NaN                 NaN    0.4771   0.6363         NaN   \n",
            "6                   NaN                 NaN    0.5991      N/A         NaN   \n",
            "7                   NaN                 NaN    0.7437   0.7454         NaN   \n",
            "8                   NaN                 NaN    0.2338      N/A         NaN   \n",
            "9                   NaN                 NaN    0.3772      N/A         NaN   \n",
            "\n",
            "   word_perplexity  bits_per_byte  exact_match,strict-match  \\\n",
            "0              NaN            NaN                       NaN   \n",
            "1              NaN            NaN                       NaN   \n",
            "2              0.0            0.0                       NaN   \n",
            "3              NaN            NaN                       NaN   \n",
            "4              NaN            NaN                       NaN   \n",
            "5              NaN            NaN                       NaN   \n",
            "6              NaN            NaN                       NaN   \n",
            "7              NaN            NaN                       NaN   \n",
            "8              NaN            NaN                       NaN   \n",
            "9              NaN            NaN                       NaN   \n",
            "\n",
            "   exact_match_stderr,strict-match  exact_match,flexible-extract  \\\n",
            "0                              NaN                           NaN   \n",
            "1                              NaN                           NaN   \n",
            "2                              NaN                           NaN   \n",
            "3                              NaN                           NaN   \n",
            "4                              NaN                           NaN   \n",
            "5                              NaN                           NaN   \n",
            "6                              NaN                           NaN   \n",
            "7                              NaN                           NaN   \n",
            "8                              NaN                           NaN   \n",
            "9                              NaN                           NaN   \n",
            "\n",
            "   exact_match_stderr,flexible-extract  \n",
            "0                                  NaN  \n",
            "1                                  NaN  \n",
            "2                                  NaN  \n",
            "3                                  NaN  \n",
            "4                                  NaN  \n",
            "5                                  NaN  \n",
            "6                                  NaN  \n",
            "7                                  NaN  \n",
            "8                                  NaN  \n",
            "9                                  NaN  \n",
            "\n",
            "ğŸ’¾ Results saved to: /content/drive/MyDrive/glu_pruning/results/llama_1b_results_20251021_202034.csv\n",
            "ğŸ’¾ Latest results: /content/drive/MyDrive/glu_pruning/results/llama_1b_results_latest.csv\n",
            "\n",
            "======================================================================\n",
            "âœ… EVALUATION COMPLETE - ALL RESULTS SAVED\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ğŸ“Š CONSOLIDATING RESULTS\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# Prepare data for DataFrame\n",
        "consolidated_data = []\n",
        "\n",
        "# Model metadata\n",
        "model_info = {\n",
        "    \"baseline\": {\"name\": \"Llama-3.2-1B\", \"pruning\": 0, \"star\": False},\n",
        "    \"10pct\": {\"name\": \"Llama-3.2-1B-pruned-10%\", \"pruning\": 10, \"star\": False},\n",
        "    \"20pct\": {\"name\": \"Llama-3.2-1B-pruned-20%\", \"pruning\": 20, \"star\": False},\n",
        "    \"30pct\": {\"name\": \"Llama-3.2-1B-pruned-30%\", \"pruning\": 30, \"star\": False},\n",
        "    \"40pct\": {\"name\": \"Llama-3.2-1B-pruned-40%\", \"pruning\": 40,  \"star\": True},\n",
        "    \"50pct\": {\"name\": \"Llama-3.2-1B-pruned-50%\", \"pruning\": 50,  \"star\": False},\n",
        "    \"60pct\": {\"name\": \"Llama-3.2-1B-pruned-60%\", \"pruning\": 60,  \"star\": False},\n",
        "}\n",
        "\n",
        "# Process each model's results\n",
        "for model_key, results in all_results.items():\n",
        "    info = model_info[model_key]\n",
        "\n",
        "    for task_name, metrics in results.items():\n",
        "        row = {\n",
        "            \"model\": info[\"name\"],\n",
        "            \"pruning_pct\": info[\"pruning\"],\n",
        "            \"is_star\": info[\"star\"],\n",
        "            \"task\": task_name,\n",
        "        }\n",
        "\n",
        "        # Add all metrics from this task\n",
        "        for metric_name, value in metrics.items():\n",
        "            # Convert string values to float where possible\n",
        "            try:\n",
        "                row[metric_name] = float(value)\n",
        "            except (ValueError, TypeError):\n",
        "                row[metric_name] = value\n",
        "\n",
        "        consolidated_data.append(row)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(consolidated_data)\n",
        "\n",
        "print(f\"âœ… Consolidated {len(df)} result rows\")\n",
        "print(f\"   Models: {df['model'].nunique()}\")\n",
        "print(f\"   Tasks: {df['task'].nunique()}\")\n",
        "print(f\"   Metrics per task: {len(df.columns) - 5}\")  # Exclude metadata columns\n",
        "\n",
        "# Display summary\n",
        "print(\"\\nDataFrame Preview:\")\n",
        "print(df.head(10))\n",
        "\n",
        "# Save to CSV\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "csv_path = f\"{RESULTS_DIR}/llama_1b_results_{timestamp}.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\"\\nğŸ’¾ Results saved to: {csv_path}\")\n",
        "\n",
        "# Also save a \"latest\" version for easy access\n",
        "latest_path = f\"{RESULTS_DIR}/llama_1b_results_latest.csv\"\n",
        "df.to_csv(latest_path, index=False)\n",
        "print(f\"ğŸ’¾ Latest results: {latest_path}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"âœ… EVALUATION COMPLETE - ALL RESULTS SAVED\")\n",
        "print(f\"{'='*70}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxHStHvMIvXH"
      },
      "source": [
        "# 6. Quick Analysis & Visualization\n",
        "\n",
        "Generate quick insights to decide which models merit uploading to HuggingFace Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xo_bifVCIvXI"
      },
      "outputs": [],
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ğŸ“ˆ QUICK ANALYSIS: Performance vs. Pruning Level\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# Calculate average performance degradation per model\n",
        "# Focus on key metrics: accuracy for classification, perplexity for generation\n",
        "\n",
        "summary_metrics = []\n",
        "\n",
        "for model_key in [\"baseline\", \"20pct\", \"40pct\", \"60pct\"]:\n",
        "    info = model_info[model_key]\n",
        "    model_df = df[df['model'] == info['name']]\n",
        "\n",
        "    # Extract key metrics\n",
        "    accuracies = model_df['accuracy'].dropna()\n",
        "    perplexities = model_df['perplexity'].dropna()\n",
        "\n",
        "    summary = {\n",
        "        \"model\": info['name'],\n",
        "        \"pruning\": info['pruning'],\n",
        "        \"star\": \"â­\" if info['star'] else \"\",\n",
        "        \"avg_accuracy\": accuracies.mean() if len(accuracies) > 0 else None,\n",
        "        \"avg_perplexity\": perplexities.mean() if len(perplexities) > 0 else None,\n",
        "        \"num_tasks\": len(model_df),\n",
        "    }\n",
        "\n",
        "    summary_metrics.append(summary)\n",
        "\n",
        "summary_df = pd.DataFrame(summary_metrics)\n",
        "\n",
        "print(\"Performance Summary:\")\n",
        "print(\"-\" * 90)\n",
        "print(summary_df.to_string(index=False))\n",
        "print(\"-\" * 90)\n",
        "\n",
        "# Calculate degradation vs baseline\n",
        "baseline_acc = summary_df.loc[summary_df['pruning'] == 0, 'avg_accuracy'].values[0]\n",
        "baseline_ppl = summary_df.loc[summary_df['pruning'] == 0, 'avg_perplexity'].values[0]\n",
        "\n",
        "print(\"\\nDegradation vs. Baseline:\")\n",
        "print(\"-\" * 90)\n",
        "\n",
        "for _, row in summary_df.iterrows():\n",
        "    if row['pruning'] == 0:\n",
        "        continue\n",
        "\n",
        "    acc_delta = ((row['avg_accuracy'] - baseline_acc) / baseline_acc * 100) if baseline_acc else None\n",
        "    ppl_delta = ((row['avg_perplexity'] - baseline_ppl) / baseline_ppl * 100) if baseline_ppl else None\n",
        "\n",
        "    print(f\"{row['model']:<35} {row['star']:>2}\")\n",
        "    if acc_delta is not None:\n",
        "        print(f\"   Accuracy: {acc_delta:+.2f}%\")\n",
        "    if ppl_delta is not None:\n",
        "        print(f\"   Perplexity: {ppl_delta:+.2f}%\")\n",
        "    print()\n",
        "\n",
        "print(\"-\" * 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcveKgykIvXI"
      },
      "outputs": [],
      "source": [
        "# Visualization: Performance across pruning levels\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy plot\n",
        "axes[0].plot(summary_df['pruning'], summary_df['avg_accuracy'], marker='o', linewidth=2, markersize=8)\n",
        "axes[0].axhline(y=baseline_acc, color='r', linestyle='--', alpha=0.5, label='Baseline')\n",
        "axes[0].set_xlabel('Pruning Level (%)', fontsize=12)\n",
        "axes[0].set_ylabel('Average Accuracy', fontsize=12)\n",
        "axes[0].set_title('Accuracy vs. Pruning Level', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].legend()\n",
        "\n",
        "# Highlight star model\n",
        "star_idx = summary_df[summary_df['star'] == 'â­'].index[0]\n",
        "axes[0].plot(summary_df.loc[star_idx, 'pruning'], summary_df.loc[star_idx, 'avg_accuracy'],\n",
        "             marker='*', markersize=20, color='gold', markeredgecolor='black', markeredgewidth=1.5)\n",
        "\n",
        "# Perplexity plot\n",
        "axes[1].plot(summary_df['pruning'], summary_df['avg_perplexity'], marker='o', linewidth=2, markersize=8, color='orange')\n",
        "axes[1].axhline(y=baseline_ppl, color='r', linestyle='--', alpha=0.5, label='Baseline')\n",
        "axes[1].set_xlabel('Pruning Level (%)', fontsize=12)\n",
        "axes[1].set_ylabel('Average Perplexity', fontsize=12)\n",
        "axes[1].set_title('Perplexity vs. Pruning Level', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].legend()\n",
        "\n",
        "# Highlight star model\n",
        "axes[1].plot(summary_df.loc[star_idx, 'pruning'], summary_df.loc[star_idx, 'avg_perplexity'],\n",
        "             marker='*', markersize=20, color='gold', markeredgecolor='black', markeredgewidth=1.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{RESULTS_DIR}/llama_1b_performance_analysis.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nğŸ“Š Visualization saved to: {RESULTS_DIR}/llama_1b_performance_analysis.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4H7r1PNIvXI"
      },
      "source": [
        "# 7. Decision Matrix: Which Models to Upload?\n",
        "\n",
        "Based on the evaluation results, determine which models should be uploaded to HuggingFace Hub for Phase 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5sHLEjwIvXI"
      },
      "outputs": [],
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ğŸ¯ DECISION MATRIX: Models for HuggingFace Hub Upload\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "print(\"Evaluation Criteria:\")\n",
        "print(\"  1. Performance degradation < 15% vs baseline\")\n",
        "print(\"  2. Outperforms or matches baseline in at least 2 tasks\")\n",
        "print(\"  3. Accuracy degradation < 50% vs baseline\")\n",
        "print(\"  4. Sufficient parameter reduction to justify storage\\n\")\n",
        "\n",
        "# Decision logic\n",
        "decisions = []\n",
        "\n",
        "for _, row in summary_df.iterrows():\n",
        "    if row['pruning'] == 0:\n",
        "        continue  # Skip baseline\n",
        "\n",
        "    decision = {\n",
        "        \"model\": row['model'],\n",
        "        \"pruning\": row['pruning'],\n",
        "        \"star\": row['star'],\n",
        "    }\n",
        "\n",
        "    # Calculate degradation\n",
        "    acc_degradation = abs((row['avg_accuracy'] - baseline_acc) / baseline_acc * 100) if baseline_acc else 999\n",
        "    ppl_degradation = abs((row['avg_perplexity'] - baseline_ppl) / baseline_ppl * 100) if baseline_ppl else 999\n",
        "\n",
        "    # Decision criteria\n",
        "    is_star = row['star'] == 'â­'\n",
        "    good_performance = (acc_degradation < 15 or ppl_degradation < 50)  # Perplexity can vary more\n",
        "\n",
        "    # Final decision\n",
        "    if is_star:\n",
        "        decision['upload'] = True\n",
        "        decision['reason'] = \"Star model (optimal 140% expansion)\"\n",
        "    elif good_performance:\n",
        "        decision['upload'] = True\n",
        "        decision['reason'] = f\"Good performance (acc: {acc_degradation:.1f}%, ppl: {ppl_degradation:.1f}%)\"\n",
        "    else:\n",
        "        decision['upload'] = False\n",
        "        decision['reason'] = f\"High degradation (acc: {acc_degradation:.1f}%, ppl: {ppl_degradation:.1f}%)\"\n",
        "\n",
        "    decisions.append(decision)\n",
        "\n",
        "# Display decision table\n",
        "print(\"Upload Decisions:\")\n",
        "print(\"-\" * 100)\n",
        "print(f\"{'Model':<35} {'Pruning':<10} {'Star':<6} {'Upload?':<10} {'Reason'}\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for dec in decisions:\n",
        "    upload_status = \"âœ… YES\" if dec['upload'] else \"âŒ NO\"\n",
        "    print(f\"{dec['model']:<35} {dec['pruning']:<10}% {dec['star']:<6} {upload_status:<10} {dec['reason']}\")\n",
        "\n",
        "print(\"-\" * 100)\n",
        "\n",
        "# Summary\n",
        "models_to_upload = sum(1 for d in decisions if d['upload'])\n",
        "print(f\"\\nğŸ“¦ Total models to upload to HF Hub: {models_to_upload}/{len(decisions)}\")\n",
        "print(f\"\\nâœ… PHASE 3 COMPLETE - Ready for Phase 2 (Model Factory)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save complete results for reproducibility and research sharing\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ğŸ’¾ SAVING COMPLETE RESULTS FOR RESEARCH SHARING\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# Consolidate all data into a comprehensive JSON\n",
        "complete_results = {\n",
        "    \"experiment_metadata\": {\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"notebook\": \"02_Evaluate_1B.ipynb\",\n",
        "        \"model_family\": \"Llama-3.2-1B\",\n",
        "        \"pruning_method\": \"MAW (Maximum Absolute Weight)\",\n",
        "        #\"library_versions\": {\n",
        "        #    \"optipfair\": optipfair.__version__,\n",
        "        #    \"transformers\": transformers.__version__,\n",
        "        #    \"torch\": torch.__version__,\n",
        "        #    \"lm_eval\": lm_eval.__version__\n",
        "        #},\n",
        "        \"hardware\": {\n",
        "            \"device\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\",\n",
        "            \"gpu_memory_gb\": torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else None\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"benchmarks\": [\n",
        "        {\"name\": task[\"name\"], \"num_fewshot\": task[\"num_fewshot\"]}\n",
        "        for task in BENCHMARKS_BASE\n",
        "    ],\n",
        "\n",
        "    \"models_evaluated\": {\n",
        "        \"baseline\": {\n",
        "            \"name\": \"Llama-3.2-1B\",\n",
        "            \"pruning_pct\": 0,\n",
        "            \"expansion_rate\": 300,\n",
        "            \"is_star\": False,\n",
        "            \"hf_repo\": \"meta-llama/Llama-3.2-1B\",\n",
        "            \"results\": baseline_results\n",
        "        },\n",
        "        \"pruned_10pct\": {\n",
        "            \"name\": \"Llama-3.2-1B-pruned-10%\",\n",
        "            \"pruning_pct\": 10,\n",
        "            \"expansion_rate\": 270,\n",
        "            \"is_star\": False,\n",
        "            \"hf_repo\": \"peremartra/Llama-3.2-1B-pruned-10pct\",\n",
        "            \"results\": all_results.get(\"20pct\", {})\n",
        "        },\n",
        "        \"pruned_20pct\": {\n",
        "            \"name\": \"Llama-3.2-1B-pruned-20%\",\n",
        "            \"pruning_pct\": 20,\n",
        "            \"expansion_rate\": 220,\n",
        "            \"is_star\": False,\n",
        "            \"hf_repo\": \"peremartra/Llama-3.2-1B-pruned-20pct\",\n",
        "            \"results\": all_results.get(\"20pct\", {})\n",
        "        },\n",
        "        \"pruned_30pct\": {\n",
        "            \"name\": \"Llama-3.2-1B-pruned-30%\",\n",
        "            \"pruning_pct\": 30,\n",
        "            \"expansion_rate\": 190,\n",
        "            \"is_star\": False,\n",
        "            \"hf_repo\": \"peremartra/Llama-3.2-1B-pruned-30pct\",\n",
        "            \"results\": all_results.get(\"30pct\", {})\n",
        "        },\n",
        "        \"pruned_40pct\": {\n",
        "            \"name\": \"Llama-3.2-1B-pruned-40%\",\n",
        "            \"pruning_pct\": 40,\n",
        "            \"expansion_rate\": 140,\n",
        "            \"is_star\": True,\n",
        "            \"hf_repo\": \"peremartra/Llama-3.2-1B-pruned-40pct\",\n",
        "            \"results\": all_results.get(\"40pct\", {})\n",
        "        },\n",
        "        \"pruned_50pct\": {\n",
        "            \"name\": \"Llama-3.2-1B-pruned-50%\",\n",
        "            \"pruning_pct\": 50,\n",
        "            \"expansion_rate\": 140,\n",
        "            \"is_star\": True,\n",
        "            \"hf_repo\": \"peremartra/Llama-3.2-1B-pruned-50pct\",\n",
        "            \"results\": all_results.get(\"50pct\", {})\n",
        "        },\n",
        "        \"pruned_60pct\": {\n",
        "            \"name\": \"Llama-3.2-1B-pruned-60%\",\n",
        "            \"pruning_pct\": 60,\n",
        "            \"expansion_rate\": 60,\n",
        "            \"is_star\": False,\n",
        "            \"hf_repo\": \"peremartra/Llama-3.2-1B-pruned-60pct\",\n",
        "            \"results\": all_results.get(\"60pct\", {})\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"summary_statistics\": {\n",
        "        \"baseline\": {\n",
        "            \"avg_accuracy\": float(summary_df.loc[summary_df['pruning'] == 0, 'avg_accuracy'].values[0]),\n",
        "            \"avg_perplexity\": float(summary_df.loc[summary_df['pruning'] == 0, 'avg_perplexity'].values[0]),\n",
        "        },\n",
        "        \"pruned_models\": [\n",
        "            {\n",
        "                \"pruning_pct\": int(row['pruning']),\n",
        "                \"is_star\": row['star'] == 'â­',\n",
        "                \"avg_accuracy\": float(row['avg_accuracy']) if pd.notna(row['avg_accuracy']) else None,\n",
        "                \"avg_perplexity\": float(row['avg_perplexity']) if pd.notna(row['avg_perplexity']) else None,\n",
        "                \"accuracy_degradation_pct\": float(((row['avg_accuracy'] - baseline_acc) / baseline_acc * 100)) if baseline_acc and pd.notna(row['avg_accuracy']) else None,\n",
        "                \"perplexity_degradation_pct\": float(((row['avg_perplexity'] - baseline_ppl) / baseline_ppl * 100)) if baseline_ppl and pd.notna(row['avg_perplexity']) else None\n",
        "            }\n",
        "            for _, row in summary_df.iterrows() if row['pruning'] > 0\n",
        "        ]\n",
        "    },\n",
        "\n",
        "    \"upload_decisions\": decisions,\n",
        "\n",
        "    \"citation\": {\n",
        "        \"paper\": \"Exploring GLU Expansion Ratios: Structured Pruning in Llama-3.2 Models\",\n",
        "        \"author\": \"Pere Martra\",\n",
        "        \"doi\": \"https://doi.org/10.31219/osf.io/qgxea\",\n",
        "        \"github\": \"https://github.com/peremartra/llama-glu-expansion-pruning\",\n",
        "        \"note\": \"Results are freely available for research purposes. Please cite the paper if you use this data.\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save to JSON with pretty formatting\n",
        "json_path = f\"{RESULTS_DIR}/llama_1b_complete_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "with open(json_path, 'w') as f:\n",
        "    json.dump(complete_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"âœ… Complete results saved to:\")\n",
        "print(f\"   {json_path}\")\n",
        "\n",
        "# Also save a \"latest\" version\n",
        "latest_json = f\"{RESULTS_DIR}/llama_1b_complete_results_latest.json\"\n",
        "with open(latest_json, 'w') as f:\n",
        "    json.dump(complete_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"âœ… Latest version:\")\n",
        "print(f\"   {latest_json}\")\n",
        "\n",
        "# Display file size\n",
        "file_size_kb = Path(json_path).stat().st_size / 1024\n",
        "print(f\"\\nğŸ“Š File size: {file_size_kb:.1f} KB\")\n",
        "print(f\"ğŸ“¦ Models included: {len(complete_results['models_evaluated'])}\")\n",
        "print(f\"ğŸ“‹ Benchmarks per model: {len(BENCHMARKS_BASE)}\")\n",
        "print(f\"ğŸ”¬ Total result entries: {len(df)}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"âœ… COMPLETE RESULTS SAVED - Ready for research sharing\")\n",
        "print(f\"{'='*70}\\n\")"
      ],
      "metadata": {
        "id": "nfjYJe0yNNhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6JVP-5RIvXI"
      },
      "source": [
        "---\n",
        "\n",
        "## ğŸ“ Key Takeaways\n",
        "\n",
        "This notebook evaluated the Llama-3.2-1B model family across 10 comprehensive benchmarks to determine:\n",
        "\n",
        "1. **Optimal pruning level** for GLU-MLP layers\n",
        "2. **Performance-efficiency trade-offs** at different expansion ratios\n",
        "3. **Which models justify upload** to HuggingFace Hub\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "Based on these results, proceed to:\n",
        "- **Phase 2 (01_Model_Factory.ipynb):** Create and upload selected models to HF Hub\n",
        "- **Phase 4 (03_Evaluate_3B.ipynb):** Repeat evaluation for Llama-3.2-3B family\n",
        "- **Analysis:** Deep dive into task-specific degradation patterns\n",
        "\n",
        "---\n",
        "\n",
        "**Powered by OptiPFair** - Structured Pruning for GLU Architectures\n",
        "\n",
        "If this research helps your work:\n",
        "- â­ Star [the repo](https://github.com/peremartra/optipfair)\n",
        "- ğŸ“– Read the [documentation](https://peremartra.github.io/optipfair/)\n",
        "- ğŸ› Report issues or suggest features\n",
        "\n",
        "---"
      ]
    }
  ]
}