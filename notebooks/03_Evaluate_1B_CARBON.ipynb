{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7572cbca1dd643c1afda8f6bbf8f838e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da75f2064b4c4e48aa1fd09b22986145",
              "IPY_MODEL_8b3f7bbc6b2347518c2db3f2de45e51b",
              "IPY_MODEL_5ed04aaa326640c2ba70c78e5bc15b58"
            ],
            "layout": "IPY_MODEL_99883832aaea4ed79d391c567037e548"
          }
        },
        "da75f2064b4c4e48aa1fd09b22986145": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fb62010772a4c04911f67c07d3ebd37",
            "placeholder": "​",
            "style": "IPY_MODEL_28d11591b8e2430da79765edfe5b4d3f",
            "value": "config.json: 100%"
          }
        },
        "8b3f7bbc6b2347518c2db3f2de45e51b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b92fc6903fc447090851c3acc26adb9",
            "max": 843,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3268d59e9a640bcbd73813d713c1fa2",
            "value": 843
          }
        },
        "5ed04aaa326640c2ba70c78e5bc15b58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dabdf6343df7482d9bd2560877e5eb99",
            "placeholder": "​",
            "style": "IPY_MODEL_df8341dcc2de4185b3ef2b669b23c468",
            "value": " 843/843 [00:00&lt;00:00, 115kB/s]"
          }
        },
        "99883832aaea4ed79d391c567037e548": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fb62010772a4c04911f67c07d3ebd37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28d11591b8e2430da79765edfe5b4d3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b92fc6903fc447090851c3acc26adb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3268d59e9a640bcbd73813d713c1fa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dabdf6343df7482d9bd2560877e5eb99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df8341dcc2de4185b3ef2b669b23c468": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34755551a8254f3bb1c5dab866c2d972": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5667b43c0fa949d2981eaeb2fa538535",
              "IPY_MODEL_6e13a351aad64e42b1e9513015762d22",
              "IPY_MODEL_59aae802b4464f88a420cd6f03e3f888"
            ],
            "layout": "IPY_MODEL_56dabeab1b7146eaada7a6b7bba0bf8b"
          }
        },
        "5667b43c0fa949d2981eaeb2fa538535": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1457e2b14d949a6b7d27899038b7690",
            "placeholder": "​",
            "style": "IPY_MODEL_05c8d29035c64286870d00033dbb77f5",
            "value": "model.safetensors: 100%"
          }
        },
        "6e13a351aad64e42b1e9513015762d22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5af671fba1c40688eb5d506d6d196d1",
            "max": 2471645608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_356432fbb90f40579ff1f64c1947a849",
            "value": 2471645608
          }
        },
        "59aae802b4464f88a420cd6f03e3f888": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e758a10fdde54aeca56f4e66542f44f1",
            "placeholder": "​",
            "style": "IPY_MODEL_91dd52f93e4b46bdb6a2adc3c96cc19f",
            "value": " 2.47G/2.47G [00:06&lt;00:00, 250MB/s]"
          }
        },
        "56dabeab1b7146eaada7a6b7bba0bf8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1457e2b14d949a6b7d27899038b7690": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05c8d29035c64286870d00033dbb77f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5af671fba1c40688eb5d506d6d196d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "356432fbb90f40579ff1f64c1947a849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e758a10fdde54aeca56f4e66542f44f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91dd52f93e4b46bdb6a2adc3c96cc19f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c446f05931bc43cc84964ad98eeecaa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8343d8a458a4d4792e947201233d77a",
              "IPY_MODEL_6bafdfd579c0474c8e6a2f2ab75438a4",
              "IPY_MODEL_a1ee751cd7f044c78b2015ae5d290e22"
            ],
            "layout": "IPY_MODEL_5794eb99344c4384bc2b445cf6108bdf"
          }
        },
        "a8343d8a458a4d4792e947201233d77a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea1062ad73954f05952d2cdca248bdf0",
            "placeholder": "​",
            "style": "IPY_MODEL_0dfd5124f0d34477bf0d03b9b41da54e",
            "value": "generation_config.json: 100%"
          }
        },
        "6bafdfd579c0474c8e6a2f2ab75438a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6917bddd0adb4a9fbe61a7b28e8de452",
            "max": 185,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9cf5d7e42c674c9aa307131bca7a4e93",
            "value": 185
          }
        },
        "a1ee751cd7f044c78b2015ae5d290e22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff509d98a1434b7abc816e708146080a",
            "placeholder": "​",
            "style": "IPY_MODEL_5a18c17ef0404cfca3b3a6f81191c311",
            "value": " 185/185 [00:00&lt;00:00, 25.8kB/s]"
          }
        },
        "5794eb99344c4384bc2b445cf6108bdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea1062ad73954f05952d2cdca248bdf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dfd5124f0d34477bf0d03b9b41da54e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6917bddd0adb4a9fbe61a7b28e8de452": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cf5d7e42c674c9aa307131bca7a4e93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff509d98a1434b7abc816e708146080a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a18c17ef0404cfca3b3a6f81191c311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6b67a0b419b4ce7b4109fdf2d2ecf00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4da59f50558443d7a78d187664dbdc93",
              "IPY_MODEL_9b85cdfc09854c43b75776e131133d40",
              "IPY_MODEL_046d49f3da1143aca83383cf1ceffb98"
            ],
            "layout": "IPY_MODEL_5feada8a95c841c994ebe327db7214d7"
          }
        },
        "4da59f50558443d7a78d187664dbdc93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c98fe6255d44c85a0010ccc5d9a9f6f",
            "placeholder": "​",
            "style": "IPY_MODEL_b89bcec14d364aa990393cc9578d9d0f",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "9b85cdfc09854c43b75776e131133d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8680ffe770f45e299fc72739eb1dbe0",
            "max": 50500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_491be75b46884828a871e0404f9ab0ed",
            "value": 50500
          }
        },
        "046d49f3da1143aca83383cf1ceffb98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc9e55fb09be4c95b7954da408fa9148",
            "placeholder": "​",
            "style": "IPY_MODEL_8627d8504e18473e9a7f823a8c5adee9",
            "value": " 50.5k/50.5k [00:00&lt;00:00, 4.69MB/s]"
          }
        },
        "5feada8a95c841c994ebe327db7214d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c98fe6255d44c85a0010ccc5d9a9f6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b89bcec14d364aa990393cc9578d9d0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8680ffe770f45e299fc72739eb1dbe0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "491be75b46884828a871e0404f9ab0ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc9e55fb09be4c95b7954da408fa9148": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8627d8504e18473e9a7f823a8c5adee9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4942fda5a72040e69ff85e155e7e17a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5b392e3125140b09e215cf8afcda315",
              "IPY_MODEL_1eaff9638e4845488fc14f2b70be37f5",
              "IPY_MODEL_4006e445f1ab4c0e994c62fe66745fa8"
            ],
            "layout": "IPY_MODEL_0723154f46574b5abc6ba6c87476e5b8"
          }
        },
        "e5b392e3125140b09e215cf8afcda315": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aaf1b32cb2134014bb8c100ec50e46cc",
            "placeholder": "​",
            "style": "IPY_MODEL_db11b5384c194ea4b2a8fef6507c4da4",
            "value": "tokenizer.json: 100%"
          }
        },
        "1eaff9638e4845488fc14f2b70be37f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b335faeeefe4451a1e2d3e4ee81c888",
            "max": 9085657,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58241ee9ac5849aa8dbc57e0a68a8b43",
            "value": 9085657
          }
        },
        "4006e445f1ab4c0e994c62fe66745fa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62a8f18ceaec4b419464fccc3adff426",
            "placeholder": "​",
            "style": "IPY_MODEL_933dab48ac7943d4952d5c23fae1b411",
            "value": " 9.09M/9.09M [00:00&lt;00:00, 21.8MB/s]"
          }
        },
        "0723154f46574b5abc6ba6c87476e5b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaf1b32cb2134014bb8c100ec50e46cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db11b5384c194ea4b2a8fef6507c4da4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b335faeeefe4451a1e2d3e4ee81c888": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58241ee9ac5849aa8dbc57e0a68a8b43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62a8f18ceaec4b419464fccc3adff426": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "933dab48ac7943d4952d5c23fae1b411": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "986ad92dff2b4c9fb7730f896e7d61b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6685dcc7cc6496497ac15e9c0b0a62b",
              "IPY_MODEL_4fdc70e77f474d208a57e78813377f0d",
              "IPY_MODEL_83fbe92fa2dd4042a109cdaaeeb419d3"
            ],
            "layout": "IPY_MODEL_f27d255e766d4488b5165078530da727"
          }
        },
        "e6685dcc7cc6496497ac15e9c0b0a62b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ea217d80741479db132853acb4a8ba7",
            "placeholder": "​",
            "style": "IPY_MODEL_05eac3137841498bb347161f9c086dbe",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "4fdc70e77f474d208a57e78813377f0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_030886d9eda94fbd94f978679da0fa71",
            "max": 301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54767050e0d64c88b550a7a4a934b16d",
            "value": 301
          }
        },
        "83fbe92fa2dd4042a109cdaaeeb419d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea4005eb7c4647188a509dae334eb873",
            "placeholder": "​",
            "style": "IPY_MODEL_016e05ffc8754c75b15e7d18597b7cd9",
            "value": " 301/301 [00:00&lt;00:00, 42.5kB/s]"
          }
        },
        "f27d255e766d4488b5165078530da727": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ea217d80741479db132853acb4a8ba7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05eac3137841498bb347161f9c086dbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "030886d9eda94fbd94f978679da0fa71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54767050e0d64c88b550a7a4a934b16d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ea4005eb7c4647188a509dae334eb873": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "016e05ffc8754c75b15e7d18597b7cd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc1bbf4bc4104fada66bd1f1e61dbcde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1773fda29eca4017bbaf0eb1014854ea",
              "IPY_MODEL_b564f74df0cd4cfe957daa2a86ea1a7e",
              "IPY_MODEL_530d3b8c90a4484f8af3fffbd98cdca7"
            ],
            "layout": "IPY_MODEL_5cc56b2ad8114c9198a59b96b59d92ae"
          }
        },
        "1773fda29eca4017bbaf0eb1014854ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33c6c1d432a548c9838ea5f214248985",
            "placeholder": "​",
            "style": "IPY_MODEL_4138cb1046c341248efde776ec5b9b11",
            "value": "README.md: "
          }
        },
        "b564f74df0cd4cfe957daa2a86ea1a7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f184832292642c283aa02419a64fc8c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f1b6ba75d364224b9644cfd56e464bd",
            "value": 1
          }
        },
        "530d3b8c90a4484f8af3fffbd98cdca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a92e3e255be84519a09a1bb105b68d19",
            "placeholder": "​",
            "style": "IPY_MODEL_1310decfd6244865a3808bd34fdcaab1",
            "value": " 7.02k/? [00:00&lt;00:00, 823kB/s]"
          }
        },
        "5cc56b2ad8114c9198a59b96b59d92ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33c6c1d432a548c9838ea5f214248985": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4138cb1046c341248efde776ec5b9b11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f184832292642c283aa02419a64fc8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "4f1b6ba75d364224b9644cfd56e464bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a92e3e255be84519a09a1bb105b68d19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1310decfd6244865a3808bd34fdcaab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9a8c4475d3446409dd6ae355a255a2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb2a559c2f5a40caa68f80a5032711d7",
              "IPY_MODEL_c37cf3426e084b1a82ec247e41384ef4",
              "IPY_MODEL_b89ab8504ed1476c92e3d9205e70f49f"
            ],
            "layout": "IPY_MODEL_3a5d366737214fb8b6a46b083b87bff1"
          }
        },
        "eb2a559c2f5a40caa68f80a5032711d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ea256230e524dea994a5715c78bfa11",
            "placeholder": "​",
            "style": "IPY_MODEL_bce76b023f1d4512a5af0ad750da4a63",
            "value": "data/train-00000-of-00001.parquet: 100%"
          }
        },
        "c37cf3426e084b1a82ec247e41384ef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9599c4e379584c8e85d7072a51465659",
            "max": 24365524,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7148228d1f96407482b2a01e446d835e",
            "value": 24365524
          }
        },
        "b89ab8504ed1476c92e3d9205e70f49f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc12251e970144d0b3cd6bafbbe2b2ee",
            "placeholder": "​",
            "style": "IPY_MODEL_47ed2276b02a484eb1016fbae3b20eb5",
            "value": " 24.4M/24.4M [00:00&lt;00:00, 45.7MB/s]"
          }
        },
        "3a5d366737214fb8b6a46b083b87bff1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ea256230e524dea994a5715c78bfa11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bce76b023f1d4512a5af0ad750da4a63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9599c4e379584c8e85d7072a51465659": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7148228d1f96407482b2a01e446d835e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc12251e970144d0b3cd6bafbbe2b2ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47ed2276b02a484eb1016fbae3b20eb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38b78e6f79c14689bfe8c422ef954437": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3eca9def6cb4fa59ae7322a7da766df",
              "IPY_MODEL_22dcfd61b5b14e36a479a6a655c4f785",
              "IPY_MODEL_8905263b5b2d49528b057d84e72c9ee7"
            ],
            "layout": "IPY_MODEL_19cd908ba7f0449786f708534e794ed4"
          }
        },
        "f3eca9def6cb4fa59ae7322a7da766df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d28b08512244409acfb342d594a4bc7",
            "placeholder": "​",
            "style": "IPY_MODEL_8da17ca3955a40f38687b36f76a8e332",
            "value": "data/test-00000-of-00001.parquet: 100%"
          }
        },
        "22dcfd61b5b14e36a479a6a655c4f785": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58c66e9d481f4ae2b92a9cb6aadaf4f8",
            "max": 6112397,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cdb8e822102a4dfba793629ed329fca8",
            "value": 6112397
          }
        },
        "8905263b5b2d49528b057d84e72c9ee7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bca502122ca14decaaf2f3bf7af5f66d",
            "placeholder": "​",
            "style": "IPY_MODEL_b07212e20db14a9fa3d5812cd225ad0d",
            "value": " 6.11M/6.11M [00:00&lt;00:00, 23.7MB/s]"
          }
        },
        "19cd908ba7f0449786f708534e794ed4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d28b08512244409acfb342d594a4bc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8da17ca3955a40f38687b36f76a8e332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58c66e9d481f4ae2b92a9cb6aadaf4f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdb8e822102a4dfba793629ed329fca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bca502122ca14decaaf2f3bf7af5f66d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b07212e20db14a9fa3d5812cd225ad0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "749d9ce021fc428590fbf49ee2caa512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9113ccab573943e9ad0be9730ea5a968",
              "IPY_MODEL_b60b1d633d9c41a9a3e4f640cb1b0074",
              "IPY_MODEL_9b2372eea8db4d2796b6c6c9c3853c85"
            ],
            "layout": "IPY_MODEL_76ade0615b59461e9e60d957abbd2905"
          }
        },
        "9113ccab573943e9ad0be9730ea5a968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd01cd776aef4350b7b730fd47f233f0",
            "placeholder": "​",
            "style": "IPY_MODEL_38698557502546e593eedcbcf5e1747b",
            "value": "data/validation-00000-of-00001.parquet: 100%"
          }
        },
        "b60b1d633d9c41a9a3e4f640cb1b0074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c056bbc5f1a46489be322fcd9239e5c",
            "max": 6315951,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b55840af4729443097812cafe46f90bf",
            "value": 6315951
          }
        },
        "9b2372eea8db4d2796b6c6c9c3853c85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5a24bf0fdea4c3bafb1949a930ba2e7",
            "placeholder": "​",
            "style": "IPY_MODEL_5c3622bda34a4ab4a98c04e11af0faa5",
            "value": " 6.32M/6.32M [00:00&lt;00:00, 20.2MB/s]"
          }
        },
        "76ade0615b59461e9e60d957abbd2905": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd01cd776aef4350b7b730fd47f233f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38698557502546e593eedcbcf5e1747b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c056bbc5f1a46489be322fcd9239e5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b55840af4729443097812cafe46f90bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5a24bf0fdea4c3bafb1949a930ba2e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c3622bda34a4ab4a98c04e11af0faa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce4ad91d9e144ecba36c6ea720302a3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6cd1f911903545b6960a61c3ecf699ad",
              "IPY_MODEL_d5f793a240c2400fbf285380a9c655f5",
              "IPY_MODEL_9fc51b2a505548e090a009dede635810"
            ],
            "layout": "IPY_MODEL_68b63e407e2041ba9d3114570b507445"
          }
        },
        "6cd1f911903545b6960a61c3ecf699ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1006c5ddab34d4f9de2332d5d6543fc",
            "placeholder": "​",
            "style": "IPY_MODEL_9530285040154011bb0af77185e6c5a4",
            "value": "Generating train split: 100%"
          }
        },
        "d5f793a240c2400fbf285380a9c655f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c53ef5cc87d4b5dac4fe064878c32c6",
            "max": 39905,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15d0543678d149bda56438929a5a51de",
            "value": 39905
          }
        },
        "9fc51b2a505548e090a009dede635810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c392ef1910c4965b8a3edcc48f5cda6",
            "placeholder": "​",
            "style": "IPY_MODEL_c98434109c544163a9d268625e908d12",
            "value": " 39905/39905 [00:00&lt;00:00, 209362.92 examples/s]"
          }
        },
        "68b63e407e2041ba9d3114570b507445": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1006c5ddab34d4f9de2332d5d6543fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9530285040154011bb0af77185e6c5a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c53ef5cc87d4b5dac4fe064878c32c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15d0543678d149bda56438929a5a51de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c392ef1910c4965b8a3edcc48f5cda6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c98434109c544163a9d268625e908d12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efd62f7415dc41e298b7e5f7a079a21f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab519d33a32644deb4a7f15b94a5b22b",
              "IPY_MODEL_3109439121fa42f789fa042fa49f4eee",
              "IPY_MODEL_f195830f12544aadb8a2f28d936b7697"
            ],
            "layout": "IPY_MODEL_c98c233370894efaaf3a533133222233"
          }
        },
        "ab519d33a32644deb4a7f15b94a5b22b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_141d1afd6916486087fafd779d9fd697",
            "placeholder": "​",
            "style": "IPY_MODEL_8baf8c63c1ee457aa685cc5dbd5d6a0b",
            "value": "Generating test split: 100%"
          }
        },
        "3109439121fa42f789fa042fa49f4eee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_017ab8e5fe664a9db569dcae688404c2",
            "max": 10003,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_84e36f110bd64016bd2edd40a15ed51b",
            "value": 10003
          }
        },
        "f195830f12544aadb8a2f28d936b7697": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b5a4bb2d3984a5d9b706111ab82ea4f",
            "placeholder": "​",
            "style": "IPY_MODEL_d4651befbf6f44af9231aeb3c2d06e6a",
            "value": " 10003/10003 [00:00&lt;00:00, 248102.51 examples/s]"
          }
        },
        "c98c233370894efaaf3a533133222233": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "141d1afd6916486087fafd779d9fd697": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8baf8c63c1ee457aa685cc5dbd5d6a0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "017ab8e5fe664a9db569dcae688404c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84e36f110bd64016bd2edd40a15ed51b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b5a4bb2d3984a5d9b706111ab82ea4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4651befbf6f44af9231aeb3c2d06e6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3528c78e3934f188ae6396a35007270": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8fa9abd886a54cd3a8b8a4fc0bf4f10d",
              "IPY_MODEL_546cb5de8b1044148c15c051740cb960",
              "IPY_MODEL_816476359e99474081b9cc3338626cd4"
            ],
            "layout": "IPY_MODEL_97648046e3d34af3a98950bfbd2538af"
          }
        },
        "8fa9abd886a54cd3a8b8a4fc0bf4f10d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9eb7d9fd93ac4153b7167af69b37e571",
            "placeholder": "​",
            "style": "IPY_MODEL_115a0b8055a14cf1a8e6c1d8147d2c6c",
            "value": "Generating validation split: 100%"
          }
        },
        "546cb5de8b1044148c15c051740cb960": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3b86132f82e4f42b76d2df64581c61b",
            "max": 10042,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aea5094dbb5440b7a72db351c1651086",
            "value": 10042
          }
        },
        "816476359e99474081b9cc3338626cd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de2376283aae451f9c937a0a548aef16",
            "placeholder": "​",
            "style": "IPY_MODEL_9a8085ea570f402aa52f45c94dce8c4c",
            "value": " 10042/10042 [00:00&lt;00:00, 254359.90 examples/s]"
          }
        },
        "97648046e3d34af3a98950bfbd2538af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eb7d9fd93ac4153b7167af69b37e571": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "115a0b8055a14cf1a8e6c1d8147d2c6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3b86132f82e4f42b76d2df64581c61b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aea5094dbb5440b7a72db351c1651086": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de2376283aae451f9c937a0a548aef16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a8085ea570f402aa52f45c94dce8c4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7995f176e2c24de184e639e289761571": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d1fac0ab71a4adfaa4b3fe8609bf6da",
              "IPY_MODEL_d3c98d07f5104d1593c5a82eca1045bb",
              "IPY_MODEL_19294e6b36694f5a8cf5367118068425"
            ],
            "layout": "IPY_MODEL_fb1da3e16e3546f68265260245f9631a"
          }
        },
        "8d1fac0ab71a4adfaa4b3fe8609bf6da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7296fb1104a64a028ac104856cbe4e7a",
            "placeholder": "​",
            "style": "IPY_MODEL_19e64565d4e74e8f8fb2c89c0b9976c0",
            "value": "README.md: "
          }
        },
        "d3c98d07f5104d1593c5a82eca1045bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_454c731a56b44da78326e15d6507318b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_712a1ae5b1104b9db6059d5eb977df59",
            "value": 1
          }
        },
        "19294e6b36694f5a8cf5367118068425": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a1f8ae233f340f990067c9198f0cc82",
            "placeholder": "​",
            "style": "IPY_MODEL_7cd75f65dd1246be80143b2537b78dd0",
            "value": " 53.2k/? [00:00&lt;00:00, 6.40MB/s]"
          }
        },
        "fb1da3e16e3546f68265260245f9631a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7296fb1104a64a028ac104856cbe4e7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19e64565d4e74e8f8fb2c89c0b9976c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "454c731a56b44da78326e15d6507318b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "712a1ae5b1104b9db6059d5eb977df59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a1f8ae233f340f990067c9198f0cc82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cd75f65dd1246be80143b2537b78dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe429a8a161a49e6ad0055e54d844881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90ef2655e07840f7b557fa33d4b66c7a",
              "IPY_MODEL_52c03f70d5f84033a86b6f40fc3cba59",
              "IPY_MODEL_3d0d047bbbdc4da9a780fda678d18248"
            ],
            "layout": "IPY_MODEL_147fbcaaab8d4f76b1216bd22ac1b6b0"
          }
        },
        "90ef2655e07840f7b557fa33d4b66c7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0e336493b83497f976486e36a1a0a5e",
            "placeholder": "​",
            "style": "IPY_MODEL_49c2030496c943b686eaa8baa3a046ac",
            "value": "dataset_infos.json: "
          }
        },
        "52c03f70d5f84033a86b6f40fc3cba59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_581e62a2767142ff96dca9db7df7d8f6",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f17792c0b42432383a5f229fc5fe6e3",
            "value": 1
          }
        },
        "3d0d047bbbdc4da9a780fda678d18248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46c93d59cc1a4a6d9196f9bb357b6dbb",
            "placeholder": "​",
            "style": "IPY_MODEL_ed02714f0fca464e92e4419d88b13eb7",
            "value": " 138k/? [00:00&lt;00:00, 14.7MB/s]"
          }
        },
        "147fbcaaab8d4f76b1216bd22ac1b6b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0e336493b83497f976486e36a1a0a5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49c2030496c943b686eaa8baa3a046ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "581e62a2767142ff96dca9db7df7d8f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "0f17792c0b42432383a5f229fc5fe6e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "46c93d59cc1a4a6d9196f9bb357b6dbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed02714f0fca464e92e4419d88b13eb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0642b17afcef417a88a3d069fa9b787e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c4291ddfeb6f42b69bc3406320ed473f",
              "IPY_MODEL_5de38ce401d14297892ef313cdc03f37",
              "IPY_MODEL_9403290437ec4ca3bcd7be88534b5778"
            ],
            "layout": "IPY_MODEL_73d8a9bc099242d291c51374a0c686f1"
          }
        },
        "c4291ddfeb6f42b69bc3406320ed473f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b0b10e7feef4b6f8ad3f9e6f5e7ef05",
            "placeholder": "​",
            "style": "IPY_MODEL_a2b843acf9e9492ca354dd26e6355bae",
            "value": "all/test-00000-of-00001.parquet: 100%"
          }
        },
        "5de38ce401d14297892ef313cdc03f37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aec133099a8d435987850939c940654e",
            "max": 3504718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d56ab87d1ad42dc825d7278eabcc368",
            "value": 3504718
          }
        },
        "9403290437ec4ca3bcd7be88534b5778": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51eaed4b63494f8abc3904ace91c90d2",
            "placeholder": "​",
            "style": "IPY_MODEL_ff6751d7df3e4d35aba4f5ae20cbb148",
            "value": " 3.50M/3.50M [00:00&lt;00:00, 9.62MB/s]"
          }
        },
        "73d8a9bc099242d291c51374a0c686f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b0b10e7feef4b6f8ad3f9e6f5e7ef05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2b843acf9e9492ca354dd26e6355bae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aec133099a8d435987850939c940654e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d56ab87d1ad42dc825d7278eabcc368": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "51eaed4b63494f8abc3904ace91c90d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff6751d7df3e4d35aba4f5ae20cbb148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce45a57520794f9a90a8a2ef1d340f46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61d8ed7d062747649fbc273cf6a86ae4",
              "IPY_MODEL_d6a49d28858c4e71843d8f620f0139ae",
              "IPY_MODEL_a15ffa674e6c446db0bf5cf3fbf489a3"
            ],
            "layout": "IPY_MODEL_48ab3f3909b241788066d5f60d63c778"
          }
        },
        "61d8ed7d062747649fbc273cf6a86ae4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2112d9fee48433ab6539b49fea0dca1",
            "placeholder": "​",
            "style": "IPY_MODEL_6e7d0637c4bb439faaad62308aa81169",
            "value": "all/validation-00000-of-00001.parquet: 100%"
          }
        },
        "d6a49d28858c4e71843d8f620f0139ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3045f05b927e41cc8707575bc5008491",
            "max": 408449,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d571c24b435a4bf7a9178a3dcf8c4664",
            "value": 408449
          }
        },
        "a15ffa674e6c446db0bf5cf3fbf489a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1884a3c69f8c460abdcc1449f1b80b6b",
            "placeholder": "​",
            "style": "IPY_MODEL_5044c192ccd445868450fd2dcbd87382",
            "value": " 408k/408k [00:00&lt;00:00, 1.77MB/s]"
          }
        },
        "48ab3f3909b241788066d5f60d63c778": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2112d9fee48433ab6539b49fea0dca1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e7d0637c4bb439faaad62308aa81169": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3045f05b927e41cc8707575bc5008491": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d571c24b435a4bf7a9178a3dcf8c4664": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1884a3c69f8c460abdcc1449f1b80b6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5044c192ccd445868450fd2dcbd87382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f30d2b5fb20547d983d672dd968b15ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d374c6e6a5fd4820b4eb0a62464e3990",
              "IPY_MODEL_c8852e030dad4458aafa2772eaf3c58e",
              "IPY_MODEL_a240cc77d68a4fd9b424e9abe66c722e"
            ],
            "layout": "IPY_MODEL_1856c62b959b422abf49a7419cf73ced"
          }
        },
        "d374c6e6a5fd4820b4eb0a62464e3990": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c9de7ba411b47af88351a353e775c51",
            "placeholder": "​",
            "style": "IPY_MODEL_876a8c12927443759d880736bd20860e",
            "value": "all/dev-00000-of-00001.parquet: 100%"
          }
        },
        "c8852e030dad4458aafa2772eaf3c58e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3245c00f399845b3be20c6a59f790eef",
            "max": 76504,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b38f5dfa034d4fc399c0d47b48e5b366",
            "value": 76504
          }
        },
        "a240cc77d68a4fd9b424e9abe66c722e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7dcc5d11733a4e7ca0b5a42fcba4984b",
            "placeholder": "​",
            "style": "IPY_MODEL_769f76e0e97144b5b72039ee54354513",
            "value": " 76.5k/76.5k [00:00&lt;00:00, 401kB/s]"
          }
        },
        "1856c62b959b422abf49a7419cf73ced": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c9de7ba411b47af88351a353e775c51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "876a8c12927443759d880736bd20860e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3245c00f399845b3be20c6a59f790eef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b38f5dfa034d4fc399c0d47b48e5b366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7dcc5d11733a4e7ca0b5a42fcba4984b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "769f76e0e97144b5b72039ee54354513": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec131be15b8344e3950c5eb17c474203": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95b687d60ed542e5b5dbac9405070911",
              "IPY_MODEL_05136a42ebf84bcea3a4c5a258f2ac84",
              "IPY_MODEL_56e88c0f2cdf49329a20a5ab90674409"
            ],
            "layout": "IPY_MODEL_d3d2a4f712c8427dacd8e7ae7d2317ab"
          }
        },
        "95b687d60ed542e5b5dbac9405070911": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2591637cba5748c19758fa5c2298b408",
            "placeholder": "​",
            "style": "IPY_MODEL_f0a7415d4b414563a6fcc15d930c5617",
            "value": "all/auxiliary_train-00000-of-00001.parqu(…): 100%"
          }
        },
        "05136a42ebf84bcea3a4c5a258f2ac84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e87029f6f0e649cd929cdb607cc00aa7",
            "max": 47513731,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19a651dafe43411cbadbd58278ccb84b",
            "value": 47513731
          }
        },
        "56e88c0f2cdf49329a20a5ab90674409": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91a61a35d40146fcbfaf47c0a5cc73cf",
            "placeholder": "​",
            "style": "IPY_MODEL_d8ada065ed6d4a96958ccabdb63cf525",
            "value": " 47.5M/47.5M [00:00&lt;00:00, 94.9MB/s]"
          }
        },
        "d3d2a4f712c8427dacd8e7ae7d2317ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2591637cba5748c19758fa5c2298b408": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0a7415d4b414563a6fcc15d930c5617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e87029f6f0e649cd929cdb607cc00aa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19a651dafe43411cbadbd58278ccb84b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91a61a35d40146fcbfaf47c0a5cc73cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8ada065ed6d4a96958ccabdb63cf525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "860793d30bf24acebe6528168e82319f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_888bb6e41c514377b396c4c9aa44133b",
              "IPY_MODEL_ac659aa9accd4538b6284cec6413d095",
              "IPY_MODEL_0490e8b9793a42949bad67fa8af6f6a8"
            ],
            "layout": "IPY_MODEL_a44da007596e46839bb1343270ffc0e4"
          }
        },
        "888bb6e41c514377b396c4c9aa44133b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7865120138c54a0394c6549e59f746c5",
            "placeholder": "​",
            "style": "IPY_MODEL_dc82aa968b2b4b69a928d989a17d5ac9",
            "value": "Generating test split: 100%"
          }
        },
        "ac659aa9accd4538b6284cec6413d095": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e95250dcd96b4262baa48b4280a0255d",
            "max": 14042,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21f92bf01a5f4879ab2c2e58845f841d",
            "value": 14042
          }
        },
        "0490e8b9793a42949bad67fa8af6f6a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e6619f6fbf2480a96c175fc037f0db7",
            "placeholder": "​",
            "style": "IPY_MODEL_14967283b24448e494a26ce37d27231f",
            "value": " 14042/14042 [00:00&lt;00:00, 447299.84 examples/s]"
          }
        },
        "a44da007596e46839bb1343270ffc0e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7865120138c54a0394c6549e59f746c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc82aa968b2b4b69a928d989a17d5ac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e95250dcd96b4262baa48b4280a0255d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21f92bf01a5f4879ab2c2e58845f841d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e6619f6fbf2480a96c175fc037f0db7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14967283b24448e494a26ce37d27231f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e43d20088e204837b74ea431a5218c99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_627f3a4938b04589a3a184deedba370f",
              "IPY_MODEL_92450aaa3efc44268aae782e998f93f5",
              "IPY_MODEL_73a52807cde54f89b58c7b2bec9e9dc6"
            ],
            "layout": "IPY_MODEL_95bbfc527450419ba89027d74a943eec"
          }
        },
        "627f3a4938b04589a3a184deedba370f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ad44a4aeac34c9cbdedad72f6a8d806",
            "placeholder": "​",
            "style": "IPY_MODEL_d736d4a13eea432f9d5db75abff83eca",
            "value": "Generating validation split: 100%"
          }
        },
        "92450aaa3efc44268aae782e998f93f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd67a5bdf2e14d1b881dbfbd4093a915",
            "max": 1531,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91909cb4aa4a4e78a48ddef061cdcfb8",
            "value": 1531
          }
        },
        "73a52807cde54f89b58c7b2bec9e9dc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c20a4423aa74aa38035731b19c7d494",
            "placeholder": "​",
            "style": "IPY_MODEL_af0ed337ce2e44379b8ef9999170c0e1",
            "value": " 1531/1531 [00:00&lt;00:00, 112651.60 examples/s]"
          }
        },
        "95bbfc527450419ba89027d74a943eec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ad44a4aeac34c9cbdedad72f6a8d806": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d736d4a13eea432f9d5db75abff83eca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd67a5bdf2e14d1b881dbfbd4093a915": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91909cb4aa4a4e78a48ddef061cdcfb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1c20a4423aa74aa38035731b19c7d494": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af0ed337ce2e44379b8ef9999170c0e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22dd3027683446f3898d8d479958ea07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5454baa15b9e4b3bbc2c0c4c031421d4",
              "IPY_MODEL_5119638e628f40ec9587ca4ae66c6113",
              "IPY_MODEL_646bdc75efeb4d1ca5e1abd38ff6053d"
            ],
            "layout": "IPY_MODEL_92857c6c85b942d69d521c2f8e30c57f"
          }
        },
        "5454baa15b9e4b3bbc2c0c4c031421d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf76b9bad25743caa61e81c7740b3b7e",
            "placeholder": "​",
            "style": "IPY_MODEL_0006fb8e1c3649f2b42d09fd42f66d2c",
            "value": "Generating dev split: 100%"
          }
        },
        "5119638e628f40ec9587ca4ae66c6113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc1b8d806fb144509ae9cb9e2575209c",
            "max": 285,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9a74cce62b4e40768b39cda94e8fabde",
            "value": 285
          }
        },
        "646bdc75efeb4d1ca5e1abd38ff6053d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eede06dd1de948aa90d9fe9a242de089",
            "placeholder": "​",
            "style": "IPY_MODEL_ccdb273e892a4b5092f14515d323c9ba",
            "value": " 285/285 [00:00&lt;00:00, 25909.85 examples/s]"
          }
        },
        "92857c6c85b942d69d521c2f8e30c57f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf76b9bad25743caa61e81c7740b3b7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0006fb8e1c3649f2b42d09fd42f66d2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc1b8d806fb144509ae9cb9e2575209c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a74cce62b4e40768b39cda94e8fabde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eede06dd1de948aa90d9fe9a242de089": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccdb273e892a4b5092f14515d323c9ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8642a5246f584e6c9d644abd36cbbd49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85241d86b64b45c7b9ca29f6fca35d0c",
              "IPY_MODEL_e955638e925b4944b6cf1bb0008d45e7",
              "IPY_MODEL_5842d75ed65f46f4a9abbbbccbf5f62a"
            ],
            "layout": "IPY_MODEL_aa151b8bc3684bc2bb3b005cd16bea6c"
          }
        },
        "85241d86b64b45c7b9ca29f6fca35d0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3dd1cc2508394d9e8efdc0a379e56183",
            "placeholder": "​",
            "style": "IPY_MODEL_3bd4127c3fd14c21b02bcff60c65d7a6",
            "value": "Generating auxiliary_train split: 100%"
          }
        },
        "e955638e925b4944b6cf1bb0008d45e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe48e60b85414da7b8246fd0c3fcae8b",
            "max": 99842,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f5234b9a350b42919e691f229635b6a3",
            "value": 99842
          }
        },
        "5842d75ed65f46f4a9abbbbccbf5f62a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0be903d3c22a4a0aa509f3c503ca1ef5",
            "placeholder": "​",
            "style": "IPY_MODEL_4e6a08fd7d5a4ac98a29914809bf50e5",
            "value": " 99842/99842 [00:00&lt;00:00, 282692.84 examples/s]"
          }
        },
        "aa151b8bc3684bc2bb3b005cd16bea6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dd1cc2508394d9e8efdc0a379e56183": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bd4127c3fd14c21b02bcff60c65d7a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe48e60b85414da7b8246fd0c3fcae8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5234b9a350b42919e691f229635b6a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0be903d3c22a4a0aa509f3c503ca1ef5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e6a08fd7d5a4ac98a29914809bf50e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec33f7fb0a7d48bfb10637dfb95c29e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72ed3fffd8f14540b70a3504199529eb",
              "IPY_MODEL_fb44594e641449e281708fe72920f3d2",
              "IPY_MODEL_b4248eddb05042eb9bd71f1089ecb7ac"
            ],
            "layout": "IPY_MODEL_45e918832e5148b28f74bf4833dc8b0a"
          }
        },
        "72ed3fffd8f14540b70a3504199529eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d696125dc9a74baa929215f1eaed5d47",
            "placeholder": "​",
            "style": "IPY_MODEL_165d46b3840b4c7ebe336fc0d2866418",
            "value": "README.md: "
          }
        },
        "fb44594e641449e281708fe72920f3d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73aee57f4d2345bbabed2add0664b05e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8caaa1f003fd4cc496b1a4adfd250a73",
            "value": 1
          }
        },
        "b4248eddb05042eb9bd71f1089ecb7ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7b3f826f9d44b3f8f41309d514478c2",
            "placeholder": "​",
            "style": "IPY_MODEL_304da39ffaee4a7eb2fe828ce79e2e5f",
            "value": " 5.52k/? [00:00&lt;00:00, 714kB/s]"
          }
        },
        "45e918832e5148b28f74bf4833dc8b0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d696125dc9a74baa929215f1eaed5d47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "165d46b3840b4c7ebe336fc0d2866418": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73aee57f4d2345bbabed2add0664b05e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8caaa1f003fd4cc496b1a4adfd250a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7b3f826f9d44b3f8f41309d514478c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "304da39ffaee4a7eb2fe828ce79e2e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d69bca0d85946eda36617896f20af42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4d251af49d7435faeda8a4ec103253c",
              "IPY_MODEL_386c8634b75048aa9514de642c83f9cf",
              "IPY_MODEL_33bc209344514eb0a35551f1759f2ece"
            ],
            "layout": "IPY_MODEL_d7b925746ba7483d95f04d7a36bd1a00"
          }
        },
        "d4d251af49d7435faeda8a4ec103253c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5a5460ccaf2435dbff77f2404b50f20",
            "placeholder": "​",
            "style": "IPY_MODEL_e01b42a0eb7343b2ace41f3e5da7f236",
            "value": "ifeval_input_data.jsonl: "
          }
        },
        "386c8634b75048aa9514de642c83f9cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06187f55f7404dfda2874f9bb1935827",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b70ceb3d67ef47daad95364493e4a543",
            "value": 1
          }
        },
        "33bc209344514eb0a35551f1759f2ece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08d025f2c7f643209ec2c47c24fbd44f",
            "placeholder": "​",
            "style": "IPY_MODEL_b55cde61f2e3481d8a0ae244e27a6937",
            "value": " 207k/? [00:00&lt;00:00, 19.8MB/s]"
          }
        },
        "d7b925746ba7483d95f04d7a36bd1a00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5a5460ccaf2435dbff77f2404b50f20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e01b42a0eb7343b2ace41f3e5da7f236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06187f55f7404dfda2874f9bb1935827": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b70ceb3d67ef47daad95364493e4a543": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "08d025f2c7f643209ec2c47c24fbd44f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b55cde61f2e3481d8a0ae244e27a6937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04c14198df904bb1b31eb5758f4afdb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2511da2821e142b1915a8346a815d2a4",
              "IPY_MODEL_4fc5c176018640d6977be09834060362",
              "IPY_MODEL_f7b74a29d4754f689e346a7738e05853"
            ],
            "layout": "IPY_MODEL_99096615310f4d388ebebef783276fb3"
          }
        },
        "2511da2821e142b1915a8346a815d2a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a4a795c965b4fd796b74f9c8b051dad",
            "placeholder": "​",
            "style": "IPY_MODEL_f1f3a2206dbb4f848fa84ec697729aa3",
            "value": "Generating train split: 100%"
          }
        },
        "4fc5c176018640d6977be09834060362": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6ec7264effe4110b1b4f21950fb6589",
            "max": 541,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81e3192e1c3449c89f2678cb774f2ebb",
            "value": 541
          }
        },
        "f7b74a29d4754f689e346a7738e05853": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_532352a5c7f44dac8c2762f6e974c2ef",
            "placeholder": "​",
            "style": "IPY_MODEL_60a97319977d4c85b9606e6ffabbed8d",
            "value": " 541/541 [00:00&lt;00:00, 28655.55 examples/s]"
          }
        },
        "99096615310f4d388ebebef783276fb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a4a795c965b4fd796b74f9c8b051dad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1f3a2206dbb4f848fa84ec697729aa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6ec7264effe4110b1b4f21950fb6589": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81e3192e1c3449c89f2678cb774f2ebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "532352a5c7f44dac8c2762f6e974c2ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60a97319977d4c85b9606e6ffabbed8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7589d3e0a93b410db5231c697566a242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_482c0939e2394cbbad37b3bb989cebd6",
              "IPY_MODEL_0a6ed5d8518b4ed58c3de48c13e7daab",
              "IPY_MODEL_2387d9cb911b4a8eae71315f746f6bf1"
            ],
            "layout": "IPY_MODEL_246c065129384a608f62b88cd1927f6a"
          }
        },
        "482c0939e2394cbbad37b3bb989cebd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d60950f83a84f51beeb055f24f71447",
            "placeholder": "​",
            "style": "IPY_MODEL_d7b928b1b8444ca69841e16d0ce95c27",
            "value": "config.json: 100%"
          }
        },
        "0a6ed5d8518b4ed58c3de48c13e7daab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_857fb21759d74c6e965e2847db3c6ee2",
            "max": 884,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2aeb2813d601436c90b4618563cc6621",
            "value": 884
          }
        },
        "2387d9cb911b4a8eae71315f746f6bf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6bd1873a3054b0f866ce1bf94dbf5b9",
            "placeholder": "​",
            "style": "IPY_MODEL_330ea5ee4ea743949f4507d9dd8d9cbb",
            "value": " 884/884 [00:00&lt;00:00, 110kB/s]"
          }
        },
        "246c065129384a608f62b88cd1927f6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d60950f83a84f51beeb055f24f71447": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7b928b1b8444ca69841e16d0ce95c27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "857fb21759d74c6e965e2847db3c6ee2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2aeb2813d601436c90b4618563cc6621": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e6bd1873a3054b0f866ce1bf94dbf5b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "330ea5ee4ea743949f4507d9dd8d9cbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "baced79b6bcb4edfbf9f4370d4710963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_449276807c4645629a93348d779706e3",
              "IPY_MODEL_f9678abff0334d6abd7dcfcf16b92f2e",
              "IPY_MODEL_85523ad9cd474127ae52d41e7fc15b3c"
            ],
            "layout": "IPY_MODEL_824e4774a88e4a3f86c08ed707d4a85e"
          }
        },
        "449276807c4645629a93348d779706e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cb4038b1666473b88a7f3c81045a62b",
            "placeholder": "​",
            "style": "IPY_MODEL_7cf0d26db57a4b9aa97dd46e901ec38b",
            "value": "model.safetensors: 100%"
          }
        },
        "f9678abff0334d6abd7dcfcf16b92f2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4ca2e089c4e46df9a620101feadd7cc",
            "max": 1827557768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cbdcbf3c27f74097894da2b6ef207590",
            "value": 1827557768
          }
        },
        "85523ad9cd474127ae52d41e7fc15b3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20338370912a4922a8171ffa4d883c99",
            "placeholder": "​",
            "style": "IPY_MODEL_5bd672eb3e1b45e691d46b900c760ae5",
            "value": " 1.83G/1.83G [00:05&lt;00:00, 324MB/s]"
          }
        },
        "824e4774a88e4a3f86c08ed707d4a85e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cb4038b1666473b88a7f3c81045a62b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cf0d26db57a4b9aa97dd46e901ec38b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4ca2e089c4e46df9a620101feadd7cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbdcbf3c27f74097894da2b6ef207590": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "20338370912a4922a8171ffa4d883c99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bd672eb3e1b45e691d46b900c760ae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b5bdbb155014df2aad77fd1bae41379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38575cfb5ef743a483b3f51e8c45aaa8",
              "IPY_MODEL_b3ca964e7c1e4fdd8358ef07ee94aea3",
              "IPY_MODEL_4cb21e0a66ce4033a29a18bd47af9c9b"
            ],
            "layout": "IPY_MODEL_d3533eb54f824463835828b7f767c0bf"
          }
        },
        "38575cfb5ef743a483b3f51e8c45aaa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c4813af605f4fcb9e15f819dc68c980",
            "placeholder": "​",
            "style": "IPY_MODEL_f3e5459ca30a4e60a986bb0a7a3be2e2",
            "value": "generation_config.json: 100%"
          }
        },
        "b3ca964e7c1e4fdd8358ef07ee94aea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a500a1845754450944c6382595fe17e",
            "max": 180,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16b473633a0349f08fdcd1be83c2b085",
            "value": 180
          }
        },
        "4cb21e0a66ce4033a29a18bd47af9c9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc4f315f20d74889ab0d69891ef08bfc",
            "placeholder": "​",
            "style": "IPY_MODEL_62825453cbd74d2e96d3f2e945d6be63",
            "value": " 180/180 [00:00&lt;00:00, 24.0kB/s]"
          }
        },
        "d3533eb54f824463835828b7f767c0bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c4813af605f4fcb9e15f819dc68c980": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3e5459ca30a4e60a986bb0a7a3be2e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a500a1845754450944c6382595fe17e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16b473633a0349f08fdcd1be83c2b085": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc4f315f20d74889ab0d69891ef08bfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62825453cbd74d2e96d3f2e945d6be63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/llama-glu-expansion-pruning/blob/main/notebooks/03_Evaluate_1B_CARBON.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzfjNg_SIvXD"
      },
      "source": [
        "# GLU Pruning Research - Llama-3.2-1B Evaluation\n",
        "## 03 - Measuring Environmental Impact & Inference Performance\n",
        "\n",
        "### Exploring energy efficiency of width-pruned GLU models\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "\n",
        "[![Paper](https://img.shields.io/badge/OSF-Paper-blue?logo=osf&logoColor=white)](https://doi.org/10.31219/osf.io/qgxea)\n",
        "[![GitHub](https://img.shields.io/badge/⭐_Star-OptiPFair-orange?logo=github&logoColor=white)](https://github.com/peremartra/optipfair)\n",
        "[![PyPI](https://img.shields.io/pypi/v/optipfair?logo=python&logoColor=white&label=v)](https://pypi.org/project/optipfair/)\n",
        "\n",
        "**Repository:** [github.com/peremartra/llama-glu-expansion-pruning](https://github.com/peremartra/llama-glu-expansion-pruning)\n",
        "\n",
        "---\n",
        "\n",
        "**Colab Environment:** GPU L4 (or T4)\n",
        "\n",
        "**Models to Profile:**\n",
        "- Llama-3.2-1B (baseline)\n",
        "- Llama-3.2-1B-pruned-40%\n",
        "\n",
        "**Workloads (2 representative):**\n",
        "- GSM8K: Math reasoning (100 prompts, 100 max tokens)\n",
        "- MMLU: Knowledge QA (100 prompts, 50 max tokens)\n",
        "\n",
        "**Metrics Collected:**\n",
        "- Energy consumption (kWh via CodeCarbon)\n",
        "- Throughput (tokens/second)\n",
        "- Latency (TTFT - Time To First Token)\n",
        "- Memory footprint (GPU allocation)\n",
        "\n",
        "**Estimated Runtime:** ~1-2 hours total\n",
        "\n",
        "---\n",
        "\n",
        "## 📋 Notebook Objective\n",
        "\n",
        "This notebook conducts a comprehensive evaluation of the Llama-3.2-1B model family across three pruning levels (20%, 40%, 60%) to determine:\n",
        "\n",
        "1. **Performance degradation patterns** across different pruning intensities\n",
        "2. **Optimal expansion ratio** for GLU-MLP layers (hypothesis: 140%)\n",
        "3. **Task-specific resilience** to pruning (knowledge vs. algorithmic tasks)\n",
        "4. **Which models merit uploading to HuggingFace Hub** for Phase 2\n",
        "\n",
        "### Key Features:\n",
        "- ✅ **Checkpoint/Resume Support:** Survives Colab disconnections\n",
        "- ✅ **On-the-fly Pruning:** No need to pre-create models\n",
        "- ✅ **Robust Error Handling:** Continues if individual benchmarks fail\n",
        "- ✅ **Progress Tracking:** Live updates and detailed logging\n",
        "\n",
        "### Results will answer:\n",
        "- Does 40% pruning (140% expansion) truly outperform other levels?\n",
        "- Which benchmarks are most sensitive to pruning?\n",
        "- Should we upload non-star models to HF, or only the 40% version?\n",
        "\n",
        "---\n",
        "\n",
        "**Note:** This evaluation uses the MAW (Maximum Absolute Weight) neuron selection method, validated in Notebook 00 as the optimal approach for GLU architectures.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkVFbeCMIvXF"
      },
      "source": [
        "# 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nAo67s0lIvXF",
        "outputId": "d4e8cc2e-ba7b-45e2-b790-5ed92d1a7b59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.3/278.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.3/263.3 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "google-genai 1.48.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install -q optipfair\n",
        "!pip install -q lm-eval\n",
        "!pip install -q langdetect\n",
        "!pip install -q codecarbon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWIHQuIGIvXG",
        "outputId": "141ec740-f840-4825-fabc-52fa8cca6483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive for checkpoint persistence\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG2nO7YpIvXG",
        "outputId": "3950146c-1849-4725-f108-26c2dfb41282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ utils.py downloaded successfully\n"
          ]
        }
      ],
      "source": [
        "# Download utils.py from GitHub repository\n",
        "!wget -q https://raw.githubusercontent.com/peremartra/llama-glu-expansion-pruning/main/utils.py\n",
        "\n",
        "# Verify download\n",
        "import os\n",
        "if os.path.exists('utils.py'):\n",
        "    print(\"✅ utils.py downloaded successfully\")\n",
        "else:\n",
        "    print(\"❌ Failed to download utils.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHjkx6_QIvXG",
        "outputId": "752e2608-876f-4964-db7b-516670b3a242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All imports successful\n",
            "📱 Device: GPU\n",
            "   GPU: NVIDIA L4\n",
            "   Memory: 23.8 GB\n"
          ]
        }
      ],
      "source": [
        "# Import core libraries and utilities\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import glob\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Import our utility functions\n",
        "from utils import (\n",
        "    EXPERIMENT_CONFIG_CARBON,\n",
        "    BENCHMARKS_CARBON,\n",
        "    load_or_create_model,\n",
        "    run_carbon_profiling,\n",
        "    run_robust_evaluation,\n",
        "    clear_gpu_cache,\n",
        "    get_model_stats,\n",
        "    format_results_table,\n",
        "    calibrate_idle_power  # ← NUEVA FUNCIÓN\n",
        ")\n",
        "\n",
        "print(\"✅ All imports successful\")\n",
        "print(f\"📱 Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# EXPERIMENTAL RUNS CONFIGURATION\n",
        "# =============================================================================\n",
        "NUM_EXPERIMENTAL_RUNS = 3\n",
        "RANDOM_SEEDS = [42, 123, 456]  # Fixed seeds for reproducibility\n",
        "\n",
        "print(f\"🔄 Experiment Configuration:\")\n",
        "print(f\"   Number of runs: {NUM_EXPERIMENTAL_RUNS}\")\n",
        "print(f\"   Random seeds: {RANDOM_SEEDS}\")\n",
        "print(f\"   Total evaluations per model: {len(BENCHMARKS_CARBON)} workloads × {NUM_EXPERIMENTAL_RUNS} runs\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_36x9scTpv2",
        "outputId": "a3f3b585-f21e-485f-9f2a-44dad8c8b511"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Experiment Configuration:\n",
            "   Number of runs: 3\n",
            "   Random seeds: [42, 123, 456]\n",
            "   Total evaluations per model: 6 workloads × 3 runs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LixoDuXJIvXG"
      },
      "source": [
        "# 2. Configuration & Planning\n",
        "\n",
        "This section filters the experiment configuration for 1B models and displays the profiling plan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5fDt6IxIvXG",
        "outputId": "ee718a8d-d30d-4aef-d926-4884b594c50c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "📊 EVALUATION PLAN: Llama-3.2-1B Family\n",
            "======================================================================\n",
            "\n",
            "Total models to evaluate: 8\n",
            "Benchmarks per model: 6\n",
            "Total evaluations: 48\n",
            "Estimated runtime: ~1-1.5 hours\n",
            "\n",
            "Models to profile:\n",
            "----------------------------------------------------------------------\n",
            "Model                                    Pruning    Star  \n",
            "----------------------------------------------------------------------\n",
            "Llama-3.2-1B (baseline)                  0%         No    \n",
            "Llama-3.2-1B-pruned-10%                  10%        No    \n",
            "Llama-3.2-1B-pruned-20%                  20%        No    \n",
            "Llama-3.2-1B-pruned-30%                  30%        No    \n",
            "Llama-3.2-1B-pruned-40%                  40%        ⭐ Yes \n",
            "Llama-3.2-1B-pruned-50%                  50%        No    \n",
            "Llama-3.2-1B-pruned-60%                  60%        No    \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Workloads to run:\n",
            "----------------------------------------------------------------------\n",
            "1. hellaswag_latency_b1      100 prompts, 20 tokens - Short responses (Latency, TTFT, bsz=1)\n",
            "2. mmlu_latency_b1           100 prompts, 50 tokens - Knowledge QA (Latency, TTFT, bsz=1)\n",
            "3. ifeval_latency_b1         30 prompts, 150 tokens - Instruction (Latency, TTFT, bsz=1)\n",
            "4. hellaswag_throughput_b8   100 prompts, 20 tokens - Short responses (Throughput, bsz=8)\n",
            "5. mmlu_throughput_b8        100 prompts, 50 tokens - Knowledge QA (Throughput, bsz=8)\n",
            "6. ifeval_throughput_b8      30 prompts, 150 tokens - Instruction (Throughput, bsz=8)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "⚙️ Configuration:\n",
            "   - Neuron selection method: MAW (Maximum Absolute Weight)\n",
            "   - Checkpointing: Enabled (per-workload granularity)\n",
            "   - Model creation: On-the-fly pruning (no pre-creation needed)\n",
            "   - GPU warm-up: First 5 prompts excluded from metrics\n",
            "   - Energy tracking: CodeCarbon\n"
          ]
        }
      ],
      "source": [
        "# Filter configuration for 1B models only\n",
        "models_1b = [\n",
        "    config for config in EXPERIMENT_CONFIG_CARBON\n",
        "    if \"1B\" in config[\"base_model\"] and \"3B\" not in config[\"base_model\"] and \"Instruct\" not in config[\"base_model\"]\n",
        "]\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"📊 EVALUATION PLAN: Llama-3.2-1B Family\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "print(f\"Total models to evaluate: {len(models_1b) + 1}\")  # +1 for base model\n",
        "print(f\"Benchmarks per model: {len(BENCHMARKS_CARBON)}\")\n",
        "print(f\"Total evaluations: {(len(models_1b) + 1) * len(BENCHMARKS_CARBON)}\")\n",
        "print(f\"Estimated runtime: ~1-1.5 hours\\n\")\n",
        "\n",
        "# Display models table\n",
        "print(\"Models to profile:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Model':<40} {'Pruning':<10} {'Star':<6}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for config in models_1b:\n",
        "    if config['pruning_pct'] == 0:\n",
        "        model_name = \"Llama-3.2-1B (baseline)\"\n",
        "    else:\n",
        "        model_name = f\"Llama-3.2-1B-pruned-{config['pruning_pct']}%\"\n",
        "\n",
        "    pruning = f\"{config['pruning_pct']}%\"\n",
        "    star = \"⭐ Yes\" if config['is_star'] else \"No\"\n",
        "    print(f\"{model_name:<40} {pruning:<10} {star:<6}\")\n",
        "\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Display workloads\n",
        "print(\"\\nWorkloads to run:\")\n",
        "print(\"-\" * 70)\n",
        "for i, workload in enumerate(BENCHMARKS_CARBON, 1):\n",
        "    name = workload['name']\n",
        "    prompts = workload['num_prompts']\n",
        "    tokens = workload['max_new_tokens']\n",
        "    desc = workload['description']\n",
        "    print(f\"{i}. {name:<25} {prompts} prompts, {tokens} tokens - {desc}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "print(\"\\n⚙️ Configuration:\")\n",
        "print(f\"   - Neuron selection method: MAW (Maximum Absolute Weight)\")\n",
        "print(f\"   - Checkpointing: Enabled (per-workload granularity)\")\n",
        "print(f\"   - Model creation: On-the-fly pruning (no pre-creation needed)\")\n",
        "print(f\"   - GPU warm-up: First 5 prompts excluded from metrics\")\n",
        "print(f\"   - Energy tracking: CodeCarbon\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7U0stRUIvXG",
        "outputId": "807c05fe-a7f4-4ba4-c0ec-bfde70005185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Checkpoint directory: /content/drive/MyDrive/glu_pruning/checkpoints/1b_carbon\n",
            "✅ Results directory: /content/drive/MyDrive/glu_pruning/results\n",
            "\n",
            "📂 Auto-generated 7 checkpoint paths:\n",
            "  baseline  : 🆕 New\n",
            "  10pct     : 🆕 New\n",
            "  20pct     : 🆕 New\n",
            "  30pct     : 🆕 New\n",
            "  40pct     : 🆕 New\n",
            "  50pct     : 🆕 New\n",
            "  60pct     : 🆕 New\n"
          ]
        }
      ],
      "source": [
        "# Setup checkpoint paths (manual construction since helpers are internal)\n",
        "BASE_CHECKPOINT_DIR = \"/content/drive/MyDrive/glu_pruning/checkpoints\"\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/glu_pruning/results\"\n",
        "\n",
        "# Carbon-specific subdirectory\n",
        "CHECKPOINT_DIR = f\"{BASE_CHECKPOINT_DIR}/1b_carbon\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "Path(CHECKPOINT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"✅ Checkpoint directory: {CHECKPOINT_DIR}\")\n",
        "print(f\"✅ Results directory: {RESULTS_DIR}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONSTRUCT CHECKPOINT PATHS DYNAMICALLY FROM models_1b\n",
        "# ============================================================================\n",
        "checkpoint_paths = {}\n",
        "\n",
        "for config in models_1b:\n",
        "    pruning_pct = config['pruning_pct']\n",
        "\n",
        "    # Create key: \"baseline\" for 0%, \"{X}pct\" for others\n",
        "    if pruning_pct == 0:\n",
        "        key = \"baseline\"\n",
        "        filename = \"llama_3.2_1b_baseline_carbon.json\"\n",
        "    else:\n",
        "        key = f\"{pruning_pct}pct\"\n",
        "        filename = f\"llama_3.2_1b_pruned_{pruning_pct}pct_carbon.json\"\n",
        "\n",
        "    checkpoint_paths[key] = f\"{CHECKPOINT_DIR}/{filename}\"\n",
        "\n",
        "print(f\"\\n📂 Auto-generated {len(checkpoint_paths)} checkpoint paths:\")\n",
        "for key, path in checkpoint_paths.items():\n",
        "    exists = \"✅ Exists\" if Path(path).exists() else \"🆕 New\"\n",
        "    print(f\"  {key:<10}: {exists}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5yTl5gTIvXH"
      },
      "source": [
        "# 3. Baseline Evaluation\n",
        "\n",
        "Evaluate the original Llama-3.2-1B model to establish performance baseline."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================\n",
        "# CALIBRATE IDLE POWER (Run once at start)\n",
        "# ====================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"🔋 STEP 0: IDLE POWER CALIBRATION\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "idle_calibration = calibrate_idle_power(\n",
        "    device=\"cuda\",\n",
        "    duration_seconds=30,  # 30s is enough for stable measurement\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Save calibration to drive for reproducibility\n",
        "calibration_path = f\"{RESULTS_DIR}/idle_power_calibration.json\"\n",
        "with open(calibration_path, 'w') as f:\n",
        "    json.dump(idle_calibration, f, indent=2)\n",
        "\n",
        "print(f\"\\n💾 Calibration saved to: {calibration_path}\")\n",
        "print(\"=\"*70 + \"\\n\")"
      ],
      "metadata": {
        "id": "GHJPPtiHDtiy",
        "outputId": "fb76236b-8ef8-474d-897b-1e5bcb17187c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "🔋 STEP 0: IDLE POWER CALIBRATION\n",
            "======================================================================\n",
            "\n",
            "🔋 Starting idle power calibration (30s)...\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:03:52] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Measuring idle power for 30s...\n",
            "✅ Calibration complete!\n",
            "   Idle Power: 31.55 W\n",
            "   Idle Energy (30s): 0.000263 kWh\n",
            "   GPU Temperature: 42.0°C\n",
            "\n",
            "💾 Calibration saved to: /content/drive/MyDrive/glu_pruning/results/idle_power_calibration.json\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PjcHGNrsIvXH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7572cbca1dd643c1afda8f6bbf8f838e",
            "da75f2064b4c4e48aa1fd09b22986145",
            "8b3f7bbc6b2347518c2db3f2de45e51b",
            "5ed04aaa326640c2ba70c78e5bc15b58",
            "99883832aaea4ed79d391c567037e548",
            "1fb62010772a4c04911f67c07d3ebd37",
            "28d11591b8e2430da79765edfe5b4d3f",
            "9b92fc6903fc447090851c3acc26adb9",
            "f3268d59e9a640bcbd73813d713c1fa2",
            "dabdf6343df7482d9bd2560877e5eb99",
            "df8341dcc2de4185b3ef2b669b23c468",
            "34755551a8254f3bb1c5dab866c2d972",
            "5667b43c0fa949d2981eaeb2fa538535",
            "6e13a351aad64e42b1e9513015762d22",
            "59aae802b4464f88a420cd6f03e3f888",
            "56dabeab1b7146eaada7a6b7bba0bf8b",
            "e1457e2b14d949a6b7d27899038b7690",
            "05c8d29035c64286870d00033dbb77f5",
            "e5af671fba1c40688eb5d506d6d196d1",
            "356432fbb90f40579ff1f64c1947a849",
            "e758a10fdde54aeca56f4e66542f44f1",
            "91dd52f93e4b46bdb6a2adc3c96cc19f",
            "c446f05931bc43cc84964ad98eeecaa5",
            "a8343d8a458a4d4792e947201233d77a",
            "6bafdfd579c0474c8e6a2f2ab75438a4",
            "a1ee751cd7f044c78b2015ae5d290e22",
            "5794eb99344c4384bc2b445cf6108bdf",
            "ea1062ad73954f05952d2cdca248bdf0",
            "0dfd5124f0d34477bf0d03b9b41da54e",
            "6917bddd0adb4a9fbe61a7b28e8de452",
            "9cf5d7e42c674c9aa307131bca7a4e93",
            "ff509d98a1434b7abc816e708146080a",
            "5a18c17ef0404cfca3b3a6f81191c311",
            "a6b67a0b419b4ce7b4109fdf2d2ecf00",
            "4da59f50558443d7a78d187664dbdc93",
            "9b85cdfc09854c43b75776e131133d40",
            "046d49f3da1143aca83383cf1ceffb98",
            "5feada8a95c841c994ebe327db7214d7",
            "9c98fe6255d44c85a0010ccc5d9a9f6f",
            "b89bcec14d364aa990393cc9578d9d0f",
            "e8680ffe770f45e299fc72739eb1dbe0",
            "491be75b46884828a871e0404f9ab0ed",
            "dc9e55fb09be4c95b7954da408fa9148",
            "8627d8504e18473e9a7f823a8c5adee9",
            "4942fda5a72040e69ff85e155e7e17a4",
            "e5b392e3125140b09e215cf8afcda315",
            "1eaff9638e4845488fc14f2b70be37f5",
            "4006e445f1ab4c0e994c62fe66745fa8",
            "0723154f46574b5abc6ba6c87476e5b8",
            "aaf1b32cb2134014bb8c100ec50e46cc",
            "db11b5384c194ea4b2a8fef6507c4da4",
            "5b335faeeefe4451a1e2d3e4ee81c888",
            "58241ee9ac5849aa8dbc57e0a68a8b43",
            "62a8f18ceaec4b419464fccc3adff426",
            "933dab48ac7943d4952d5c23fae1b411",
            "986ad92dff2b4c9fb7730f896e7d61b1",
            "e6685dcc7cc6496497ac15e9c0b0a62b",
            "4fdc70e77f474d208a57e78813377f0d",
            "83fbe92fa2dd4042a109cdaaeeb419d3",
            "f27d255e766d4488b5165078530da727",
            "0ea217d80741479db132853acb4a8ba7",
            "05eac3137841498bb347161f9c086dbe",
            "030886d9eda94fbd94f978679da0fa71",
            "54767050e0d64c88b550a7a4a934b16d",
            "ea4005eb7c4647188a509dae334eb873",
            "016e05ffc8754c75b15e7d18597b7cd9",
            "dc1bbf4bc4104fada66bd1f1e61dbcde",
            "1773fda29eca4017bbaf0eb1014854ea",
            "b564f74df0cd4cfe957daa2a86ea1a7e",
            "530d3b8c90a4484f8af3fffbd98cdca7",
            "5cc56b2ad8114c9198a59b96b59d92ae",
            "33c6c1d432a548c9838ea5f214248985",
            "4138cb1046c341248efde776ec5b9b11",
            "3f184832292642c283aa02419a64fc8c",
            "4f1b6ba75d364224b9644cfd56e464bd",
            "a92e3e255be84519a09a1bb105b68d19",
            "1310decfd6244865a3808bd34fdcaab1",
            "c9a8c4475d3446409dd6ae355a255a2d",
            "eb2a559c2f5a40caa68f80a5032711d7",
            "c37cf3426e084b1a82ec247e41384ef4",
            "b89ab8504ed1476c92e3d9205e70f49f",
            "3a5d366737214fb8b6a46b083b87bff1",
            "9ea256230e524dea994a5715c78bfa11",
            "bce76b023f1d4512a5af0ad750da4a63",
            "9599c4e379584c8e85d7072a51465659",
            "7148228d1f96407482b2a01e446d835e",
            "fc12251e970144d0b3cd6bafbbe2b2ee",
            "47ed2276b02a484eb1016fbae3b20eb5",
            "38b78e6f79c14689bfe8c422ef954437",
            "f3eca9def6cb4fa59ae7322a7da766df",
            "22dcfd61b5b14e36a479a6a655c4f785",
            "8905263b5b2d49528b057d84e72c9ee7",
            "19cd908ba7f0449786f708534e794ed4",
            "5d28b08512244409acfb342d594a4bc7",
            "8da17ca3955a40f38687b36f76a8e332",
            "58c66e9d481f4ae2b92a9cb6aadaf4f8",
            "cdb8e822102a4dfba793629ed329fca8",
            "bca502122ca14decaaf2f3bf7af5f66d",
            "b07212e20db14a9fa3d5812cd225ad0d",
            "749d9ce021fc428590fbf49ee2caa512",
            "9113ccab573943e9ad0be9730ea5a968",
            "b60b1d633d9c41a9a3e4f640cb1b0074",
            "9b2372eea8db4d2796b6c6c9c3853c85",
            "76ade0615b59461e9e60d957abbd2905",
            "cd01cd776aef4350b7b730fd47f233f0",
            "38698557502546e593eedcbcf5e1747b",
            "0c056bbc5f1a46489be322fcd9239e5c",
            "b55840af4729443097812cafe46f90bf",
            "d5a24bf0fdea4c3bafb1949a930ba2e7",
            "5c3622bda34a4ab4a98c04e11af0faa5",
            "ce4ad91d9e144ecba36c6ea720302a3f",
            "6cd1f911903545b6960a61c3ecf699ad",
            "d5f793a240c2400fbf285380a9c655f5",
            "9fc51b2a505548e090a009dede635810",
            "68b63e407e2041ba9d3114570b507445",
            "b1006c5ddab34d4f9de2332d5d6543fc",
            "9530285040154011bb0af77185e6c5a4",
            "4c53ef5cc87d4b5dac4fe064878c32c6",
            "15d0543678d149bda56438929a5a51de",
            "2c392ef1910c4965b8a3edcc48f5cda6",
            "c98434109c544163a9d268625e908d12",
            "efd62f7415dc41e298b7e5f7a079a21f",
            "ab519d33a32644deb4a7f15b94a5b22b",
            "3109439121fa42f789fa042fa49f4eee",
            "f195830f12544aadb8a2f28d936b7697",
            "c98c233370894efaaf3a533133222233",
            "141d1afd6916486087fafd779d9fd697",
            "8baf8c63c1ee457aa685cc5dbd5d6a0b",
            "017ab8e5fe664a9db569dcae688404c2",
            "84e36f110bd64016bd2edd40a15ed51b",
            "3b5a4bb2d3984a5d9b706111ab82ea4f",
            "d4651befbf6f44af9231aeb3c2d06e6a",
            "f3528c78e3934f188ae6396a35007270",
            "8fa9abd886a54cd3a8b8a4fc0bf4f10d",
            "546cb5de8b1044148c15c051740cb960",
            "816476359e99474081b9cc3338626cd4",
            "97648046e3d34af3a98950bfbd2538af",
            "9eb7d9fd93ac4153b7167af69b37e571",
            "115a0b8055a14cf1a8e6c1d8147d2c6c",
            "c3b86132f82e4f42b76d2df64581c61b",
            "aea5094dbb5440b7a72db351c1651086",
            "de2376283aae451f9c937a0a548aef16",
            "9a8085ea570f402aa52f45c94dce8c4c",
            "7995f176e2c24de184e639e289761571",
            "8d1fac0ab71a4adfaa4b3fe8609bf6da",
            "d3c98d07f5104d1593c5a82eca1045bb",
            "19294e6b36694f5a8cf5367118068425",
            "fb1da3e16e3546f68265260245f9631a",
            "7296fb1104a64a028ac104856cbe4e7a",
            "19e64565d4e74e8f8fb2c89c0b9976c0",
            "454c731a56b44da78326e15d6507318b",
            "712a1ae5b1104b9db6059d5eb977df59",
            "8a1f8ae233f340f990067c9198f0cc82",
            "7cd75f65dd1246be80143b2537b78dd0",
            "fe429a8a161a49e6ad0055e54d844881",
            "90ef2655e07840f7b557fa33d4b66c7a",
            "52c03f70d5f84033a86b6f40fc3cba59",
            "3d0d047bbbdc4da9a780fda678d18248",
            "147fbcaaab8d4f76b1216bd22ac1b6b0",
            "e0e336493b83497f976486e36a1a0a5e",
            "49c2030496c943b686eaa8baa3a046ac",
            "581e62a2767142ff96dca9db7df7d8f6",
            "0f17792c0b42432383a5f229fc5fe6e3",
            "46c93d59cc1a4a6d9196f9bb357b6dbb",
            "ed02714f0fca464e92e4419d88b13eb7",
            "0642b17afcef417a88a3d069fa9b787e",
            "c4291ddfeb6f42b69bc3406320ed473f",
            "5de38ce401d14297892ef313cdc03f37",
            "9403290437ec4ca3bcd7be88534b5778",
            "73d8a9bc099242d291c51374a0c686f1",
            "4b0b10e7feef4b6f8ad3f9e6f5e7ef05",
            "a2b843acf9e9492ca354dd26e6355bae",
            "aec133099a8d435987850939c940654e",
            "7d56ab87d1ad42dc825d7278eabcc368",
            "51eaed4b63494f8abc3904ace91c90d2",
            "ff6751d7df3e4d35aba4f5ae20cbb148",
            "ce45a57520794f9a90a8a2ef1d340f46",
            "61d8ed7d062747649fbc273cf6a86ae4",
            "d6a49d28858c4e71843d8f620f0139ae",
            "a15ffa674e6c446db0bf5cf3fbf489a3",
            "48ab3f3909b241788066d5f60d63c778",
            "d2112d9fee48433ab6539b49fea0dca1",
            "6e7d0637c4bb439faaad62308aa81169",
            "3045f05b927e41cc8707575bc5008491",
            "d571c24b435a4bf7a9178a3dcf8c4664",
            "1884a3c69f8c460abdcc1449f1b80b6b",
            "5044c192ccd445868450fd2dcbd87382",
            "f30d2b5fb20547d983d672dd968b15ae",
            "d374c6e6a5fd4820b4eb0a62464e3990",
            "c8852e030dad4458aafa2772eaf3c58e",
            "a240cc77d68a4fd9b424e9abe66c722e",
            "1856c62b959b422abf49a7419cf73ced",
            "2c9de7ba411b47af88351a353e775c51",
            "876a8c12927443759d880736bd20860e",
            "3245c00f399845b3be20c6a59f790eef",
            "b38f5dfa034d4fc399c0d47b48e5b366",
            "7dcc5d11733a4e7ca0b5a42fcba4984b",
            "769f76e0e97144b5b72039ee54354513",
            "ec131be15b8344e3950c5eb17c474203",
            "95b687d60ed542e5b5dbac9405070911",
            "05136a42ebf84bcea3a4c5a258f2ac84",
            "56e88c0f2cdf49329a20a5ab90674409",
            "d3d2a4f712c8427dacd8e7ae7d2317ab",
            "2591637cba5748c19758fa5c2298b408",
            "f0a7415d4b414563a6fcc15d930c5617",
            "e87029f6f0e649cd929cdb607cc00aa7",
            "19a651dafe43411cbadbd58278ccb84b",
            "91a61a35d40146fcbfaf47c0a5cc73cf",
            "d8ada065ed6d4a96958ccabdb63cf525",
            "860793d30bf24acebe6528168e82319f",
            "888bb6e41c514377b396c4c9aa44133b",
            "ac659aa9accd4538b6284cec6413d095",
            "0490e8b9793a42949bad67fa8af6f6a8",
            "a44da007596e46839bb1343270ffc0e4",
            "7865120138c54a0394c6549e59f746c5",
            "dc82aa968b2b4b69a928d989a17d5ac9",
            "e95250dcd96b4262baa48b4280a0255d",
            "21f92bf01a5f4879ab2c2e58845f841d",
            "5e6619f6fbf2480a96c175fc037f0db7",
            "14967283b24448e494a26ce37d27231f",
            "e43d20088e204837b74ea431a5218c99",
            "627f3a4938b04589a3a184deedba370f",
            "92450aaa3efc44268aae782e998f93f5",
            "73a52807cde54f89b58c7b2bec9e9dc6",
            "95bbfc527450419ba89027d74a943eec",
            "6ad44a4aeac34c9cbdedad72f6a8d806",
            "d736d4a13eea432f9d5db75abff83eca",
            "bd67a5bdf2e14d1b881dbfbd4093a915",
            "91909cb4aa4a4e78a48ddef061cdcfb8",
            "1c20a4423aa74aa38035731b19c7d494",
            "af0ed337ce2e44379b8ef9999170c0e1",
            "22dd3027683446f3898d8d479958ea07",
            "5454baa15b9e4b3bbc2c0c4c031421d4",
            "5119638e628f40ec9587ca4ae66c6113",
            "646bdc75efeb4d1ca5e1abd38ff6053d",
            "92857c6c85b942d69d521c2f8e30c57f",
            "cf76b9bad25743caa61e81c7740b3b7e",
            "0006fb8e1c3649f2b42d09fd42f66d2c",
            "cc1b8d806fb144509ae9cb9e2575209c",
            "9a74cce62b4e40768b39cda94e8fabde",
            "eede06dd1de948aa90d9fe9a242de089",
            "ccdb273e892a4b5092f14515d323c9ba",
            "8642a5246f584e6c9d644abd36cbbd49",
            "85241d86b64b45c7b9ca29f6fca35d0c",
            "e955638e925b4944b6cf1bb0008d45e7",
            "5842d75ed65f46f4a9abbbbccbf5f62a",
            "aa151b8bc3684bc2bb3b005cd16bea6c",
            "3dd1cc2508394d9e8efdc0a379e56183",
            "3bd4127c3fd14c21b02bcff60c65d7a6",
            "fe48e60b85414da7b8246fd0c3fcae8b",
            "f5234b9a350b42919e691f229635b6a3",
            "0be903d3c22a4a0aa509f3c503ca1ef5",
            "4e6a08fd7d5a4ac98a29914809bf50e5",
            "ec33f7fb0a7d48bfb10637dfb95c29e0",
            "72ed3fffd8f14540b70a3504199529eb",
            "fb44594e641449e281708fe72920f3d2",
            "b4248eddb05042eb9bd71f1089ecb7ac",
            "45e918832e5148b28f74bf4833dc8b0a",
            "d696125dc9a74baa929215f1eaed5d47",
            "165d46b3840b4c7ebe336fc0d2866418",
            "73aee57f4d2345bbabed2add0664b05e",
            "8caaa1f003fd4cc496b1a4adfd250a73",
            "b7b3f826f9d44b3f8f41309d514478c2",
            "304da39ffaee4a7eb2fe828ce79e2e5f",
            "1d69bca0d85946eda36617896f20af42",
            "d4d251af49d7435faeda8a4ec103253c",
            "386c8634b75048aa9514de642c83f9cf",
            "33bc209344514eb0a35551f1759f2ece",
            "d7b925746ba7483d95f04d7a36bd1a00",
            "c5a5460ccaf2435dbff77f2404b50f20",
            "e01b42a0eb7343b2ace41f3e5da7f236",
            "06187f55f7404dfda2874f9bb1935827",
            "b70ceb3d67ef47daad95364493e4a543",
            "08d025f2c7f643209ec2c47c24fbd44f",
            "b55cde61f2e3481d8a0ae244e27a6937",
            "04c14198df904bb1b31eb5758f4afdb0",
            "2511da2821e142b1915a8346a815d2a4",
            "4fc5c176018640d6977be09834060362",
            "f7b74a29d4754f689e346a7738e05853",
            "99096615310f4d388ebebef783276fb3",
            "0a4a795c965b4fd796b74f9c8b051dad",
            "f1f3a2206dbb4f848fa84ec697729aa3",
            "b6ec7264effe4110b1b4f21950fb6589",
            "81e3192e1c3449c89f2678cb774f2ebb",
            "532352a5c7f44dac8c2762f6e974c2ef",
            "60a97319977d4c85b9606e6ffabbed8d"
          ]
        },
        "outputId": "b59e1225-4122-4034-ecbe-84ec71f82497"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "📊  PHASE 1: BASELINE PROFILING\n",
            "======================================================================\n",
            "\n",
            "\n",
            "\n",
            "🔄  BASELINE - RUN 1/3 (seed=42)\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "Loading base model: meta-llama/Llama-3.2-1B...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7572cbca1dd643c1afda8f6bbf8f838e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34755551a8254f3bb1c5dab866c2d972"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c446f05931bc43cc84964ad98eeecaa5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a6b67a0b419b4ce7b4109fdf2d2ecf00"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4942fda5a72040e69ff85e155e7e17a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "986ad92dff2b4c9fb7730f896e7d61b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅  Model loaded successfully\n",
            "🆕 Creating new checkpoint: /content/drive/MyDrive/glu_pruning/checkpoints/1b_carbon/llama_3.2_1b_baseline_carbon_run1_seed42.json\n",
            "✅ Loaded checkpoint. Completed: 0/6 workloads\n",
            "✅ Using idle power calibration: 31.55 W\n",
            "   (Measured at: 2025-11-07T12:04:23.315788)\n",
            "\n",
            "\n",
            "🚀 Starting profiling: 6 workloads remaining\n",
            "======================================================================\n",
            "\n",
            "[1/6] Profiling: hellaswag_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:04:37] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:04:37] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:04:37] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 42 for reproducible sampling\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc1bbf4bc4104fada66bd1f1e61dbcde"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00000-of-00001.parquet:   0%|          | 0.00/24.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9a8c4475d3446409dd6ae355a255a2d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/test-00000-of-00001.parquet:   0%|          | 0.00/6.11M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "38b78e6f79c14689bfe8c422ef954437"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/validation-00000-of-00001.parquet:   0%|          | 0.00/6.32M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "749d9ce021fc428590fbf49ee2caa512"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/39905 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce4ad91d9e144ecba36c6ea720302a3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/10003 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "efd62f7415dc41e298b7e5f7a079a21f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/10042 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3528c78e3934f188ae6396a35007270"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000161 kWh\n",
            "   Net emissions (idle-corrected): 0.000033 kWh\n",
            "   (Idle contribution removed: 0.000127 kWh)\n",
            "✅ hellaswag_latency_b1 completed\n",
            "   Energy: 0.000033 kWh\n",
            "   Throughput: 49.94 tok/s\n",
            "   Avg TTFT: 100.13 ms\n",
            "   Memory: 2.30 GB\n",
            "\n",
            "[2/6] Profiling: mmlu_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:04:54] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:04:55] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:04:55] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:04:55] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 42 for reproducible sampling\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7995f176e2c24de184e639e289761571"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "dataset_infos.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe429a8a161a49e6ad0055e54d844881"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "all/test-00000-of-00001.parquet:   0%|          | 0.00/3.50M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0642b17afcef417a88a3d069fa9b787e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "all/validation-00000-of-00001.parquet:   0%|          | 0.00/408k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce45a57520794f9a90a8a2ef1d340f46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "all/dev-00000-of-00001.parquet:   0%|          | 0.00/76.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f30d2b5fb20547d983d672dd968b15ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "all/auxiliary_train-00000-of-00001.parqu(…):   0%|          | 0.00/47.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec131be15b8344e3950c5eb17c474203"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/14042 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "860793d30bf24acebe6528168e82319f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/1531 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e43d20088e204837b74ea431a5218c99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating dev split:   0%|          | 0/285 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22dd3027683446f3898d8d479958ea07"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating auxiliary_train split:   0%|          | 0/99842 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8642a5246f584e6c9d644abd36cbbd49"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.001032 kWh\n",
            "   Net emissions (idle-corrected): 0.000272 kWh\n",
            "   (Idle contribution removed: 0.000761 kWh)\n",
            "✅ mmlu_latency_b1 completed\n",
            "   Energy: 0.000272 kWh\n",
            "   Throughput: 51.82 tok/s\n",
            "   Avg TTFT: 830.16 ms\n",
            "   Memory: 2.30 GB\n",
            "\n",
            "[3/6] Profiling: ifeval_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:06:22] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 12:06:23] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:06:23] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:06:23] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 42 for reproducible sampling\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec33f7fb0a7d48bfb10637dfb95c29e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ifeval_input_data.jsonl: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d69bca0d85946eda36617896f20af42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/541 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04c14198df904bb1b31eb5758f4afdb0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 30 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000586 kWh\n",
            "   Net emissions (idle-corrected): 0.000171 kWh\n",
            "   (Idle contribution removed: 0.000414 kWh)\n",
            "✅ ifeval_latency_b1 completed\n",
            "   Energy: 0.000171 kWh\n",
            "   Throughput: 51.74 tok/s\n",
            "   Avg TTFT: 1701.54 ms\n",
            "   Memory: 2.30 GB\n",
            "\n",
            "[4/6] Profiling: hellaswag_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:07:11] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:07:12] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:07:12] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:07:12] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000085 kWh\n",
            "   Net emissions (idle-corrected): 0.000024 kWh\n",
            "   (Idle contribution removed: 0.000061 kWh)\n",
            "✅ hellaswag_throughput_b8 completed\n",
            "   Energy: 0.000024 kWh\n",
            "   Throughput: 240.20 tok/s\n",
            "   Avg Batch Time: 484.44 ms\n",
            "   Memory: 2.30 GB\n",
            "\n",
            "[5/6] Profiling: mmlu_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:07:20] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:07:21] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:07:21] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:07:21] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000226 kWh\n",
            "   Net emissions (idle-corrected): 0.000063 kWh\n",
            "   (Idle contribution removed: 0.000162 kWh)\n",
            "✅ mmlu_throughput_b8 completed\n",
            "   Energy: 0.000063 kWh\n",
            "   Throughput: 280.99 tok/s\n",
            "   Avg Batch Time: 1358.85 ms\n",
            "   Memory: 2.30 GB\n",
            "\n",
            "[6/6] Profiling: ifeval_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:07:40] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:07:41] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:07:41] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:07:41] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 1 warmup iterations (out of 4 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000207 kWh\n",
            "   Net emissions (idle-corrected): 0.000053 kWh\n",
            "   (Idle contribution removed: 0.000153 kWh)\n",
            "✅ ifeval_throughput_b8 completed\n",
            "   Energy: 0.000053 kWh\n",
            "   Throughput: 273.75 tok/s\n",
            "   Avg Batch Time: 4018.31 ms\n",
            "   Memory: 2.30 GB\n",
            "\n",
            "======================================================================\n",
            "🎉 ALL WORKLOADS COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "\n",
            "✅ Run 1 completed\n",
            "Results summary:\n",
            "\n",
            "hellaswag_latency_b1:\n",
            "  Energy: 0.000033 kWh\n",
            "  Throughput: 49.94 tok/s\n",
            "\n",
            "mmlu_latency_b1:\n",
            "  Energy: 0.000272 kWh\n",
            "  Throughput: 51.82 tok/s\n",
            "\n",
            "ifeval_latency_b1:\n",
            "  Energy: 0.000171 kWh\n",
            "  Throughput: 51.74 tok/s\n",
            "\n",
            "hellaswag_throughput_b8:\n",
            "  Energy: 0.000024 kWh\n",
            "  Throughput: 240.20 tok/s\n",
            "\n",
            "mmlu_throughput_b8:\n",
            "  Energy: 0.000063 kWh\n",
            "  Throughput: 280.99 tok/s\n",
            "\n",
            "ifeval_throughput_b8:\n",
            "  Energy: 0.000053 kWh\n",
            "  Throughput: 273.75 tok/s\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "🧹  Memory cleared\n",
            "\n",
            "\n",
            "\n",
            "🔄  BASELINE - RUN 2/3 (seed=123)\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "Loading base model: meta-llama/Llama-3.2-1B...\n",
            "✅  Model loaded successfully\n",
            "🆕 Creating new checkpoint: /content/drive/MyDrive/glu_pruning/checkpoints/1b_carbon/llama_3.2_1b_baseline_carbon_run2_seed123.json\n",
            "✅ Loaded checkpoint. Completed: 0/6 workloads\n",
            "✅ Using idle power calibration: 31.55 W\n",
            "   (Measured at: 2025-11-07T12:04:23.315788)\n",
            "\n",
            "\n",
            "🚀 Starting profiling: 6 workloads remaining\n",
            "======================================================================\n",
            "\n",
            "[1/6] Profiling: hellaswag_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:08:01] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:08:02] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:08:02] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:08:02] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000127 kWh\n",
            "   Net emissions (idle-corrected): 0.000037 kWh\n",
            "   (Idle contribution removed: 0.000090 kWh)\n",
            "✅ hellaswag_latency_b1 completed\n",
            "   Energy: 0.000037 kWh\n",
            "   Throughput: 49.19 tok/s\n",
            "   Avg TTFT: 96.50 ms\n",
            "   Memory: 2.30 GB\n",
            "\n",
            "[2/6] Profiling: mmlu_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:08:13] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:08:14] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:08:14] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:08:14] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000997 kWh\n",
            "   Net emissions (idle-corrected): 0.000297 kWh\n",
            "   (Idle contribution removed: 0.000699 kWh)\n",
            "✅ mmlu_latency_b1 completed\n",
            "   Energy: 0.000297 kWh\n",
            "   Throughput: 50.69 tok/s\n",
            "   Avg TTFT: 787.65 ms\n",
            "   Memory: 2.30 GB\n",
            "\n",
            "[3/6] Profiling: ifeval_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:09:35] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:09:36] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:09:36] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:09:36] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 30 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000454 kWh\n",
            "   Net emissions (idle-corrected): 0.000136 kWh\n",
            "   (Idle contribution removed: 0.000318 kWh)\n",
            "✅ ifeval_latency_b1 completed\n",
            "   Energy: 0.000136 kWh\n",
            "   Throughput: 51.16 tok/s\n",
            "   Avg TTFT: 1111.84 ms\n",
            "   Memory: 2.30 GB\n",
            "\n",
            "[4/6] Profiling: hellaswag_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:10:13] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:10:14] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:10:14] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:10:14] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000090 kWh\n",
            "   Net emissions (idle-corrected): 0.000024 kWh\n",
            "   (Idle contribution removed: 0.000066 kWh)\n",
            "✅ hellaswag_throughput_b8 completed\n",
            "   Energy: 0.000024 kWh\n",
            "   Throughput: 248.77 tok/s\n",
            "   Avg Batch Time: 485.29 ms\n",
            "   Memory: 2.30 GB\n",
            "\n",
            "[5/6] Profiling: mmlu_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:10:22] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 12:10:23] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:10:23] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:10:23] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000236 kWh\n",
            "   Net emissions (idle-corrected): 0.000066 kWh\n",
            "   (Idle contribution removed: 0.000170 kWh)\n",
            "✅ mmlu_throughput_b8 completed\n",
            "   Energy: 0.000066 kWh\n",
            "   Throughput: 268.76 tok/s\n",
            "   Avg Batch Time: 1420.66 ms\n",
            "   Memory: 2.30 GB\n",
            "\n",
            "[6/6] Profiling: ifeval_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:10:43] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:10:44] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:10:44] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:10:44] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 1 warmup iterations (out of 4 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000200 kWh\n",
            "   Net emissions (idle-corrected): 0.000052 kWh\n",
            "   (Idle contribution removed: 0.000148 kWh)\n",
            "✅ ifeval_throughput_b8 completed\n",
            "   Energy: 0.000052 kWh\n",
            "   Throughput: 275.01 tok/s\n",
            "   Avg Batch Time: 3999.85 ms\n",
            "   Memory: 2.30 GB\n",
            "\n",
            "======================================================================\n",
            "🎉 ALL WORKLOADS COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "\n",
            "✅ Run 2 completed\n",
            "Results summary:\n",
            "\n",
            "hellaswag_latency_b1:\n",
            "  Energy: 0.000037 kWh\n",
            "  Throughput: 49.19 tok/s\n",
            "\n",
            "mmlu_latency_b1:\n",
            "  Energy: 0.000297 kWh\n",
            "  Throughput: 50.69 tok/s\n",
            "\n",
            "ifeval_latency_b1:\n",
            "  Energy: 0.000136 kWh\n",
            "  Throughput: 51.16 tok/s\n",
            "\n",
            "hellaswag_throughput_b8:\n",
            "  Energy: 0.000024 kWh\n",
            "  Throughput: 248.77 tok/s\n",
            "\n",
            "mmlu_throughput_b8:\n",
            "  Energy: 0.000066 kWh\n",
            "  Throughput: 268.76 tok/s\n",
            "\n",
            "ifeval_throughput_b8:\n",
            "  Energy: 0.000052 kWh\n",
            "  Throughput: 275.01 tok/s\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "🧹  Memory cleared\n",
            "\n",
            "\n",
            "\n",
            "🔄  BASELINE - RUN 3/3 (seed=456)\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "Loading base model: meta-llama/Llama-3.2-1B...\n",
            "✅  Model loaded successfully\n",
            "🆕 Creating new checkpoint: /content/drive/MyDrive/glu_pruning/checkpoints/1b_carbon/llama_3.2_1b_baseline_carbon_run3_seed456.json\n",
            "✅ Loaded checkpoint. Completed: 0/6 workloads\n",
            "✅ Using idle power calibration: 31.55 W\n",
            "   (Measured at: 2025-11-07T12:04:23.315788)\n",
            "\n",
            "\n",
            "🚀 Starting profiling: 6 workloads remaining\n",
            "======================================================================\n",
            "\n",
            "[1/6] Profiling: hellaswag_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:11:04] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:11:05] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:11:05] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:11:05] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000162 kWh\n",
            "   Net emissions (idle-corrected): 0.000047 kWh\n",
            "   (Idle contribution removed: 0.000114 kWh)\n",
            "✅ hellaswag_latency_b1 completed\n",
            "   Energy: 0.000047 kWh\n",
            "   Throughput: 49.47 tok/s\n",
            "   Avg TTFT: 121.29 ms\n",
            "   Memory: 2.30 GB\n",
            "\n",
            "[2/6] Profiling: mmlu_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:11:19] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:11:20] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:11:20] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:11:20] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.001095 kWh\n",
            "   Net emissions (idle-corrected): 0.000330 kWh\n",
            "   (Idle contribution removed: 0.000764 kWh)\n",
            "✅ mmlu_latency_b1 completed\n",
            "   Energy: 0.000330 kWh\n",
            "   Throughput: 51.74 tok/s\n",
            "   Avg TTFT: 855.34 ms\n",
            "   Memory: 2.30 GB\n",
            "\n",
            "[3/6] Profiling: ifeval_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:12:47] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:12:49] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:12:49] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:12:49] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 30 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000494 kWh\n",
            "   Net emissions (idle-corrected): 0.000148 kWh\n",
            "   (Idle contribution removed: 0.000346 kWh)\n",
            "✅ ifeval_latency_b1 completed\n",
            "   Energy: 0.000148 kWh\n",
            "   Throughput: 51.51 tok/s\n",
            "   Avg TTFT: 1272.64 ms\n",
            "   Memory: 2.30 GB\n",
            "\n",
            "[4/6] Profiling: hellaswag_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:13:29] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:13:30] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:13:30] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:13:30] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000090 kWh\n",
            "   Net emissions (idle-corrected): 0.000024 kWh\n",
            "   (Idle contribution removed: 0.000066 kWh)\n",
            "✅ hellaswag_throughput_b8 completed\n",
            "   Energy: 0.000024 kWh\n",
            "   Throughput: 239.87 tok/s\n",
            "   Avg Batch Time: 491.18 ms\n",
            "   Memory: 2.30 GB\n",
            "\n",
            "[5/6] Profiling: mmlu_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:13:38] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:13:39] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:13:39] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:13:39] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000229 kWh\n",
            "   Net emissions (idle-corrected): 0.000063 kWh\n",
            "   (Idle contribution removed: 0.000166 kWh)\n",
            "✅ mmlu_throughput_b8 completed\n",
            "   Energy: 0.000063 kWh\n",
            "   Throughput: 275.75 tok/s\n",
            "   Avg Batch Time: 1384.65 ms\n",
            "   Memory: 2.30 GB\n",
            "\n",
            "[6/6] Profiling: ifeval_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:13:58] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:13:59] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:13:59] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:13:59] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 1 warmup iterations (out of 4 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000201 kWh\n",
            "   Net emissions (idle-corrected): 0.000053 kWh\n",
            "   (Idle contribution removed: 0.000149 kWh)\n",
            "✅ ifeval_throughput_b8 completed\n",
            "   Energy: 0.000053 kWh\n",
            "   Throughput: 276.58 tok/s\n",
            "   Avg Batch Time: 3977.16 ms\n",
            "   Memory: 2.30 GB\n",
            "\n",
            "======================================================================\n",
            "🎉 ALL WORKLOADS COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "\n",
            "✅ Run 3 completed\n",
            "Results summary:\n",
            "\n",
            "hellaswag_latency_b1:\n",
            "  Energy: 0.000047 kWh\n",
            "  Throughput: 49.47 tok/s\n",
            "\n",
            "mmlu_latency_b1:\n",
            "  Energy: 0.000330 kWh\n",
            "  Throughput: 51.74 tok/s\n",
            "\n",
            "ifeval_latency_b1:\n",
            "  Energy: 0.000148 kWh\n",
            "  Throughput: 51.51 tok/s\n",
            "\n",
            "hellaswag_throughput_b8:\n",
            "  Energy: 0.000024 kWh\n",
            "  Throughput: 239.87 tok/s\n",
            "\n",
            "mmlu_throughput_b8:\n",
            "  Energy: 0.000063 kWh\n",
            "  Throughput: 275.75 tok/s\n",
            "\n",
            "ifeval_throughput_b8:\n",
            "  Energy: 0.000053 kWh\n",
            "  Throughput: 276.58 tok/s\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "🧹  Memory cleared\n",
            "\n",
            "\n",
            "======================================================================\n",
            "✅  ALL BASELINE RUNS COMPLETED\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"📊  PHASE 1: BASELINE PROFILING\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "BASE_MODEL_ID = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# ============================================================================\n",
        "# MULTIPLE RUNS LOOP FOR BASELINE\n",
        "# ============================================================================\n",
        "baseline_all_runs = {}  # Store results from all runs\n",
        "\n",
        "for run_idx, seed in enumerate(RANDOM_SEEDS, 1):\n",
        "    print(f\"\\n{''*70}\")\n",
        "    print(f\"🔄  BASELINE - RUN {run_idx}/{NUM_EXPERIMENTAL_RUNS} (seed={seed})\")\n",
        "    print(f\"{'─'*70}\\n\")\n",
        "\n",
        "    # Load base model (fresh for each run to avoid state contamination)\n",
        "    print(f\"Loading base model: {BASE_MODEL_ID}...\")\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        BASE_MODEL_ID,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print(\"✅  Model loaded successfully\")\n",
        "\n",
        "    # Construct checkpoint path for this run\n",
        "    checkpoint_path_run = checkpoint_paths[\"baseline\"].replace(\".json\", f\"_run{run_idx}_seed{seed}.json\")\n",
        "\n",
        "    # Run carbon profiling with seed\n",
        "    baseline_results_run = run_carbon_profiling(\n",
        "        model=base_model,\n",
        "        tokenizer=tokenizer,\n",
        "        workloads=BENCHMARKS_CARBON,\n",
        "        checkpoint_path=checkpoint_path_run,\n",
        "        model_name=f\"Llama-3.2-1B-baseline-run{run_idx}\",\n",
        "        idle_power_calibration=idle_calibration,  # ← CAMBIO: usar None\n",
        "        device=\"cuda\",\n",
        "        random_seed=seed  # ← NUEVO: pasar seed\n",
        "    )\n",
        "\n",
        "    # Store results for this run\n",
        "    baseline_all_runs[f\"run_{run_idx}\"] = baseline_results_run\n",
        "\n",
        "    # Display run summary\n",
        "    print(f\"\\n✅ Run {run_idx} completed\")\n",
        "    print(\"Results summary:\")\n",
        "    for workload_name, metrics in baseline_results_run.items():\n",
        "        print(f\"\\n{workload_name}:\")\n",
        "        print(f\"  Energy: {metrics['energy_kwh']:.6f} kWh\")\n",
        "        print(f\"  Throughput: {metrics['throughput_tokens_per_sec']:.2f} tok/s\")\n",
        "\n",
        "    # Clear memory before next run\n",
        "    del base_model\n",
        "    clear_gpu_cache()\n",
        "    print(f\"🧹  Memory cleared\\n\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"✅  ALL BASELINE RUNS COMPLETED\")\n",
        "print(f\"{'='*70}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtbaAsxIIvXH"
      },
      "source": [
        "# 4. Pruned Models Evaluation Loop\n",
        "\n",
        "Profile the pruned variants using on-the-fly pruning with OptiPFair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FPoZiAfjIvXH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7589d3e0a93b410db5231c697566a242",
            "482c0939e2394cbbad37b3bb989cebd6",
            "0a6ed5d8518b4ed58c3de48c13e7daab",
            "2387d9cb911b4a8eae71315f746f6bf1",
            "246c065129384a608f62b88cd1927f6a",
            "7d60950f83a84f51beeb055f24f71447",
            "d7b928b1b8444ca69841e16d0ce95c27",
            "857fb21759d74c6e965e2847db3c6ee2",
            "2aeb2813d601436c90b4618563cc6621",
            "e6bd1873a3054b0f866ce1bf94dbf5b9",
            "330ea5ee4ea743949f4507d9dd8d9cbb",
            "baced79b6bcb4edfbf9f4370d4710963",
            "449276807c4645629a93348d779706e3",
            "f9678abff0334d6abd7dcfcf16b92f2e",
            "85523ad9cd474127ae52d41e7fc15b3c",
            "824e4774a88e4a3f86c08ed707d4a85e",
            "2cb4038b1666473b88a7f3c81045a62b",
            "7cf0d26db57a4b9aa97dd46e901ec38b",
            "d4ca2e089c4e46df9a620101feadd7cc",
            "cbdcbf3c27f74097894da2b6ef207590",
            "20338370912a4922a8171ffa4d883c99",
            "5bd672eb3e1b45e691d46b900c760ae5",
            "4b5bdbb155014df2aad77fd1bae41379",
            "38575cfb5ef743a483b3f51e8c45aaa8",
            "b3ca964e7c1e4fdd8358ef07ee94aea3",
            "4cb21e0a66ce4033a29a18bd47af9c9b",
            "d3533eb54f824463835828b7f767c0bf",
            "8c4813af605f4fcb9e15f819dc68c980",
            "f3e5459ca30a4e60a986bb0a7a3be2e2",
            "1a500a1845754450944c6382595fe17e",
            "16b473633a0349f08fdcd1be83c2b085",
            "fc4f315f20d74889ab0d69891ef08bfc",
            "62825453cbd74d2e96d3f2e945d6be63"
          ]
        },
        "outputId": "ff77aa68-c35c-4f53-b84a-a682b20bf415"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "📊  PHASE 2: PRUNED MODELS PROFILING\n",
            "======================================================================\n",
            "\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "🔄  PROFILING MODEL 1/6: Llama-3.2-1B-pruned-10%\n",
            "   Pruning: 10% | Star: ❌\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "\n",
            "······································································\n",
            "   RUN 1/3 (seed=42)\n",
            "······································································\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Loading model: oopere/Llama-3.2-1B-pruned-10pct\n",
            "  Base: meta-llama/Llama-3.2-1B\n",
            "  Pruning: 10%\n",
            "  Star model: No (on-the-fly)\n",
            "======================================================================\n",
            "\n",
            "🔧 Creating model via on-the-fly pruning...\n",
            "✂️  Pruning with MAW method (10%)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning layers: 100%|██████████| 16/16 [00:05<00:00,  2.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model created\n",
            "   Original params: 1,235,814,400\n",
            "   Pruned params: 1,155,303,424\n",
            "   Reduction: 6.51%\n",
            "\n",
            "📈  Model Statistics:\n",
            "   Parameters: 1,155,303,424\n",
            "   Size: 2.15 GB\n",
            "   Reduction: 6.5%\n",
            "   Source: on_the_fly_pruning\n",
            "\n",
            "🆕 Creating new checkpoint: /content/drive/MyDrive/glu_pruning/checkpoints/1b_carbon/llama_3.2_1b_pruned_10pct_carbon_run1_seed42.json\n",
            "✅ Loaded checkpoint. Completed: 0/6 workloads\n",
            "✅ Using idle power calibration: 31.55 W\n",
            "   (Measured at: 2025-11-07T12:04:23.315788)\n",
            "\n",
            "\n",
            "🚀 Starting profiling: 6 workloads remaining\n",
            "======================================================================\n",
            "\n",
            "[1/6] Profiling: hellaswag_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:14:25] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:14:26] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:14:26] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:14:26] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000254 kWh\n",
            "   Net emissions (idle-corrected): 0.000074 kWh\n",
            "   (Idle contribution removed: 0.000180 kWh)\n",
            "✅ hellaswag_latency_b1 completed\n",
            "   Energy: 0.000074 kWh\n",
            "   Throughput: 49.82 tok/s\n",
            "   Avg TTFT: 194.18 ms\n",
            "   Memory: 2.15 GB\n",
            "\n",
            "[2/6] Profiling: mmlu_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:14:47] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 12:14:48] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:14:48] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:14:48] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.001010 kWh\n",
            "   Net emissions (idle-corrected): 0.000295 kWh\n",
            "   (Idle contribution removed: 0.000715 kWh)\n",
            "✅ mmlu_latency_b1 completed\n",
            "   Energy: 0.000295 kWh\n",
            "   Throughput: 51.36 tok/s\n",
            "   Avg TTFT: 816.06 ms\n",
            "   Memory: 2.15 GB\n",
            "\n",
            "[3/6] Profiling: ifeval_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:16:10] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 12:16:12] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:16:12] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:16:12] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 30 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000695 kWh\n",
            "   Net emissions (idle-corrected): 0.000202 kWh\n",
            "   (Idle contribution removed: 0.000493 kWh)\n",
            "✅ ifeval_latency_b1 completed\n",
            "   Energy: 0.000202 kWh\n",
            "   Throughput: 51.06 tok/s\n",
            "   Avg TTFT: 2051.65 ms\n",
            "   Memory: 2.15 GB\n",
            "\n",
            "[4/6] Profiling: hellaswag_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:17:08] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:17:09] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:17:09] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:17:09] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000116 kWh\n",
            "   Net emissions (idle-corrected): 0.000035 kWh\n",
            "   (Idle contribution removed: 0.000081 kWh)\n",
            "✅ hellaswag_throughput_b8 completed\n",
            "   Energy: 0.000035 kWh\n",
            "   Throughput: 239.80 tok/s\n",
            "   Avg Batch Time: 636.89 ms\n",
            "   Memory: 2.15 GB\n",
            "\n",
            "[5/6] Profiling: mmlu_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:17:19] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:17:20] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:17:20] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:17:20] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000243 kWh\n",
            "   Net emissions (idle-corrected): 0.000074 kWh\n",
            "   (Idle contribution removed: 0.000168 kWh)\n",
            "✅ mmlu_throughput_b8 completed\n",
            "   Energy: 0.000074 kWh\n",
            "   Throughput: 274.56 tok/s\n",
            "   Avg Batch Time: 1390.65 ms\n",
            "   Memory: 2.15 GB\n",
            "\n",
            "[6/6] Profiling: ifeval_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:17:40] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:17:41] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:17:41] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:17:41] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 1 warmup iterations (out of 4 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000212 kWh\n",
            "   Net emissions (idle-corrected): 0.000063 kWh\n",
            "   (Idle contribution removed: 0.000149 kWh)\n",
            "✅ ifeval_throughput_b8 completed\n",
            "   Energy: 0.000063 kWh\n",
            "   Throughput: 274.12 tok/s\n",
            "   Avg Batch Time: 4012.90 ms\n",
            "   Memory: 2.15 GB\n",
            "\n",
            "======================================================================\n",
            "🎉 ALL WORKLOADS COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "\n",
            "✅ Run 1 completed for Llama-3.2-1B-pruned-10%\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "\n",
            "······································································\n",
            "   RUN 2/3 (seed=123)\n",
            "······································································\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Loading model: oopere/Llama-3.2-1B-pruned-10pct\n",
            "  Base: meta-llama/Llama-3.2-1B\n",
            "  Pruning: 10%\n",
            "  Star model: No (on-the-fly)\n",
            "======================================================================\n",
            "\n",
            "🔧 Creating model via on-the-fly pruning...\n",
            "✂️  Pruning with MAW method (10%)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning layers: 100%|██████████| 16/16 [00:05<00:00,  2.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model created\n",
            "   Original params: 1,235,814,400\n",
            "   Pruned params: 1,155,303,424\n",
            "   Reduction: 6.51%\n",
            "🆕 Creating new checkpoint: /content/drive/MyDrive/glu_pruning/checkpoints/1b_carbon/llama_3.2_1b_pruned_10pct_carbon_run2_seed123.json\n",
            "✅ Loaded checkpoint. Completed: 0/6 workloads\n",
            "✅ Using idle power calibration: 31.55 W\n",
            "   (Measured at: 2025-11-07T12:04:23.315788)\n",
            "\n",
            "\n",
            "🚀 Starting profiling: 6 workloads remaining\n",
            "======================================================================\n",
            "\n",
            "[1/6] Profiling: hellaswag_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:18:07] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 12:18:08] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:18:08] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:18:08] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000204 kWh\n",
            "   Net emissions (idle-corrected): 0.000058 kWh\n",
            "   (Idle contribution removed: 0.000146 kWh)\n",
            "✅ hellaswag_latency_b1 completed\n",
            "   Energy: 0.000058 kWh\n",
            "   Throughput: 49.77 tok/s\n",
            "   Avg TTFT: 162.02 ms\n",
            "   Memory: 2.15 GB\n",
            "\n",
            "[2/6] Profiling: mmlu_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:18:25] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:18:26] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:18:26] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:18:26] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000926 kWh\n",
            "   Net emissions (idle-corrected): 0.000273 kWh\n",
            "   (Idle contribution removed: 0.000652 kWh)\n",
            "✅ mmlu_latency_b1 completed\n",
            "   Energy: 0.000273 kWh\n",
            "   Throughput: 51.27 tok/s\n",
            "   Avg TTFT: 730.87 ms\n",
            "   Memory: 2.15 GB\n",
            "\n",
            "[3/6] Profiling: ifeval_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:19:41] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:19:42] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:19:42] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:19:42] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 30 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000696 kWh\n",
            "   Net emissions (idle-corrected): 0.000204 kWh\n",
            "   (Idle contribution removed: 0.000491 kWh)\n",
            "✅ ifeval_latency_b1 completed\n",
            "   Energy: 0.000204 kWh\n",
            "   Throughput: 51.62 tok/s\n",
            "   Avg TTFT: 1811.69 ms\n",
            "   Memory: 2.15 GB\n",
            "\n",
            "[4/6] Profiling: hellaswag_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:20:39] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:20:40] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:20:40] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:20:40] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000110 kWh\n",
            "   Net emissions (idle-corrected): 0.000034 kWh\n",
            "   (Idle contribution removed: 0.000076 kWh)\n",
            "✅ hellaswag_throughput_b8 completed\n",
            "   Energy: 0.000034 kWh\n",
            "   Throughput: 245.26 tok/s\n",
            "   Avg Batch Time: 598.99 ms\n",
            "   Memory: 2.15 GB\n",
            "\n",
            "[5/6] Profiling: mmlu_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:20:49] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:20:50] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:20:50] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:20:50] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000249 kWh\n",
            "   Net emissions (idle-corrected): 0.000077 kWh\n",
            "   (Idle contribution removed: 0.000171 kWh)\n",
            "✅ mmlu_throughput_b8 completed\n",
            "   Energy: 0.000077 kWh\n",
            "   Throughput: 269.42 tok/s\n",
            "   Avg Batch Time: 1417.20 ms\n",
            "   Memory: 2.15 GB\n",
            "\n",
            "[6/6] Profiling: ifeval_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:21:10] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 12:21:12] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:21:12] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:21:12] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 1 warmup iterations (out of 4 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000209 kWh\n",
            "   Net emissions (idle-corrected): 0.000062 kWh\n",
            "   (Idle contribution removed: 0.000146 kWh)\n",
            "✅ ifeval_throughput_b8 completed\n",
            "   Energy: 0.000062 kWh\n",
            "   Throughput: 280.05 tok/s\n",
            "   Avg Batch Time: 3927.83 ms\n",
            "   Memory: 2.15 GB\n",
            "\n",
            "======================================================================\n",
            "🎉 ALL WORKLOADS COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "\n",
            "✅ Run 2 completed for Llama-3.2-1B-pruned-10%\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "\n",
            "······································································\n",
            "   RUN 3/3 (seed=456)\n",
            "······································································\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Loading model: oopere/Llama-3.2-1B-pruned-10pct\n",
            "  Base: meta-llama/Llama-3.2-1B\n",
            "  Pruning: 10%\n",
            "  Star model: No (on-the-fly)\n",
            "======================================================================\n",
            "\n",
            "🔧 Creating model via on-the-fly pruning...\n",
            "✂️  Pruning with MAW method (10%)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning layers: 100%|██████████| 16/16 [00:05<00:00,  2.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model created\n",
            "   Original params: 1,235,814,400\n",
            "   Pruned params: 1,155,303,424\n",
            "   Reduction: 6.51%\n",
            "🆕 Creating new checkpoint: /content/drive/MyDrive/glu_pruning/checkpoints/1b_carbon/llama_3.2_1b_pruned_10pct_carbon_run3_seed456.json\n",
            "✅ Loaded checkpoint. Completed: 0/6 workloads\n",
            "✅ Using idle power calibration: 31.55 W\n",
            "   (Measured at: 2025-11-07T12:04:23.315788)\n",
            "\n",
            "\n",
            "🚀 Starting profiling: 6 workloads remaining\n",
            "======================================================================\n",
            "\n",
            "[1/6] Profiling: hellaswag_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:21:37] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 12:21:38] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:21:38] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:21:38] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000270 kWh\n",
            "   Net emissions (idle-corrected): 0.000077 kWh\n",
            "   (Idle contribution removed: 0.000193 kWh)\n",
            "✅ hellaswag_latency_b1 completed\n",
            "   Energy: 0.000077 kWh\n",
            "   Throughput: 50.27 tok/s\n",
            "   Avg TTFT: 214.02 ms\n",
            "   Memory: 2.15 GB\n",
            "\n",
            "[2/6] Profiling: mmlu_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:22:00] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 12:22:02] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:22:02] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:22:02] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.001006 kWh\n",
            "   Net emissions (idle-corrected): 0.000297 kWh\n",
            "   (Idle contribution removed: 0.000709 kWh)\n",
            "✅ mmlu_latency_b1 completed\n",
            "   Energy: 0.000297 kWh\n",
            "   Throughput: 51.56 tok/s\n",
            "   Avg TTFT: 787.98 ms\n",
            "   Memory: 2.15 GB\n",
            "\n",
            "[3/6] Profiling: ifeval_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:23:23] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:23:24] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:23:24] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:23:24] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 30 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000727 kWh\n",
            "   Net emissions (idle-corrected): 0.000214 kWh\n",
            "   (Idle contribution removed: 0.000513 kWh)\n",
            "✅ ifeval_latency_b1 completed\n",
            "   Energy: 0.000214 kWh\n",
            "   Throughput: 51.78 tok/s\n",
            "   Avg TTFT: 1861.65 ms\n",
            "   Memory: 2.15 GB\n",
            "\n",
            "[4/6] Profiling: hellaswag_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:24:23] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:24:24] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:24:24] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:24:24] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000114 kWh\n",
            "   Net emissions (idle-corrected): 0.000034 kWh\n",
            "   (Idle contribution removed: 0.000080 kWh)\n",
            "✅ hellaswag_throughput_b8 completed\n",
            "   Energy: 0.000034 kWh\n",
            "   Throughput: 236.54 tok/s\n",
            "   Avg Batch Time: 614.92 ms\n",
            "   Memory: 2.15 GB\n",
            "\n",
            "[5/6] Profiling: mmlu_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:24:34] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:24:35] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:24:35] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:24:35] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000244 kWh\n",
            "   Net emissions (idle-corrected): 0.000075 kWh\n",
            "   (Idle contribution removed: 0.000169 kWh)\n",
            "✅ mmlu_throughput_b8 completed\n",
            "   Energy: 0.000075 kWh\n",
            "   Throughput: 268.69 tok/s\n",
            "   Avg Batch Time: 1421.04 ms\n",
            "   Memory: 2.15 GB\n",
            "\n",
            "[6/6] Profiling: ifeval_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:24:55] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:24:56] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:24:56] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:24:56] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 1 warmup iterations (out of 4 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000212 kWh\n",
            "   Net emissions (idle-corrected): 0.000063 kWh\n",
            "   (Idle contribution removed: 0.000149 kWh)\n",
            "✅ ifeval_throughput_b8 completed\n",
            "   Energy: 0.000063 kWh\n",
            "   Throughput: 278.15 tok/s\n",
            "   Avg Batch Time: 3954.71 ms\n",
            "   Memory: 2.15 GB\n",
            "\n",
            "======================================================================\n",
            "🎉 ALL WORKLOADS COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "\n",
            "✅ Run 3 completed for Llama-3.2-1B-pruned-10%\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "\n",
            "✅ All runs completed for Llama-3.2-1B-pruned-10%\n",
            "\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "🔄  PROFILING MODEL 2/6: Llama-3.2-1B-pruned-20%\n",
            "   Pruning: 20% | Star: ❌\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "\n",
            "······································································\n",
            "   RUN 1/3 (seed=42)\n",
            "······································································\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Loading model: oopere/Llama-3.2-1B-pruned-20pct\n",
            "  Base: meta-llama/Llama-3.2-1B\n",
            "  Pruning: 20%\n",
            "  Star model: No (on-the-fly)\n",
            "======================================================================\n",
            "\n",
            "🔧 Creating model via on-the-fly pruning...\n",
            "✂️  Pruning with MAW method (20%)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning layers: 100%|██████████| 16/16 [00:05<00:00,  3.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model created\n",
            "   Original params: 1,235,814,400\n",
            "   Pruned params: 1,074,792,448\n",
            "   Reduction: 13.03%\n",
            "\n",
            "📈  Model Statistics:\n",
            "   Parameters: 1,074,792,448\n",
            "   Size: 2.00 GB\n",
            "   Reduction: 13.0%\n",
            "   Source: on_the_fly_pruning\n",
            "\n",
            "🆕 Creating new checkpoint: /content/drive/MyDrive/glu_pruning/checkpoints/1b_carbon/llama_3.2_1b_pruned_20pct_carbon_run1_seed42.json\n",
            "✅ Loaded checkpoint. Completed: 0/6 workloads\n",
            "✅ Using idle power calibration: 31.55 W\n",
            "   (Measured at: 2025-11-07T12:04:23.315788)\n",
            "\n",
            "\n",
            "🚀 Starting profiling: 6 workloads remaining\n",
            "======================================================================\n",
            "\n",
            "[1/6] Profiling: hellaswag_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:25:21] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:25:22] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:25:22] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:25:22] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000377 kWh\n",
            "   Net emissions (idle-corrected): 0.000106 kWh\n",
            "   (Idle contribution removed: 0.000270 kWh)\n",
            "✅ hellaswag_latency_b1 completed\n",
            "   Energy: 0.000106 kWh\n",
            "   Throughput: 50.87 tok/s\n",
            "   Avg TTFT: 300.48 ms\n",
            "   Memory: 2.00 GB\n",
            "\n",
            "[2/6] Profiling: mmlu_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:25:54] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:25:55] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:25:55] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:25:55] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000975 kWh\n",
            "   Net emissions (idle-corrected): 0.000276 kWh\n",
            "   (Idle contribution removed: 0.000699 kWh)\n",
            "✅ mmlu_latency_b1 completed\n",
            "   Energy: 0.000276 kWh\n",
            "   Throughput: 51.89 tok/s\n",
            "   Avg TTFT: 788.17 ms\n",
            "   Memory: 2.00 GB\n",
            "\n",
            "[3/6] Profiling: ifeval_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:27:15] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:27:16] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:27:16] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:27:16] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 30 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000858 kWh\n",
            "   Net emissions (idle-corrected): 0.000243 kWh\n",
            "   (Idle contribution removed: 0.000616 kWh)\n",
            "✅ ifeval_latency_b1 completed\n",
            "   Energy: 0.000243 kWh\n",
            "   Throughput: 51.79 tok/s\n",
            "   Avg TTFT: 2306.14 ms\n",
            "   Memory: 2.00 GB\n",
            "\n",
            "[4/6] Profiling: hellaswag_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:28:27] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:28:28] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:28:28] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:28:28] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000106 kWh\n",
            "   Net emissions (idle-corrected): 0.000028 kWh\n",
            "   (Idle contribution removed: 0.000078 kWh)\n",
            "✅ hellaswag_throughput_b8 completed\n",
            "   Energy: 0.000028 kWh\n",
            "   Throughput: 252.93 tok/s\n",
            "   Avg Batch Time: 603.84 ms\n",
            "   Memory: 2.00 GB\n",
            "\n",
            "[5/6] Profiling: mmlu_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:28:37] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:28:39] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:28:39] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:28:39] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000224 kWh\n",
            "   Net emissions (idle-corrected): 0.000060 kWh\n",
            "   (Idle contribution removed: 0.000164 kWh)\n",
            "✅ mmlu_throughput_b8 completed\n",
            "   Energy: 0.000060 kWh\n",
            "   Throughput: 280.14 tok/s\n",
            "   Avg Batch Time: 1362.98 ms\n",
            "   Memory: 2.00 GB\n",
            "\n",
            "[6/6] Profiling: ifeval_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:28:58] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:28:59] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:28:59] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:28:59] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 1 warmup iterations (out of 4 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000200 kWh\n",
            "   Net emissions (idle-corrected): 0.000052 kWh\n",
            "   (Idle contribution removed: 0.000148 kWh)\n",
            "✅ ifeval_throughput_b8 completed\n",
            "   Energy: 0.000052 kWh\n",
            "   Throughput: 275.37 tok/s\n",
            "   Avg Batch Time: 3994.68 ms\n",
            "   Memory: 2.00 GB\n",
            "\n",
            "======================================================================\n",
            "🎉 ALL WORKLOADS COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "\n",
            "✅ Run 1 completed for Llama-3.2-1B-pruned-20%\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "\n",
            "······································································\n",
            "   RUN 2/3 (seed=123)\n",
            "······································································\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Loading model: oopere/Llama-3.2-1B-pruned-20pct\n",
            "  Base: meta-llama/Llama-3.2-1B\n",
            "  Pruning: 20%\n",
            "  Star model: No (on-the-fly)\n",
            "======================================================================\n",
            "\n",
            "🔧 Creating model via on-the-fly pruning...\n",
            "✂️  Pruning with MAW method (20%)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning layers: 100%|██████████| 16/16 [00:05<00:00,  3.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model created\n",
            "   Original params: 1,235,814,400\n",
            "   Pruned params: 1,074,792,448\n",
            "   Reduction: 13.03%\n",
            "🆕 Creating new checkpoint: /content/drive/MyDrive/glu_pruning/checkpoints/1b_carbon/llama_3.2_1b_pruned_20pct_carbon_run2_seed123.json\n",
            "✅ Loaded checkpoint. Completed: 0/6 workloads\n",
            "✅ Using idle power calibration: 31.55 W\n",
            "   (Measured at: 2025-11-07T12:04:23.315788)\n",
            "\n",
            "\n",
            "🚀 Starting profiling: 6 workloads remaining\n",
            "======================================================================\n",
            "\n",
            "[1/6] Profiling: hellaswag_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:29:24] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 12:29:25] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:29:25] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:29:25] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000368 kWh\n",
            "   Net emissions (idle-corrected): 0.000104 kWh\n",
            "   (Idle contribution removed: 0.000264 kWh)\n",
            "✅ hellaswag_latency_b1 completed\n",
            "   Energy: 0.000104 kWh\n",
            "   Throughput: 50.98 tok/s\n",
            "   Avg TTFT: 296.73 ms\n",
            "   Memory: 2.00 GB\n",
            "\n",
            "[2/6] Profiling: mmlu_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:29:55] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:29:57] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:29:57] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:29:57] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000929 kWh\n",
            "   Net emissions (idle-corrected): 0.000257 kWh\n",
            "   (Idle contribution removed: 0.000673 kWh)\n",
            "✅ mmlu_latency_b1 completed\n",
            "   Energy: 0.000257 kWh\n",
            "   Throughput: 49.80 tok/s\n",
            "   Avg TTFT: 755.83 ms\n",
            "   Memory: 2.00 GB\n",
            "\n",
            "[3/6] Profiling: ifeval_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:31:14] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:31:15] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:31:15] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:31:15] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 30 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000896 kWh\n",
            "   Net emissions (idle-corrected): 0.000251 kWh\n",
            "   (Idle contribution removed: 0.000644 kWh)\n",
            "✅ ifeval_latency_b1 completed\n",
            "   Energy: 0.000251 kWh\n",
            "   Throughput: 51.53 tok/s\n",
            "   Avg TTFT: 2314.86 ms\n",
            "   Memory: 2.00 GB\n",
            "\n",
            "[4/6] Profiling: hellaswag_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:32:29] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:32:30] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:32:30] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:32:30] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000104 kWh\n",
            "   Net emissions (idle-corrected): 0.000028 kWh\n",
            "   (Idle contribution removed: 0.000076 kWh)\n",
            "✅ hellaswag_throughput_b8 completed\n",
            "   Energy: 0.000028 kWh\n",
            "   Throughput: 253.11 tok/s\n",
            "   Avg Batch Time: 603.41 ms\n",
            "   Memory: 2.00 GB\n",
            "\n",
            "[5/6] Profiling: mmlu_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:32:39] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:32:41] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:32:41] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:32:41] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000234 kWh\n",
            "   Net emissions (idle-corrected): 0.000065 kWh\n",
            "   (Idle contribution removed: 0.000170 kWh)\n",
            "✅ mmlu_throughput_b8 completed\n",
            "   Energy: 0.000065 kWh\n",
            "   Throughput: 270.74 tok/s\n",
            "   Avg Batch Time: 1410.28 ms\n",
            "   Memory: 2.00 GB\n",
            "\n",
            "[6/6] Profiling: ifeval_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:33:00] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:33:02] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:33:02] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:33:02] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 1 warmup iterations (out of 4 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000201 kWh\n",
            "   Net emissions (idle-corrected): 0.000051 kWh\n",
            "   (Idle contribution removed: 0.000150 kWh)\n",
            "✅ ifeval_throughput_b8 completed\n",
            "   Energy: 0.000051 kWh\n",
            "   Throughput: 274.71 tok/s\n",
            "   Avg Batch Time: 4004.19 ms\n",
            "   Memory: 2.00 GB\n",
            "\n",
            "======================================================================\n",
            "🎉 ALL WORKLOADS COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "\n",
            "✅ Run 2 completed for Llama-3.2-1B-pruned-20%\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "\n",
            "······································································\n",
            "   RUN 3/3 (seed=456)\n",
            "······································································\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Loading model: oopere/Llama-3.2-1B-pruned-20pct\n",
            "  Base: meta-llama/Llama-3.2-1B\n",
            "  Pruning: 20%\n",
            "  Star model: No (on-the-fly)\n",
            "======================================================================\n",
            "\n",
            "🔧 Creating model via on-the-fly pruning...\n",
            "✂️  Pruning with MAW method (20%)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning layers: 100%|██████████| 16/16 [00:05<00:00,  3.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model created\n",
            "   Original params: 1,235,814,400\n",
            "   Pruned params: 1,074,792,448\n",
            "   Reduction: 13.03%\n",
            "🆕 Creating new checkpoint: /content/drive/MyDrive/glu_pruning/checkpoints/1b_carbon/llama_3.2_1b_pruned_20pct_carbon_run3_seed456.json\n",
            "✅ Loaded checkpoint. Completed: 0/6 workloads\n",
            "✅ Using idle power calibration: 31.55 W\n",
            "   (Measured at: 2025-11-07T12:04:23.315788)\n",
            "\n",
            "\n",
            "🚀 Starting profiling: 6 workloads remaining\n",
            "======================================================================\n",
            "\n",
            "[1/6] Profiling: hellaswag_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:33:27] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 12:33:28] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:33:28] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:33:28] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000353 kWh\n",
            "   Net emissions (idle-corrected): 0.000099 kWh\n",
            "   (Idle contribution removed: 0.000254 kWh)\n",
            "✅ hellaswag_latency_b1 completed\n",
            "   Energy: 0.000099 kWh\n",
            "   Throughput: 50.38 tok/s\n",
            "   Avg TTFT: 285.21 ms\n",
            "   Memory: 2.00 GB\n",
            "\n",
            "[2/6] Profiling: mmlu_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:33:57] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:33:58] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:33:58] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:33:58] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000953 kWh\n",
            "   Net emissions (idle-corrected): 0.000267 kWh\n",
            "   (Idle contribution removed: 0.000686 kWh)\n",
            "✅ mmlu_latency_b1 completed\n",
            "   Energy: 0.000267 kWh\n",
            "   Throughput: 51.05 tok/s\n",
            "   Avg TTFT: 764.31 ms\n",
            "   Memory: 2.00 GB\n",
            "\n",
            "[3/6] Profiling: ifeval_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:35:17] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 12:35:18] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:35:18] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:35:18] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 30 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000743 kWh\n",
            "   Net emissions (idle-corrected): 0.000210 kWh\n",
            "   (Idle contribution removed: 0.000533 kWh)\n",
            "✅ ifeval_latency_b1 completed\n",
            "   Energy: 0.000210 kWh\n",
            "   Throughput: 51.89 tok/s\n",
            "   Avg TTFT: 1812.35 ms\n",
            "   Memory: 2.00 GB\n",
            "\n",
            "[4/6] Profiling: hellaswag_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:36:20] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:36:21] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:36:21] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:36:21] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000104 kWh\n",
            "   Net emissions (idle-corrected): 0.000028 kWh\n",
            "   (Idle contribution removed: 0.000076 kWh)\n",
            "✅ hellaswag_throughput_b8 completed\n",
            "   Energy: 0.000028 kWh\n",
            "   Throughput: 253.93 tok/s\n",
            "   Avg Batch Time: 601.46 ms\n",
            "   Memory: 2.00 GB\n",
            "\n",
            "[5/6] Profiling: mmlu_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:36:30] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:36:31] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:36:31] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:36:31] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000228 kWh\n",
            "   Net emissions (idle-corrected): 0.000062 kWh\n",
            "   (Idle contribution removed: 0.000166 kWh)\n",
            "✅ mmlu_throughput_b8 completed\n",
            "   Energy: 0.000062 kWh\n",
            "   Throughput: 275.34 tok/s\n",
            "   Avg Batch Time: 1386.70 ms\n",
            "   Memory: 2.00 GB\n",
            "\n",
            "[6/6] Profiling: ifeval_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:36:51] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:36:52] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:36:52] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:36:52] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 1 warmup iterations (out of 4 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000201 kWh\n",
            "   Net emissions (idle-corrected): 0.000051 kWh\n",
            "   (Idle contribution removed: 0.000149 kWh)\n",
            "✅ ifeval_throughput_b8 completed\n",
            "   Energy: 0.000051 kWh\n",
            "   Throughput: 270.96 tok/s\n",
            "   Avg Batch Time: 4059.58 ms\n",
            "   Memory: 2.00 GB\n",
            "\n",
            "======================================================================\n",
            "🎉 ALL WORKLOADS COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "\n",
            "✅ Run 3 completed for Llama-3.2-1B-pruned-20%\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "\n",
            "✅ All runs completed for Llama-3.2-1B-pruned-20%\n",
            "\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "🔄  PROFILING MODEL 3/6: Llama-3.2-1B-pruned-30%\n",
            "   Pruning: 30% | Star: ❌\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "\n",
            "······································································\n",
            "   RUN 1/3 (seed=42)\n",
            "······································································\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Loading model: oopere/Llama-3.2-1B-pruned-30pct\n",
            "  Base: meta-llama/Llama-3.2-1B\n",
            "  Pruning: 30%\n",
            "  Star model: No (on-the-fly)\n",
            "======================================================================\n",
            "\n",
            "🔧 Creating model via on-the-fly pruning...\n",
            "✂️  Pruning with MAW method (30%)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning layers: 100%|██████████| 16/16 [00:04<00:00,  3.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model created\n",
            "   Original params: 1,235,814,400\n",
            "   Pruned params: 994,281,472\n",
            "   Reduction: 19.54%\n",
            "\n",
            "📈  Model Statistics:\n",
            "   Parameters: 994,281,472\n",
            "   Size: 1.85 GB\n",
            "   Reduction: 19.5%\n",
            "   Source: on_the_fly_pruning\n",
            "\n",
            "🆕 Creating new checkpoint: /content/drive/MyDrive/glu_pruning/checkpoints/1b_carbon/llama_3.2_1b_pruned_30pct_carbon_run1_seed42.json\n",
            "✅ Loaded checkpoint. Completed: 0/6 workloads\n",
            "✅ Using idle power calibration: 31.55 W\n",
            "   (Measured at: 2025-11-07T12:04:23.315788)\n",
            "\n",
            "\n",
            "🚀 Starting profiling: 6 workloads remaining\n",
            "======================================================================\n",
            "\n",
            "[1/6] Profiling: hellaswag_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:37:16] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 12:37:17] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:37:17] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:37:17] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000478 kWh\n",
            "   Net emissions (idle-corrected): 0.000131 kWh\n",
            "   (Idle contribution removed: 0.000346 kWh)\n",
            "✅ hellaswag_latency_b1 completed\n",
            "   Energy: 0.000131 kWh\n",
            "   Throughput: 50.90 tok/s\n",
            "   Avg TTFT: 385.48 ms\n",
            "   Memory: 1.85 GB\n",
            "\n",
            "[2/6] Profiling: mmlu_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:37:57] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:37:58] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:37:58] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:37:58] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.001092 kWh\n",
            "   Net emissions (idle-corrected): 0.000297 kWh\n",
            "   (Idle contribution removed: 0.000795 kWh)\n",
            "✅ mmlu_latency_b1 completed\n",
            "   Energy: 0.000297 kWh\n",
            "   Throughput: 50.88 tok/s\n",
            "   Avg TTFT: 891.53 ms\n",
            "   Memory: 1.85 GB\n",
            "\n",
            "[3/6] Profiling: ifeval_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:39:30] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:39:31] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:39:31] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:39:31] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 30 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.001009 kWh\n",
            "   Net emissions (idle-corrected): 0.000275 kWh\n",
            "   (Idle contribution removed: 0.000734 kWh)\n",
            "✅ ifeval_latency_b1 completed\n",
            "   Energy: 0.000275 kWh\n",
            "   Throughput: 51.61 tok/s\n",
            "   Avg TTFT: 2843.42 ms\n",
            "   Memory: 1.85 GB\n",
            "\n",
            "[4/6] Profiling: hellaswag_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:40:55] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:40:56] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:40:56] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:40:56] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000110 kWh\n",
            "   Net emissions (idle-corrected): 0.000032 kWh\n",
            "   (Idle contribution removed: 0.000078 kWh)\n",
            "✅ hellaswag_throughput_b8 completed\n",
            "   Energy: 0.000032 kWh\n",
            "   Throughput: 248.11 tok/s\n",
            "   Avg Batch Time: 615.56 ms\n",
            "   Memory: 1.85 GB\n",
            "\n",
            "[5/6] Profiling: mmlu_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:41:06] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:41:07] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:41:07] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:41:07] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000234 kWh\n",
            "   Net emissions (idle-corrected): 0.000069 kWh\n",
            "   (Idle contribution removed: 0.000166 kWh)\n",
            "✅ mmlu_throughput_b8 completed\n",
            "   Energy: 0.000069 kWh\n",
            "   Throughput: 277.12 tok/s\n",
            "   Avg Batch Time: 1377.81 ms\n",
            "   Memory: 1.85 GB\n",
            "\n",
            "[6/6] Profiling: ifeval_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:41:26] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:41:27] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:41:27] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:41:27] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 1 warmup iterations (out of 4 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000208 kWh\n",
            "   Net emissions (idle-corrected): 0.000058 kWh\n",
            "   (Idle contribution removed: 0.000150 kWh)\n",
            "✅ ifeval_throughput_b8 completed\n",
            "   Energy: 0.000058 kWh\n",
            "   Throughput: 274.22 tok/s\n",
            "   Avg Batch Time: 4011.39 ms\n",
            "   Memory: 1.85 GB\n",
            "\n",
            "======================================================================\n",
            "🎉 ALL WORKLOADS COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "\n",
            "✅ Run 1 completed for Llama-3.2-1B-pruned-30%\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "\n",
            "······································································\n",
            "   RUN 2/3 (seed=123)\n",
            "······································································\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Loading model: oopere/Llama-3.2-1B-pruned-30pct\n",
            "  Base: meta-llama/Llama-3.2-1B\n",
            "  Pruning: 30%\n",
            "  Star model: No (on-the-fly)\n",
            "======================================================================\n",
            "\n",
            "🔧 Creating model via on-the-fly pruning...\n",
            "✂️  Pruning with MAW method (30%)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning layers: 100%|██████████| 16/16 [00:04<00:00,  3.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model created\n",
            "   Original params: 1,235,814,400\n",
            "   Pruned params: 994,281,472\n",
            "   Reduction: 19.54%\n",
            "🆕 Creating new checkpoint: /content/drive/MyDrive/glu_pruning/checkpoints/1b_carbon/llama_3.2_1b_pruned_30pct_carbon_run2_seed123.json\n",
            "✅ Loaded checkpoint. Completed: 0/6 workloads\n",
            "✅ Using idle power calibration: 31.55 W\n",
            "   (Measured at: 2025-11-07T12:04:23.315788)\n",
            "\n",
            "\n",
            "🚀 Starting profiling: 6 workloads remaining\n",
            "======================================================================\n",
            "\n",
            "[1/6] Profiling: hellaswag_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:41:52] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 12:41:53] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:41:53] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:41:53] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000489 kWh\n",
            "   Net emissions (idle-corrected): 0.000134 kWh\n",
            "   (Idle contribution removed: 0.000356 kWh)\n",
            "✅ hellaswag_latency_b1 completed\n",
            "   Energy: 0.000134 kWh\n",
            "   Throughput: 50.49 tok/s\n",
            "   Avg TTFT: 396.14 ms\n",
            "   Memory: 1.85 GB\n",
            "\n",
            "[2/6] Profiling: mmlu_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:42:34] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:42:35] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:42:35] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:42:35] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.001118 kWh\n",
            "   Net emissions (idle-corrected): 0.000305 kWh\n",
            "   (Idle contribution removed: 0.000813 kWh)\n",
            "✅ mmlu_latency_b1 completed\n",
            "   Energy: 0.000305 kWh\n",
            "   Throughput: 51.22 tok/s\n",
            "   Avg TTFT: 923.59 ms\n",
            "   Memory: 1.85 GB\n",
            "\n",
            "[3/6] Profiling: ifeval_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:44:08] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:44:09] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:44:09] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:44:09] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 30 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000992 kWh\n",
            "   Net emissions (idle-corrected): 0.000270 kWh\n",
            "   (Idle contribution removed: 0.000722 kWh)\n",
            "✅ ifeval_latency_b1 completed\n",
            "   Energy: 0.000270 kWh\n",
            "   Throughput: 51.29 tok/s\n",
            "   Avg TTFT: 2739.19 ms\n",
            "   Memory: 1.85 GB\n",
            "\n",
            "[4/6] Profiling: hellaswag_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:45:32] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:45:33] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:45:33] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:45:33] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000108 kWh\n",
            "   Net emissions (idle-corrected): 0.000031 kWh\n",
            "   (Idle contribution removed: 0.000077 kWh)\n",
            "✅ hellaswag_throughput_b8 completed\n",
            "   Energy: 0.000031 kWh\n",
            "   Throughput: 250.57 tok/s\n",
            "   Avg Batch Time: 609.53 ms\n",
            "   Memory: 1.85 GB\n",
            "\n",
            "[5/6] Profiling: mmlu_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:45:43] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:45:44] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:45:44] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:45:44] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000241 kWh\n",
            "   Net emissions (idle-corrected): 0.000071 kWh\n",
            "   (Idle contribution removed: 0.000170 kWh)\n",
            "✅ mmlu_throughput_b8 completed\n",
            "   Energy: 0.000071 kWh\n",
            "   Throughput: 271.76 tok/s\n",
            "   Avg Batch Time: 1404.98 ms\n",
            "   Memory: 1.85 GB\n",
            "\n",
            "[6/6] Profiling: ifeval_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:46:04] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:46:05] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:46:05] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:46:05] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 1 warmup iterations (out of 4 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000203 kWh\n",
            "   Net emissions (idle-corrected): 0.000055 kWh\n",
            "   (Idle contribution removed: 0.000148 kWh)\n",
            "✅ ifeval_throughput_b8 completed\n",
            "   Energy: 0.000055 kWh\n",
            "   Throughput: 276.08 tok/s\n",
            "   Avg Batch Time: 3984.39 ms\n",
            "   Memory: 1.85 GB\n",
            "\n",
            "======================================================================\n",
            "🎉 ALL WORKLOADS COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "\n",
            "✅ Run 2 completed for Llama-3.2-1B-pruned-30%\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "\n",
            "······································································\n",
            "   RUN 3/3 (seed=456)\n",
            "······································································\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Loading model: oopere/Llama-3.2-1B-pruned-30pct\n",
            "  Base: meta-llama/Llama-3.2-1B\n",
            "  Pruning: 30%\n",
            "  Star model: No (on-the-fly)\n",
            "======================================================================\n",
            "\n",
            "🔧 Creating model via on-the-fly pruning...\n",
            "✂️  Pruning with MAW method (30%)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning layers: 100%|██████████| 16/16 [00:04<00:00,  3.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model created\n",
            "   Original params: 1,235,814,400\n",
            "   Pruned params: 994,281,472\n",
            "   Reduction: 19.54%\n",
            "🆕 Creating new checkpoint: /content/drive/MyDrive/glu_pruning/checkpoints/1b_carbon/llama_3.2_1b_pruned_30pct_carbon_run3_seed456.json\n",
            "✅ Loaded checkpoint. Completed: 0/6 workloads\n",
            "✅ Using idle power calibration: 31.55 W\n",
            "   (Measured at: 2025-11-07T12:04:23.315788)\n",
            "\n",
            "\n",
            "🚀 Starting profiling: 6 workloads remaining\n",
            "======================================================================\n",
            "\n",
            "[1/6] Profiling: hellaswag_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:46:29] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 12:46:30] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:46:30] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:46:30] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000481 kWh\n",
            "   Net emissions (idle-corrected): 0.000125 kWh\n",
            "   (Idle contribution removed: 0.000356 kWh)\n",
            "✅ hellaswag_latency_b1 completed\n",
            "   Energy: 0.000125 kWh\n",
            "   Throughput: 50.42 tok/s\n",
            "   Avg TTFT: 395.80 ms\n",
            "   Memory: 1.85 GB\n",
            "\n",
            "[2/6] Profiling: mmlu_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:47:11] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:47:12] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:47:12] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:47:12] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.001084 kWh\n",
            "   Net emissions (idle-corrected): 0.000277 kWh\n",
            "   (Idle contribution removed: 0.000806 kWh)\n",
            "✅ mmlu_latency_b1 completed\n",
            "   Energy: 0.000277 kWh\n",
            "   Throughput: 50.58 tok/s\n",
            "   Avg TTFT: 906.21 ms\n",
            "   Memory: 1.85 GB\n",
            "\n",
            "[3/6] Profiling: ifeval_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:48:45] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:48:46] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:48:46] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:48:46] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 30 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000867 kWh\n",
            "   Net emissions (idle-corrected): 0.000220 kWh\n",
            "   (Idle contribution removed: 0.000648 kWh)\n",
            "✅ ifeval_latency_b1 completed\n",
            "   Energy: 0.000220 kWh\n",
            "   Throughput: 51.30 tok/s\n",
            "   Avg TTFT: 2329.82 ms\n",
            "   Memory: 1.85 GB\n",
            "\n",
            "[4/6] Profiling: hellaswag_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:50:01] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:50:02] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:50:02] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:50:02] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000108 kWh\n",
            "   Net emissions (idle-corrected): 0.000030 kWh\n",
            "   (Idle contribution removed: 0.000078 kWh)\n",
            "✅ hellaswag_throughput_b8 completed\n",
            "   Energy: 0.000030 kWh\n",
            "   Throughput: 245.37 tok/s\n",
            "   Avg Batch Time: 622.43 ms\n",
            "   Memory: 1.85 GB\n",
            "\n",
            "[5/6] Profiling: mmlu_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:50:11] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:50:12] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:50:12] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:50:12] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000234 kWh\n",
            "   Net emissions (idle-corrected): 0.000066 kWh\n",
            "   (Idle contribution removed: 0.000168 kWh)\n",
            "✅ mmlu_throughput_b8 completed\n",
            "   Energy: 0.000066 kWh\n",
            "   Throughput: 271.15 tok/s\n",
            "   Avg Batch Time: 1408.16 ms\n",
            "   Memory: 1.85 GB\n",
            "\n",
            "[6/6] Profiling: ifeval_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:50:32] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:50:33] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:50:33] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:50:33] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 1 warmup iterations (out of 4 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000201 kWh\n",
            "   Net emissions (idle-corrected): 0.000053 kWh\n",
            "   (Idle contribution removed: 0.000149 kWh)\n",
            "✅ ifeval_throughput_b8 completed\n",
            "   Energy: 0.000053 kWh\n",
            "   Throughput: 275.01 tok/s\n",
            "   Avg Batch Time: 3999.83 ms\n",
            "   Memory: 1.85 GB\n",
            "\n",
            "======================================================================\n",
            "🎉 ALL WORKLOADS COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "\n",
            "✅ Run 3 completed for Llama-3.2-1B-pruned-30%\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "\n",
            "✅ All runs completed for Llama-3.2-1B-pruned-30%\n",
            "\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "🔄  PROFILING MODEL 4/6: Llama-3.2-1B-pruned-40%\n",
            "   Pruning: 40% | Star: ⭐ \n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "\n",
            "······································································\n",
            "   RUN 1/3 (seed=42)\n",
            "······································································\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Loading model: oopere/Llama-3.2-1B-pruned-40pct\n",
            "  Base: meta-llama/Llama-3.2-1B\n",
            "  Pruning: 40%\n",
            "  Star model: ⭐ Yes\n",
            "======================================================================\n",
            "\n",
            "📥 Attempting to load from HF Hub: oopere/Llama-3.2-1B-pruned-40pct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/884 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7589d3e0a93b410db5231c697566a242"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.83G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "baced79b6bcb4edfbf9f4370d4710963"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/180 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b5bdbb155014df2aad77fd1bae41379"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded from HF Hub\n",
            "\n",
            "📈  Model Statistics:\n",
            "   Parameters: 913,770,496\n",
            "   Size: 1.70 GB\n",
            "   Source: hf_hub\n",
            "\n",
            "🆕 Creating new checkpoint: /content/drive/MyDrive/glu_pruning/checkpoints/1b_carbon/llama_3.2_1b_pruned_40pct_carbon_run1_seed42.json\n",
            "✅ Loaded checkpoint. Completed: 0/6 workloads\n",
            "✅ Using idle power calibration: 31.55 W\n",
            "   (Measured at: 2025-11-07T12:04:23.315788)\n",
            "\n",
            "\n",
            "🚀 Starting profiling: 6 workloads remaining\n",
            "======================================================================\n",
            "\n",
            "[1/6] Profiling: hellaswag_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:50:59] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:51:00] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:51:00] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:51:00] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000468 kWh\n",
            "   Net emissions (idle-corrected): 0.000112 kWh\n",
            "   (Idle contribution removed: 0.000355 kWh)\n",
            "✅ hellaswag_latency_b1 completed\n",
            "   Energy: 0.000112 kWh\n",
            "   Throughput: 50.68 tok/s\n",
            "   Avg TTFT: 394.64 ms\n",
            "   Memory: 1.70 GB\n",
            "\n",
            "[2/6] Profiling: mmlu_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:51:41] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 12:51:42] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:51:42] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:51:42] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.001109 kWh\n",
            "   Net emissions (idle-corrected): 0.000269 kWh\n",
            "   (Idle contribution removed: 0.000840 kWh)\n",
            "✅ mmlu_latency_b1 completed\n",
            "   Energy: 0.000269 kWh\n",
            "   Throughput: 51.16 tok/s\n",
            "   Avg TTFT: 947.09 ms\n",
            "   Memory: 1.70 GB\n",
            "\n",
            "[3/6] Profiling: ifeval_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:53:18] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:53:20] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:53:20] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:53:20] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 30 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000901 kWh\n",
            "   Net emissions (idle-corrected): 0.000238 kWh\n",
            "   (Idle contribution removed: 0.000663 kWh)\n",
            "✅ ifeval_latency_b1 completed\n",
            "   Energy: 0.000238 kWh\n",
            "   Throughput: 51.86 tok/s\n",
            "   Avg TTFT: 2520.55 ms\n",
            "   Memory: 1.70 GB\n",
            "\n",
            "[4/6] Profiling: hellaswag_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:54:36] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:54:37] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:54:37] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:54:37] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000099 kWh\n",
            "   Net emissions (idle-corrected): 0.000025 kWh\n",
            "   (Idle contribution removed: 0.000074 kWh)\n",
            "✅ hellaswag_throughput_b8 completed\n",
            "   Energy: 0.000025 kWh\n",
            "   Throughput: 258.79 tok/s\n",
            "   Avg Batch Time: 590.16 ms\n",
            "   Memory: 1.70 GB\n",
            "\n",
            "[5/6] Profiling: mmlu_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:54:46] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:54:47] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:54:47] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:54:47] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000219 kWh\n",
            "   Net emissions (idle-corrected): 0.000054 kWh\n",
            "   (Idle contribution removed: 0.000165 kWh)\n",
            "✅ mmlu_throughput_b8 completed\n",
            "   Energy: 0.000054 kWh\n",
            "   Throughput: 277.12 tok/s\n",
            "   Avg Batch Time: 1377.80 ms\n",
            "   Memory: 1.70 GB\n",
            "\n",
            "[6/6] Profiling: ifeval_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:55:06] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:55:08] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:55:08] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:55:08] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 1 warmup iterations (out of 4 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000201 kWh\n",
            "   Net emissions (idle-corrected): 0.000047 kWh\n",
            "   (Idle contribution removed: 0.000153 kWh)\n",
            "✅ ifeval_throughput_b8 completed\n",
            "   Energy: 0.000047 kWh\n",
            "   Throughput: 272.28 tok/s\n",
            "   Avg Batch Time: 4039.91 ms\n",
            "   Memory: 1.70 GB\n",
            "\n",
            "======================================================================\n",
            "🎉 ALL WORKLOADS COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "\n",
            "✅ Run 1 completed for Llama-3.2-1B-pruned-40%\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "\n",
            "······································································\n",
            "   RUN 2/3 (seed=123)\n",
            "······································································\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Loading model: oopere/Llama-3.2-1B-pruned-40pct\n",
            "  Base: meta-llama/Llama-3.2-1B\n",
            "  Pruning: 40%\n",
            "  Star model: ⭐ Yes\n",
            "======================================================================\n",
            "\n",
            "📥 Attempting to load from HF Hub: oopere/Llama-3.2-1B-pruned-40pct\n",
            "✅ Loaded from HF Hub\n",
            "🆕 Creating new checkpoint: /content/drive/MyDrive/glu_pruning/checkpoints/1b_carbon/llama_3.2_1b_pruned_40pct_carbon_run2_seed123.json\n",
            "✅ Loaded checkpoint. Completed: 0/6 workloads\n",
            "✅ Using idle power calibration: 31.55 W\n",
            "   (Measured at: 2025-11-07T12:04:23.315788)\n",
            "\n",
            "\n",
            "🚀 Starting profiling: 6 workloads remaining\n",
            "======================================================================\n",
            "\n",
            "[1/6] Profiling: hellaswag_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:55:28] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:55:29] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:55:29] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:55:29] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000489 kWh\n",
            "   Net emissions (idle-corrected): 0.000130 kWh\n",
            "   (Idle contribution removed: 0.000359 kWh)\n",
            "✅ hellaswag_latency_b1 completed\n",
            "   Energy: 0.000130 kWh\n",
            "   Throughput: 50.05 tok/s\n",
            "   Avg TTFT: 398.74 ms\n",
            "   Memory: 1.70 GB\n",
            "\n",
            "[2/6] Profiling: mmlu_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:56:10] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:56:11] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:56:11] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:56:11] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.001093 kWh\n",
            "   Net emissions (idle-corrected): 0.000283 kWh\n",
            "   (Idle contribution removed: 0.000810 kWh)\n",
            "✅ mmlu_latency_b1 completed\n",
            "   Energy: 0.000283 kWh\n",
            "   Throughput: 50.51 tok/s\n",
            "   Avg TTFT: 909.63 ms\n",
            "   Memory: 1.70 GB\n",
            "\n",
            "[3/6] Profiling: ifeval_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:57:44] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:57:46] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:57:46] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:57:46] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 30 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.001051 kWh\n",
            "   Net emissions (idle-corrected): 0.000274 kWh\n",
            "   (Idle contribution removed: 0.000776 kWh)\n",
            "✅ ifeval_latency_b1 completed\n",
            "   Energy: 0.000274 kWh\n",
            "   Throughput: 51.41 tok/s\n",
            "   Avg TTFT: 2917.82 ms\n",
            "   Memory: 1.70 GB\n",
            "\n",
            "[4/6] Profiling: hellaswag_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:59:15] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:59:16] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:59:16] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:59:16] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000100 kWh\n",
            "   Net emissions (idle-corrected): 0.000025 kWh\n",
            "   (Idle contribution removed: 0.000075 kWh)\n",
            "✅ hellaswag_throughput_b8 completed\n",
            "   Energy: 0.000025 kWh\n",
            "   Throughput: 257.58 tok/s\n",
            "   Avg Batch Time: 592.92 ms\n",
            "   Memory: 1.70 GB\n",
            "\n",
            "[5/6] Profiling: mmlu_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:59:25] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 12:59:26] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:59:26] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:59:26] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000228 kWh\n",
            "   Net emissions (idle-corrected): 0.000059 kWh\n",
            "   (Idle contribution removed: 0.000169 kWh)\n",
            "✅ mmlu_throughput_b8 completed\n",
            "   Energy: 0.000059 kWh\n",
            "   Throughput: 272.03 tok/s\n",
            "   Avg Batch Time: 1403.57 ms\n",
            "   Memory: 1.70 GB\n",
            "\n",
            "[6/6] Profiling: ifeval_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:59:46] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 12:59:47] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 12:59:47] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 12:59:47] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 1 warmup iterations (out of 4 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000196 kWh\n",
            "   Net emissions (idle-corrected): 0.000045 kWh\n",
            "   (Idle contribution removed: 0.000151 kWh)\n",
            "✅ ifeval_throughput_b8 completed\n",
            "   Energy: 0.000045 kWh\n",
            "   Throughput: 270.30 tok/s\n",
            "   Avg Batch Time: 4069.53 ms\n",
            "   Memory: 1.70 GB\n",
            "\n",
            "======================================================================\n",
            "🎉 ALL WORKLOADS COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "\n",
            "✅ Run 2 completed for Llama-3.2-1B-pruned-40%\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "\n",
            "······································································\n",
            "   RUN 3/3 (seed=456)\n",
            "······································································\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Loading model: oopere/Llama-3.2-1B-pruned-40pct\n",
            "  Base: meta-llama/Llama-3.2-1B\n",
            "  Pruning: 40%\n",
            "  Star model: ⭐ Yes\n",
            "======================================================================\n",
            "\n",
            "📥 Attempting to load from HF Hub: oopere/Llama-3.2-1B-pruned-40pct\n",
            "✅ Loaded from HF Hub\n",
            "🆕 Creating new checkpoint: /content/drive/MyDrive/glu_pruning/checkpoints/1b_carbon/llama_3.2_1b_pruned_40pct_carbon_run3_seed456.json\n",
            "✅ Loaded checkpoint. Completed: 0/6 workloads\n",
            "✅ Using idle power calibration: 31.55 W\n",
            "   (Measured at: 2025-11-07T12:04:23.315788)\n",
            "\n",
            "\n",
            "🚀 Starting profiling: 6 workloads remaining\n",
            "======================================================================\n",
            "\n",
            "[1/6] Profiling: hellaswag_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:00:07] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:00:08] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:00:08] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:00:08] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000479 kWh\n",
            "   Net emissions (idle-corrected): 0.000121 kWh\n",
            "   (Idle contribution removed: 0.000359 kWh)\n",
            "✅ hellaswag_latency_b1 completed\n",
            "   Energy: 0.000121 kWh\n",
            "   Throughput: 50.06 tok/s\n",
            "   Avg TTFT: 399.53 ms\n",
            "   Memory: 1.70 GB\n",
            "\n",
            "[2/6] Profiling: mmlu_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:00:49] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:00:51] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:00:51] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:00:51] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.001104 kWh\n",
            "   Net emissions (idle-corrected): 0.000277 kWh\n",
            "   (Idle contribution removed: 0.000828 kWh)\n",
            "✅ mmlu_latency_b1 completed\n",
            "   Energy: 0.000277 kWh\n",
            "   Throughput: 50.58 tok/s\n",
            "   Avg TTFT: 931.58 ms\n",
            "   Memory: 1.70 GB\n",
            "\n",
            "[3/6] Profiling: ifeval_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:02:26] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:02:27] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:02:27] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:02:27] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 30 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000882 kWh\n",
            "   Net emissions (idle-corrected): 0.000219 kWh\n",
            "   (Idle contribution removed: 0.000663 kWh)\n",
            "✅ ifeval_latency_b1 completed\n",
            "   Energy: 0.000219 kWh\n",
            "   Throughput: 50.93 tok/s\n",
            "   Avg TTFT: 2468.45 ms\n",
            "   Memory: 1.70 GB\n",
            "\n",
            "[4/6] Profiling: hellaswag_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:03:43] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:03:44] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:03:44] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:03:44] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000100 kWh\n",
            "   Net emissions (idle-corrected): 0.000025 kWh\n",
            "   (Idle contribution removed: 0.000075 kWh)\n",
            "✅ hellaswag_throughput_b8 completed\n",
            "   Energy: 0.000025 kWh\n",
            "   Throughput: 257.58 tok/s\n",
            "   Avg Batch Time: 592.92 ms\n",
            "   Memory: 1.70 GB\n",
            "\n",
            "[5/6] Profiling: mmlu_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:03:53] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:03:54] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:03:54] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:03:54] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000222 kWh\n",
            "   Net emissions (idle-corrected): 0.000055 kWh\n",
            "   (Idle contribution removed: 0.000167 kWh)\n",
            "✅ mmlu_throughput_b8 completed\n",
            "   Energy: 0.000055 kWh\n",
            "   Throughput: 274.29 tok/s\n",
            "   Avg Batch Time: 1392.00 ms\n",
            "   Memory: 1.70 GB\n",
            "\n",
            "[6/6] Profiling: ifeval_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:04:14] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:04:15] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:04:15] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:04:15] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 1 warmup iterations (out of 4 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000194 kWh\n",
            "   Net emissions (idle-corrected): 0.000046 kWh\n",
            "   (Idle contribution removed: 0.000148 kWh)\n",
            "✅ ifeval_throughput_b8 completed\n",
            "   Energy: 0.000046 kWh\n",
            "   Throughput: 277.09 tok/s\n",
            "   Avg Batch Time: 3969.85 ms\n",
            "   Memory: 1.70 GB\n",
            "\n",
            "======================================================================\n",
            "🎉 ALL WORKLOADS COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "\n",
            "✅ Run 3 completed for Llama-3.2-1B-pruned-40%\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "\n",
            "✅ All runs completed for Llama-3.2-1B-pruned-40%\n",
            "\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "🔄  PROFILING MODEL 5/6: Llama-3.2-1B-pruned-50%\n",
            "   Pruning: 50% | Star: ❌\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "\n",
            "······································································\n",
            "   RUN 1/3 (seed=42)\n",
            "······································································\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Loading model: oopere/Llama-3.2-1B-pruned-50pct\n",
            "  Base: meta-llama/Llama-3.2-1B\n",
            "  Pruning: 50%\n",
            "  Star model: No (on-the-fly)\n",
            "======================================================================\n",
            "\n",
            "🔧 Creating model via on-the-fly pruning...\n",
            "✂️  Pruning with MAW method (50%)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning layers: 100%|██████████| 16/16 [00:03<00:00,  4.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model created\n",
            "   Original params: 1,235,814,400\n",
            "   Pruned params: 833,161,216\n",
            "   Reduction: 32.58%\n",
            "\n",
            "📈  Model Statistics:\n",
            "   Parameters: 833,161,216\n",
            "   Size: 1.55 GB\n",
            "   Reduction: 32.6%\n",
            "   Source: on_the_fly_pruning\n",
            "\n",
            "🆕 Creating new checkpoint: /content/drive/MyDrive/glu_pruning/checkpoints/1b_carbon/llama_3.2_1b_pruned_50pct_carbon_run1_seed42.json\n",
            "✅ Loaded checkpoint. Completed: 0/6 workloads\n",
            "✅ Using idle power calibration: 31.55 W\n",
            "   (Measured at: 2025-11-07T12:04:23.315788)\n",
            "\n",
            "\n",
            "🚀 Starting profiling: 6 workloads remaining\n",
            "======================================================================\n",
            "\n",
            "[1/6] Profiling: hellaswag_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:04:38] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 13:04:39] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:04:39] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:04:39] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000474 kWh\n",
            "   Net emissions (idle-corrected): 0.000122 kWh\n",
            "   (Idle contribution removed: 0.000352 kWh)\n",
            "✅ hellaswag_latency_b1 completed\n",
            "   Energy: 0.000122 kWh\n",
            "   Throughput: 51.05 tok/s\n",
            "   Avg TTFT: 391.76 ms\n",
            "   Memory: 1.55 GB\n",
            "\n",
            "[2/6] Profiling: mmlu_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:05:19] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 13:05:21] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:05:21] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:05:21] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.001135 kWh\n",
            "   Net emissions (idle-corrected): 0.000285 kWh\n",
            "   (Idle contribution removed: 0.000850 kWh)\n",
            "✅ mmlu_latency_b1 completed\n",
            "   Energy: 0.000285 kWh\n",
            "   Throughput: 51.58 tok/s\n",
            "   Avg TTFT: 959.43 ms\n",
            "   Memory: 1.55 GB\n",
            "\n",
            "[3/6] Profiling: ifeval_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:06:58] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:06:59] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:06:59] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:06:59] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 30 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000993 kWh\n",
            "   Net emissions (idle-corrected): 0.000251 kWh\n",
            "   (Idle contribution removed: 0.000742 kWh)\n",
            "✅ ifeval_latency_b1 completed\n",
            "   Energy: 0.000251 kWh\n",
            "   Throughput: 52.00 tok/s\n",
            "   Avg TTFT: 2884.55 ms\n",
            "   Memory: 1.55 GB\n",
            "\n",
            "[4/6] Profiling: hellaswag_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:08:24] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:08:26] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:08:26] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:08:26] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000098 kWh\n",
            "   Net emissions (idle-corrected): 0.000023 kWh\n",
            "   (Idle contribution removed: 0.000074 kWh)\n",
            "✅ hellaswag_throughput_b8 completed\n",
            "   Energy: 0.000023 kWh\n",
            "   Throughput: 264.39 tok/s\n",
            "   Avg Batch Time: 577.65 ms\n",
            "   Memory: 1.55 GB\n",
            "\n",
            "[5/6] Profiling: mmlu_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:08:35] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 13:08:36] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:08:36] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:08:36] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000216 kWh\n",
            "   Net emissions (idle-corrected): 0.000051 kWh\n",
            "   (Idle contribution removed: 0.000165 kWh)\n",
            "✅ mmlu_throughput_b8 completed\n",
            "   Energy: 0.000051 kWh\n",
            "   Throughput: 281.00 tok/s\n",
            "   Avg Batch Time: 1358.79 ms\n",
            "   Memory: 1.55 GB\n",
            "\n",
            "[6/6] Profiling: ifeval_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:08:55] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:08:56] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:08:56] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:08:56] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 1 warmup iterations (out of 4 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000192 kWh\n",
            "   Net emissions (idle-corrected): 0.000044 kWh\n",
            "   (Idle contribution removed: 0.000148 kWh)\n",
            "✅ ifeval_throughput_b8 completed\n",
            "   Energy: 0.000044 kWh\n",
            "   Throughput: 276.01 tok/s\n",
            "   Avg Batch Time: 3985.43 ms\n",
            "   Memory: 1.55 GB\n",
            "\n",
            "======================================================================\n",
            "🎉 ALL WORKLOADS COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "\n",
            "✅ Run 1 completed for Llama-3.2-1B-pruned-50%\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "\n",
            "······································································\n",
            "   RUN 2/3 (seed=123)\n",
            "······································································\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Loading model: oopere/Llama-3.2-1B-pruned-50pct\n",
            "  Base: meta-llama/Llama-3.2-1B\n",
            "  Pruning: 50%\n",
            "  Star model: No (on-the-fly)\n",
            "======================================================================\n",
            "\n",
            "🔧 Creating model via on-the-fly pruning...\n",
            "✂️  Pruning with MAW method (50%)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning layers: 100%|██████████| 16/16 [00:03<00:00,  4.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model created\n",
            "   Original params: 1,235,814,400\n",
            "   Pruned params: 833,161,216\n",
            "   Reduction: 32.58%\n",
            "🆕 Creating new checkpoint: /content/drive/MyDrive/glu_pruning/checkpoints/1b_carbon/llama_3.2_1b_pruned_50pct_carbon_run2_seed123.json\n",
            "✅ Loaded checkpoint. Completed: 0/6 workloads\n",
            "✅ Using idle power calibration: 31.55 W\n",
            "   (Measured at: 2025-11-07T12:04:23.315788)\n",
            "\n",
            "\n",
            "🚀 Starting profiling: 6 workloads remaining\n",
            "======================================================================\n",
            "\n",
            "[1/6] Profiling: hellaswag_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:09:19] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 13:09:20] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:09:20] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:09:20] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000472 kWh\n",
            "   Net emissions (idle-corrected): 0.000121 kWh\n",
            "   (Idle contribution removed: 0.000351 kWh)\n",
            "✅ hellaswag_latency_b1 completed\n",
            "   Energy: 0.000121 kWh\n",
            "   Throughput: 51.03 tok/s\n",
            "   Avg TTFT: 391.94 ms\n",
            "   Memory: 1.55 GB\n",
            "\n",
            "[2/6] Profiling: mmlu_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:10:01] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 13:10:02] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:10:02] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:10:02] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.001137 kWh\n",
            "   Net emissions (idle-corrected): 0.000284 kWh\n",
            "   (Idle contribution removed: 0.000852 kWh)\n",
            "✅ mmlu_latency_b1 completed\n",
            "   Energy: 0.000284 kWh\n",
            "   Throughput: 51.47 tok/s\n",
            "   Avg TTFT: 961.51 ms\n",
            "   Memory: 1.55 GB\n",
            "\n",
            "[3/6] Profiling: ifeval_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:11:40] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:11:41] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:11:41] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:11:41] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 30 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.001022 kWh\n",
            "   Net emissions (idle-corrected): 0.000258 kWh\n",
            "   (Idle contribution removed: 0.000764 kWh)\n",
            "✅ ifeval_latency_b1 completed\n",
            "   Energy: 0.000258 kWh\n",
            "   Throughput: 52.05 tok/s\n",
            "   Avg TTFT: 2871.20 ms\n",
            "   Memory: 1.55 GB\n",
            "\n",
            "[4/6] Profiling: hellaswag_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:13:09] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:13:10] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:13:10] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:13:10] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000095 kWh\n",
            "   Net emissions (idle-corrected): 0.000023 kWh\n",
            "   (Idle contribution removed: 0.000072 kWh)\n",
            "✅ hellaswag_throughput_b8 completed\n",
            "   Energy: 0.000023 kWh\n",
            "   Throughput: 272.69 tok/s\n",
            "   Avg Batch Time: 560.08 ms\n",
            "   Memory: 1.55 GB\n",
            "\n",
            "[5/6] Profiling: mmlu_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:13:19] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:13:20] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:13:20] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:13:20] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000217 kWh\n",
            "   Net emissions (idle-corrected): 0.000055 kWh\n",
            "   (Idle contribution removed: 0.000163 kWh)\n",
            "✅ mmlu_throughput_b8 completed\n",
            "   Energy: 0.000055 kWh\n",
            "   Throughput: 282.88 tok/s\n",
            "   Avg Batch Time: 1349.78 ms\n",
            "   Memory: 1.55 GB\n",
            "\n",
            "[6/6] Profiling: ifeval_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:13:39] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:13:40] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:13:40] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:13:40] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 1 warmup iterations (out of 4 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000190 kWh\n",
            "   Net emissions (idle-corrected): 0.000043 kWh\n",
            "   (Idle contribution removed: 0.000147 kWh)\n",
            "✅ ifeval_throughput_b8 completed\n",
            "   Energy: 0.000043 kWh\n",
            "   Throughput: 277.64 tok/s\n",
            "   Avg Batch Time: 3961.92 ms\n",
            "   Memory: 1.55 GB\n",
            "\n",
            "======================================================================\n",
            "🎉 ALL WORKLOADS COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "\n",
            "✅ Run 2 completed for Llama-3.2-1B-pruned-50%\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "\n",
            "······································································\n",
            "   RUN 3/3 (seed=456)\n",
            "······································································\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Loading model: oopere/Llama-3.2-1B-pruned-50pct\n",
            "  Base: meta-llama/Llama-3.2-1B\n",
            "  Pruning: 50%\n",
            "  Star model: No (on-the-fly)\n",
            "======================================================================\n",
            "\n",
            "🔧 Creating model via on-the-fly pruning...\n",
            "✂️  Pruning with MAW method (50%)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning layers: 100%|██████████| 16/16 [00:03<00:00,  4.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model created\n",
            "   Original params: 1,235,814,400\n",
            "   Pruned params: 833,161,216\n",
            "   Reduction: 32.58%\n",
            "🆕 Creating new checkpoint: /content/drive/MyDrive/glu_pruning/checkpoints/1b_carbon/llama_3.2_1b_pruned_50pct_carbon_run3_seed456.json\n",
            "✅ Loaded checkpoint. Completed: 0/6 workloads\n",
            "✅ Using idle power calibration: 31.55 W\n",
            "   (Measured at: 2025-11-07T12:04:23.315788)\n",
            "\n",
            "\n",
            "🚀 Starting profiling: 6 workloads remaining\n",
            "======================================================================\n",
            "\n",
            "[1/6] Profiling: hellaswag_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:14:03] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 13:14:04] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:14:04] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:14:04] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000469 kWh\n",
            "   Net emissions (idle-corrected): 0.000119 kWh\n",
            "   (Idle contribution removed: 0.000350 kWh)\n",
            "✅ hellaswag_latency_b1 completed\n",
            "   Energy: 0.000119 kWh\n",
            "   Throughput: 51.28 tok/s\n",
            "   Avg TTFT: 390.00 ms\n",
            "   Memory: 1.55 GB\n",
            "\n",
            "[2/6] Profiling: mmlu_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:14:44] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:14:45] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:14:45] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:14:45] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.001140 kWh\n",
            "   Net emissions (idle-corrected): 0.000286 kWh\n",
            "   (Idle contribution removed: 0.000854 kWh)\n",
            "✅ mmlu_latency_b1 completed\n",
            "   Energy: 0.000286 kWh\n",
            "   Throughput: 51.35 tok/s\n",
            "   Avg TTFT: 963.64 ms\n",
            "   Memory: 1.55 GB\n",
            "\n",
            "[3/6] Profiling: ifeval_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:16:23] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 13:16:24] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:16:24] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:16:24] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 30 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000999 kWh\n",
            "   Net emissions (idle-corrected): 0.000251 kWh\n",
            "   (Idle contribution removed: 0.000748 kWh)\n",
            "✅ ifeval_latency_b1 completed\n",
            "   Energy: 0.000251 kWh\n",
            "   Throughput: 51.55 tok/s\n",
            "   Avg TTFT: 2794.13 ms\n",
            "   Memory: 1.55 GB\n",
            "\n",
            "[4/6] Profiling: hellaswag_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:17:50] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:17:51] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:17:51] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:17:51] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000096 kWh\n",
            "   Net emissions (idle-corrected): 0.000023 kWh\n",
            "   (Idle contribution removed: 0.000073 kWh)\n",
            "✅ hellaswag_throughput_b8 completed\n",
            "   Energy: 0.000023 kWh\n",
            "   Throughput: 266.39 tok/s\n",
            "   Avg Batch Time: 573.32 ms\n",
            "   Memory: 1.55 GB\n",
            "\n",
            "[5/6] Profiling: mmlu_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:18:00] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:18:01] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:18:01] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:18:01] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000216 kWh\n",
            "   Net emissions (idle-corrected): 0.000052 kWh\n",
            "   (Idle contribution removed: 0.000164 kWh)\n",
            "✅ mmlu_throughput_b8 completed\n",
            "   Energy: 0.000052 kWh\n",
            "   Throughput: 278.78 tok/s\n",
            "   Avg Batch Time: 1369.60 ms\n",
            "   Memory: 1.55 GB\n",
            "\n",
            "[6/6] Profiling: ifeval_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:18:21] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon WARNING @ 13:18:22] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:18:22] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:18:22] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 1 warmup iterations (out of 4 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000190 kWh\n",
            "   Net emissions (idle-corrected): 0.000043 kWh\n",
            "   (Idle contribution removed: 0.000148 kWh)\n",
            "✅ ifeval_throughput_b8 completed\n",
            "   Energy: 0.000043 kWh\n",
            "   Throughput: 275.61 tok/s\n",
            "   Avg Batch Time: 3991.22 ms\n",
            "   Memory: 1.55 GB\n",
            "\n",
            "======================================================================\n",
            "🎉 ALL WORKLOADS COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "\n",
            "✅ Run 3 completed for Llama-3.2-1B-pruned-50%\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "\n",
            "✅ All runs completed for Llama-3.2-1B-pruned-50%\n",
            "\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "🔄  PROFILING MODEL 6/6: Llama-3.2-1B-pruned-60%\n",
            "   Pruning: 60% | Star: ❌\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "\n",
            "······································································\n",
            "   RUN 1/3 (seed=42)\n",
            "······································································\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Loading model: oopere/Llama-3.2-1B-pruned-60pct\n",
            "  Base: meta-llama/Llama-3.2-1B\n",
            "  Pruning: 60%\n",
            "  Star model: No (on-the-fly)\n",
            "======================================================================\n",
            "\n",
            "🔧 Creating model via on-the-fly pruning...\n",
            "✂️  Pruning with MAW method (60%)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning layers: 100%|██████████| 16/16 [00:02<00:00,  6.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model created\n",
            "   Original params: 1,235,814,400\n",
            "   Pruned params: 752,650,240\n",
            "   Reduction: 39.10%\n",
            "\n",
            "📈  Model Statistics:\n",
            "   Parameters: 752,650,240\n",
            "   Size: 1.40 GB\n",
            "   Reduction: 39.1%\n",
            "   Source: on_the_fly_pruning\n",
            "\n",
            "🆕 Creating new checkpoint: /content/drive/MyDrive/glu_pruning/checkpoints/1b_carbon/llama_3.2_1b_pruned_60pct_carbon_run1_seed42.json\n",
            "✅ Loaded checkpoint. Completed: 0/6 workloads\n",
            "✅ Using idle power calibration: 31.55 W\n",
            "   (Measured at: 2025-11-07T12:04:23.315788)\n",
            "\n",
            "\n",
            "🚀 Starting profiling: 6 workloads remaining\n",
            "======================================================================\n",
            "\n",
            "[1/6] Profiling: hellaswag_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:18:44] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:18:45] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:18:45] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:18:45] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000461 kWh\n",
            "   Net emissions (idle-corrected): 0.000113 kWh\n",
            "   (Idle contribution removed: 0.000348 kWh)\n",
            "✅ hellaswag_latency_b1 completed\n",
            "   Energy: 0.000113 kWh\n",
            "   Throughput: 51.34 tok/s\n",
            "   Avg TTFT: 387.34 ms\n",
            "   Memory: 1.40 GB\n",
            "\n",
            "[2/6] Profiling: mmlu_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:19:26] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:19:27] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:19:27] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:19:27] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.001133 kWh\n",
            "   Net emissions (idle-corrected): 0.000271 kWh\n",
            "   (Idle contribution removed: 0.000861 kWh)\n",
            "✅ mmlu_latency_b1 completed\n",
            "   Energy: 0.000271 kWh\n",
            "   Throughput: 51.40 tok/s\n",
            "   Avg TTFT: 972.85 ms\n",
            "   Memory: 1.40 GB\n",
            "\n",
            "[3/6] Profiling: ifeval_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:21:06] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:21:07] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:21:07] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:21:07] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 30 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000978 kWh\n",
            "   Net emissions (idle-corrected): 0.000236 kWh\n",
            "   (Idle contribution removed: 0.000742 kWh)\n",
            "✅ ifeval_latency_b1 completed\n",
            "   Energy: 0.000236 kWh\n",
            "   Throughput: 51.98 tok/s\n",
            "   Avg TTFT: 2885.79 ms\n",
            "   Memory: 1.40 GB\n",
            "\n",
            "[4/6] Profiling: hellaswag_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:22:32] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:22:33] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:22:33] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:22:33] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000100 kWh\n",
            "   Net emissions (idle-corrected): 0.000026 kWh\n",
            "   (Idle contribution removed: 0.000074 kWh)\n",
            "✅ hellaswag_throughput_b8 completed\n",
            "   Energy: 0.000026 kWh\n",
            "   Throughput: 264.47 tok/s\n",
            "   Avg Batch Time: 577.49 ms\n",
            "   Memory: 1.40 GB\n",
            "\n",
            "[5/6] Profiling: mmlu_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:22:42] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:22:43] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:22:43] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:22:43] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000217 kWh\n",
            "   Net emissions (idle-corrected): 0.000056 kWh\n",
            "   (Idle contribution removed: 0.000161 kWh)\n",
            "✅ mmlu_throughput_b8 completed\n",
            "   Energy: 0.000056 kWh\n",
            "   Throughput: 283.68 tok/s\n",
            "   Avg Batch Time: 1345.94 ms\n",
            "   Memory: 1.40 GB\n",
            "\n",
            "[6/6] Profiling: ifeval_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:23:02] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:23:03] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:23:03] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:23:03] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 42 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 1 warmup iterations (out of 4 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000196 kWh\n",
            "   Net emissions (idle-corrected): 0.000048 kWh\n",
            "   (Idle contribution removed: 0.000148 kWh)\n",
            "✅ ifeval_throughput_b8 completed\n",
            "   Energy: 0.000048 kWh\n",
            "   Throughput: 273.60 tok/s\n",
            "   Avg Batch Time: 4020.51 ms\n",
            "   Memory: 1.40 GB\n",
            "\n",
            "======================================================================\n",
            "🎉 ALL WORKLOADS COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "\n",
            "✅ Run 1 completed for Llama-3.2-1B-pruned-60%\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "\n",
            "······································································\n",
            "   RUN 2/3 (seed=123)\n",
            "······································································\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Loading model: oopere/Llama-3.2-1B-pruned-60pct\n",
            "  Base: meta-llama/Llama-3.2-1B\n",
            "  Pruning: 60%\n",
            "  Star model: No (on-the-fly)\n",
            "======================================================================\n",
            "\n",
            "🔧 Creating model via on-the-fly pruning...\n",
            "✂️  Pruning with MAW method (60%)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning layers: 100%|██████████| 16/16 [00:02<00:00,  6.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model created\n",
            "   Original params: 1,235,814,400\n",
            "   Pruned params: 752,650,240\n",
            "   Reduction: 39.10%\n",
            "🆕 Creating new checkpoint: /content/drive/MyDrive/glu_pruning/checkpoints/1b_carbon/llama_3.2_1b_pruned_60pct_carbon_run2_seed123.json\n",
            "✅ Loaded checkpoint. Completed: 0/6 workloads\n",
            "✅ Using idle power calibration: 31.55 W\n",
            "   (Measured at: 2025-11-07T12:04:23.315788)\n",
            "\n",
            "\n",
            "🚀 Starting profiling: 6 workloads remaining\n",
            "======================================================================\n",
            "\n",
            "[1/6] Profiling: hellaswag_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:23:25] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:23:27] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:23:27] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:23:27] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000460 kWh\n",
            "   Net emissions (idle-corrected): 0.000111 kWh\n",
            "   (Idle contribution removed: 0.000349 kWh)\n",
            "✅ hellaswag_latency_b1 completed\n",
            "   Energy: 0.000111 kWh\n",
            "   Throughput: 50.57 tok/s\n",
            "   Avg TTFT: 387.99 ms\n",
            "   Memory: 1.40 GB\n",
            "\n",
            "[2/6] Profiling: mmlu_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:24:07] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:24:08] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:24:08] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:24:08] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.001134 kWh\n",
            "   Net emissions (idle-corrected): 0.000272 kWh\n",
            "   (Idle contribution removed: 0.000862 kWh)\n",
            "✅ mmlu_latency_b1 completed\n",
            "   Energy: 0.000272 kWh\n",
            "   Throughput: 51.36 tok/s\n",
            "   Avg TTFT: 973.47 ms\n",
            "   Memory: 1.40 GB\n",
            "\n",
            "[3/6] Profiling: ifeval_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:25:47] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:25:48] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:25:48] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:25:48] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 30 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.001018 kWh\n",
            "   Net emissions (idle-corrected): 0.000244 kWh\n",
            "   (Idle contribution removed: 0.000774 kWh)\n",
            "✅ ifeval_latency_b1 completed\n",
            "   Energy: 0.000244 kWh\n",
            "   Throughput: 51.45 tok/s\n",
            "   Avg TTFT: 2915.29 ms\n",
            "   Memory: 1.40 GB\n",
            "\n",
            "[4/6] Profiling: hellaswag_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:27:17] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:27:18] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:27:18] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:27:18] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000099 kWh\n",
            "   Net emissions (idle-corrected): 0.000025 kWh\n",
            "   (Idle contribution removed: 0.000073 kWh)\n",
            "✅ hellaswag_throughput_b8 completed\n",
            "   Energy: 0.000025 kWh\n",
            "   Throughput: 264.23 tok/s\n",
            "   Avg Batch Time: 578.00 ms\n",
            "   Memory: 1.40 GB\n",
            "\n",
            "[5/6] Profiling: mmlu_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:27:27] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:27:28] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:27:28] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:27:28] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000227 kWh\n",
            "   Net emissions (idle-corrected): 0.000061 kWh\n",
            "   (Idle contribution removed: 0.000166 kWh)\n",
            "✅ mmlu_throughput_b8 completed\n",
            "   Energy: 0.000061 kWh\n",
            "   Throughput: 276.41 tok/s\n",
            "   Avg Batch Time: 1381.37 ms\n",
            "   Memory: 1.40 GB\n",
            "\n",
            "[6/6] Profiling: ifeval_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:27:48] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:27:49] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:27:49] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:27:49] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 123 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 1 warmup iterations (out of 4 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000193 kWh\n",
            "   Net emissions (idle-corrected): 0.000047 kWh\n",
            "   (Idle contribution removed: 0.000146 kWh)\n",
            "✅ ifeval_throughput_b8 completed\n",
            "   Energy: 0.000047 kWh\n",
            "   Throughput: 278.85 tok/s\n",
            "   Avg Batch Time: 3944.79 ms\n",
            "   Memory: 1.40 GB\n",
            "\n",
            "======================================================================\n",
            "🎉 ALL WORKLOADS COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "\n",
            "✅ Run 2 completed for Llama-3.2-1B-pruned-60%\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "\n",
            "······································································\n",
            "   RUN 3/3 (seed=456)\n",
            "······································································\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Loading model: oopere/Llama-3.2-1B-pruned-60pct\n",
            "  Base: meta-llama/Llama-3.2-1B\n",
            "  Pruning: 60%\n",
            "  Star model: No (on-the-fly)\n",
            "======================================================================\n",
            "\n",
            "🔧 Creating model via on-the-fly pruning...\n",
            "✂️  Pruning with MAW method (60%)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning layers: 100%|██████████| 16/16 [00:02<00:00,  6.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model created\n",
            "   Original params: 1,235,814,400\n",
            "   Pruned params: 752,650,240\n",
            "   Reduction: 39.10%\n",
            "🆕 Creating new checkpoint: /content/drive/MyDrive/glu_pruning/checkpoints/1b_carbon/llama_3.2_1b_pruned_60pct_carbon_run3_seed456.json\n",
            "✅ Loaded checkpoint. Completed: 0/6 workloads\n",
            "✅ Using idle power calibration: 31.55 W\n",
            "   (Measured at: 2025-11-07T12:04:23.315788)\n",
            "\n",
            "\n",
            "🚀 Starting profiling: 6 workloads remaining\n",
            "======================================================================\n",
            "\n",
            "[1/6] Profiling: hellaswag_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:28:11] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:28:12] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:28:12] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:28:12] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000465 kWh\n",
            "   Net emissions (idle-corrected): 0.000113 kWh\n",
            "   (Idle contribution removed: 0.000353 kWh)\n",
            "✅ hellaswag_latency_b1 completed\n",
            "   Energy: 0.000113 kWh\n",
            "   Throughput: 50.66 tok/s\n",
            "   Avg TTFT: 392.29 ms\n",
            "   Memory: 1.40 GB\n",
            "\n",
            "[2/6] Profiling: mmlu_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:28:53] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:28:54] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:28:54] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:28:54] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 100 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.001125 kWh\n",
            "   Net emissions (idle-corrected): 0.000270 kWh\n",
            "   (Idle contribution removed: 0.000855 kWh)\n",
            "✅ mmlu_latency_b1 completed\n",
            "   Energy: 0.000270 kWh\n",
            "   Throughput: 51.35 tok/s\n",
            "   Avg TTFT: 965.10 ms\n",
            "   Memory: 1.40 GB\n",
            "\n",
            "[3/6] Profiling: ifeval_latency_b1\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:30:32] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:30:33] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:30:33] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:30:33] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 1)\n",
            "   Using 5 warmup iterations (out of 30 total)\n",
            "   Running in LATENCY mode (bsz=1). Measuring TTFT...\n",
            "   Tracker stopped. Raw emissions: 0.000957 kWh\n",
            "   Net emissions (idle-corrected): 0.000230 kWh\n",
            "   (Idle contribution removed: 0.000727 kWh)\n",
            "✅ ifeval_latency_b1 completed\n",
            "   Energy: 0.000230 kWh\n",
            "   Throughput: 51.83 tok/s\n",
            "   Avg TTFT: 2690.96 ms\n",
            "   Memory: 1.40 GB\n",
            "\n",
            "[4/6] Profiling: hellaswag_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:31:57] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:31:58] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:31:58] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:31:58] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from hellaswag...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000100 kWh\n",
            "   Net emissions (idle-corrected): 0.000026 kWh\n",
            "   (Idle contribution removed: 0.000074 kWh)\n",
            "✅ hellaswag_throughput_b8 completed\n",
            "   Energy: 0.000026 kWh\n",
            "   Throughput: 263.34 tok/s\n",
            "   Avg Batch Time: 579.96 ms\n",
            "   Memory: 1.40 GB\n",
            "\n",
            "[5/6] Profiling: mmlu_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:32:07] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:32:08] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:32:08] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:32:08] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 100 prompts from mmlu...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 2 warmup iterations (out of 13 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000222 kWh\n",
            "   Net emissions (idle-corrected): 0.000059 kWh\n",
            "   (Idle contribution removed: 0.000164 kWh)\n",
            "✅ mmlu_throughput_b8 completed\n",
            "   Energy: 0.000059 kWh\n",
            "   Throughput: 279.20 tok/s\n",
            "   Avg Batch Time: 1367.55 ms\n",
            "   Memory: 1.40 GB\n",
            "\n",
            "[6/6] Profiling: ifeval_throughput_b8\n",
            "----------------------------------------------------------------------\n",
            "   Clearing GPU cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:32:27] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "   Running GPU warm-up for CodeCarbon sensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 13:32:28] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 13:32:28] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 13:32:28] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   GPU warm-up complete.\n",
            "   Loading 30 prompts from IFEval...\n",
            "   Using random seed: 456 for reproducible sampling\n",
            "   Running inference...\n",
            "   (Batch Size: 8)\n",
            "   Using 1 warmup iterations (out of 4 total)\n",
            "   Running in THROUGHPUT mode (bsz=8). TTFT will not be measured.\n",
            "   Tracker stopped. Raw emissions: 0.000195 kWh\n",
            "   Net emissions (idle-corrected): 0.000047 kWh\n",
            "   (Idle contribution removed: 0.000148 kWh)\n",
            "✅ ifeval_throughput_b8 completed\n",
            "   Energy: 0.000047 kWh\n",
            "   Throughput: 277.11 tok/s\n",
            "   Avg Batch Time: 3969.59 ms\n",
            "   Memory: 1.40 GB\n",
            "\n",
            "======================================================================\n",
            "🎉 ALL WORKLOADS COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "\n",
            "✅ Run 3 completed for Llama-3.2-1B-pruned-60%\n",
            "🧹 GPU memory cleared. Available: 23.80 GB\n",
            "\n",
            "✅ All runs completed for Llama-3.2-1B-pruned-60%\n",
            "\n",
            "\n",
            "======================================================================\n",
            "✅  ALL MODELS PROFILED\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"📊  PHASE 2: PRUNED MODELS PROFILING\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# Store all results for final comparison\n",
        "# Structure: all_results[model_key][run_key] = results_dict\n",
        "all_results = {\n",
        "    \"baseline\": baseline_all_runs  # ← CAMBIO: ahora contiene múltiples runs\n",
        "}\n",
        "\n",
        "# Filter out baseline (already done)\n",
        "pruned_models = [m for m in models_1b if m['pruning_pct'] > 0]\n",
        "\n",
        "# Evaluate each pruned model\n",
        "for i, config in enumerate(pruned_models, 1):\n",
        "    model_name = f\"Llama-3.2-1B-pruned-{config['pruning_pct']}%\"\n",
        "    pruning_pct = config['pruning_pct']\n",
        "    is_star = config['is_star']\n",
        "\n",
        "    print(f\"\\n{'─'*70}\")\n",
        "    print(f\"🔄  PROFILING MODEL {i}/{len(pruned_models)}: {model_name}\")\n",
        "    print(f\"   Pruning: {pruning_pct}% | Star: {'⭐ ' if is_star else '❌'}\")\n",
        "    print(f\"{'─'*70}\\n\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # MULTIPLE RUNS LOOP FOR THIS PRUNED MODEL\n",
        "    # ========================================================================\n",
        "    pruned_model_all_runs = {}\n",
        "\n",
        "    for run_idx, seed in enumerate(RANDOM_SEEDS, 1):\n",
        "        print(f\"\\n{'··'*35}\")\n",
        "        print(f\"   RUN {run_idx}/{NUM_EXPERIMENTAL_RUNS} (seed={seed})\")\n",
        "        print(f\"{'··'*35}\\n\")\n",
        "\n",
        "        try:\n",
        "            # Load or create model (fresh for each run)\n",
        "            model, tokenizer, stats = load_or_create_model(config, device=\"cuda\")\n",
        "\n",
        "            # Display model statistics (once per model, not per run)\n",
        "            if run_idx == 1:\n",
        "                print(f\"\\n📈  Model Statistics:\")\n",
        "                print(f\"   Parameters: {stats['total_parameters']:,}\")\n",
        "                print(f\"   Size: {stats['size_gb']:.2f} GB\")\n",
        "                if 'pruning_stats' in stats:\n",
        "                    print(f\"   Reduction: {stats['pruning_stats']['percentage_reduction']:.1f}%\")\n",
        "                print(f\"   Source: {stats['source']}\\n\")\n",
        "\n",
        "            # Construct checkpoint path for this run\n",
        "            checkpoint_key = f\"{pruning_pct}pct\"\n",
        "            checkpoint_path_run = checkpoint_paths[checkpoint_key].replace(\".json\", f\"_run{run_idx}_seed{seed}.json\")\n",
        "\n",
        "            # Run profiling with seed\n",
        "            results_run = run_carbon_profiling(\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                workloads=BENCHMARKS_CARBON,\n",
        "                checkpoint_path=checkpoint_path_run,\n",
        "                model_name=f\"{model_name}-run{run_idx}\",\n",
        "                idle_power_calibration=idle_calibration,  # ← CAMBIO: usar None\n",
        "                device=\"cuda\",\n",
        "                random_seed=seed  # ← NUEVO: pasar seed\n",
        "            )\n",
        "\n",
        "            # Store results for this run\n",
        "            pruned_model_all_runs[f\"run_{run_idx}\"] = results_run\n",
        "\n",
        "            print(f\"\\n✅ Run {run_idx} completed for {model_name}\")\n",
        "\n",
        "            # Clear memory before next run\n",
        "            del model\n",
        "            clear_gpu_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❌  ERROR in run {run_idx}: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            clear_gpu_cache()\n",
        "            continue\n",
        "\n",
        "    # Store all runs for this model\n",
        "    all_results[checkpoint_key] = pruned_model_all_runs\n",
        "\n",
        "    print(f\"\\n✅ All runs completed for {model_name}\\n\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"✅  ALL MODELS PROFILED\")\n",
        "print(f\"{'='*70}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================================\n",
        "# CREATE GLOBAL MAPPINGS (FOR LATER CELLS)\n",
        "# ===========================================================================\n",
        "# These dicts are used by cells 17, 18, and 21 for aggregation and display.\n",
        "\n",
        "model_names = {}\n",
        "model_pruning = {}\n",
        "model_is_star = {}\n",
        "\n",
        "print(\"\\n🛠️  Creating global model mappings (model_names, model_pruning, model_is_star)...\")\n",
        "\n",
        "for config in models_1b:\n",
        "    pruning_pct = config['pruning_pct']\n",
        "\n",
        "    # Determine the key (must match Cell 10's logic: 'baseline' or '40pct')\n",
        "    if pruning_pct == 0:\n",
        "        key = \"baseline\"\n",
        "        model_name = \"Llama-3.2-1B (baseline)\"\n",
        "    else:\n",
        "        key = f\"{pruning_pct}pct\"\n",
        "        model_name = f\"Llama-3.2-1B-pruned-{pruning_pct}%\"\n",
        "\n",
        "    # Populate the dictionaries\n",
        "    model_names[key] = model_name\n",
        "    model_pruning[key] = pruning_pct\n",
        "    model_is_star[key] = config['is_star']\n",
        "\n",
        "print(f\"   ...Mappings created for {len(model_names)} models: {list(model_names.keys())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgFIvjfSUxyY",
        "outputId": "2455f659-9d74-4914-db70-843e0c66b391"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🛠️  Creating global model mappings (model_names, model_pruning, model_is_star)...\n",
            "   ...Mappings created for 7 models: ['baseline', '10pct', '20pct', '30pct', '40pct', '50pct', '60pct']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1MptG-dIvXH"
      },
      "source": [
        "# 5. Results Consolidation & Export\n",
        "\n",
        "Consolidate all evaluation results and export to CSV for analysis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"📊  AGGREGATING RESULTS ACROSS RUNS\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "JOULES_THRESHOLD = 0.30  # 300 mJ/token\n",
        "OUTLIER_RUN_THRESHOLD = 0.3  # Si >80% de benchmarks son outliers, descartar run completo\n",
        "\n",
        "def detect_problematic_runs(runs_dict):\n",
        "    \"\"\"\n",
        "    Detecta runs donde la mayoría de benchmarks son outliers.\n",
        "    Retorna un dict con el % de outliers por run.\n",
        "    \"\"\"\n",
        "    run_outlier_stats = {}\n",
        "\n",
        "    for run_key, workloads in runs_dict.items():\n",
        "        outlier_count = 0\n",
        "        total_count = len(workloads)\n",
        "\n",
        "        for workload_name, data in workloads.items():\n",
        "            joules = data.get('joules_per_token', 0.0)\n",
        "            if joules > JOULES_THRESHOLD:\n",
        "                outlier_count += 1\n",
        "\n",
        "        outlier_percentage = outlier_count / total_count if total_count > 0 else 0\n",
        "        run_outlier_stats[run_key] = {\n",
        "            'outlier_count': outlier_count,\n",
        "            'total_count': total_count,\n",
        "            'outlier_percentage': outlier_percentage,\n",
        "            'is_problematic': outlier_percentage > OUTLIER_RUN_THRESHOLD\n",
        "        }\n",
        "\n",
        "    return run_outlier_stats\n",
        "\n",
        "def aggregate_runs(runs_dict):\n",
        "    \"\"\"\n",
        "    Aggregate metrics from multiple runs with intelligent outlier handling.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    # PASO 1: Detectar runs completamente problemáticos\n",
        "    run_stats = detect_problematic_runs(runs_dict)\n",
        "\n",
        "    print(\"\\n    🔍 Run quality assessment:\")\n",
        "    for run_key, stats in run_stats.items():\n",
        "        status = \"❌ PROBLEMATIC\" if stats['is_problematic'] else \"✅ OK\"\n",
        "        print(f\"    {run_key}: {stats['outlier_count']}/{stats['total_count']} outliers ({stats['outlier_percentage']*100:.0f}%) - {status}\")\n",
        "\n",
        "    # Filtrar runs problemáticos\n",
        "    valid_runs = {k: v for k, v in runs_dict.items()\n",
        "                  if not run_stats[k]['is_problematic']}\n",
        "    problematic_runs = {k: v for k, v in runs_dict.items()\n",
        "                       if run_stats[k]['is_problematic']}\n",
        "\n",
        "    if not valid_runs:\n",
        "        print(\"\\n    ⚠️ WARNING: All runs are problematic. Using all runs with per-benchmark filtering.\")\n",
        "        valid_runs = runs_dict  # Usar todos si no hay ninguno válido\n",
        "    else:\n",
        "        print(f\"\\n    ✅ Using {len(valid_runs)}/{len(runs_dict)} runs for aggregation\")\n",
        "        if problematic_runs:\n",
        "            print(f\"    🚫 Excluding {len(problematic_runs)} problematic runs: {list(problematic_runs.keys())}\")\n",
        "\n",
        "    # PASO 2: Agregar por workload\n",
        "    aggregated = {}\n",
        "    outliers_log = {}\n",
        "\n",
        "    first_valid_run_key = next((key for key, val in valid_runs.items() if val), None)\n",
        "    if not first_valid_run_key:\n",
        "        print(\"    ⚠️ Error: All runs dictionaries are empty.\")\n",
        "        return {}, {}, problematic_runs\n",
        "\n",
        "    workload_names = list(valid_runs[first_valid_run_key].keys())\n",
        "\n",
        "    for workload_name in workload_names:\n",
        "        energy_samples = []\n",
        "        throughput_samples = []\n",
        "        ttft_samples = []\n",
        "        joules_per_token_samples = []\n",
        "\n",
        "        print(f\"\\n    Processing {workload_name}...\")\n",
        "        run_data = {}\n",
        "        valid_seeds_used = []\n",
        "        outliers_detected = []\n",
        "\n",
        "        for run_key in valid_runs.keys():\n",
        "            if workload_name not in valid_runs[run_key]:\n",
        "                print(f\"      ⚠️ Warning: '{workload_name}' missing in '{run_key}'\")\n",
        "                continue\n",
        "\n",
        "            run_data_current = valid_runs[run_key][workload_name]\n",
        "            joules = run_data_current.get('joules_per_token', 0.0)\n",
        "\n",
        "            # Filtrado por benchmark\n",
        "            if joules > JOULES_THRESHOLD:\n",
        "                outlier_info = {\n",
        "                    \"run_key\": run_key,\n",
        "                    \"joules_per_token\": source_data.get('joules_per_token', 0.0),\n",
        "                    \"energy_kwh\": run_data_current['energy_kwh'],\n",
        "                    \"reason\": f\">{JOULES_THRESHOLD:.4f} J/token\"\n",
        "                }\n",
        "                outliers_detected.append(outlier_info)\n",
        "                print(f\"      🚫 Outlier: {run_key} = {joules:.4f} J/token\")\n",
        "            else:\n",
        "                # Valor válido\n",
        "                energy_samples.append(run_data_current['energy_kwh'])\n",
        "                throughput_samples.append(run_data_current['throughput_tokens_per_sec'])\n",
        "                joules_per_token_samples.append(joules)\n",
        "\n",
        "                if run_data_current.get('avg_ttft_ms') is not None:\n",
        "                    ttft_samples.append(run_data_current['avg_ttft_ms'])\n",
        "\n",
        "                try:\n",
        "                    run_index = int(run_key.split('_')[-1]) - 1\n",
        "                    if 0 <= run_index < len(RANDOM_SEEDS):\n",
        "                        valid_seeds_used.append(RANDOM_SEEDS[run_index])\n",
        "                except ValueError:\n",
        "                    print(f\"      ⚠️ Could not parse seed from '{run_key}'\")\n",
        "\n",
        "                run_data = run_data_current\n",
        "\n",
        "        if outliers_detected:\n",
        "            outliers_log[workload_name] = outliers_detected\n",
        "\n",
        "        # Verificar datos válidos\n",
        "        if not energy_samples:\n",
        "            print(f\"      ❌ No valid data for {workload_name}\")\n",
        "            aggregated[workload_name] = {\n",
        "                \"error\": \"All measurements were outliers\",\n",
        "                \"outliers_excluded\": outliers_detected\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        print(f\"      ✅ {len(energy_samples)} valid measurements\")\n",
        "\n",
        "        # Raw runs solo con válidos\n",
        "        valid_raw_runs = {}\n",
        "        for run_key in valid_runs.keys():\n",
        "            if workload_name in valid_runs[run_key]:\n",
        "                source_data = valid_runs[run_key][workload_name]\n",
        "\n",
        "                valid_raw_runs[run_key] = {\n",
        "                    # --- Datos existentes ---\n",
        "                    \"energy_kwh\": source_data['energy_kwh'], # Net (actual)\n",
        "                    \"throughput\": source_data['throughput_tokens_per_sec'],\n",
        "                    \"ttft_ms\": source_data.get('avg_ttft_ms'),\n",
        "                    \"joules_per_token\": source_data.get('joules_per_token', 0.0),\n",
        "\n",
        "                    # --- NUEVOS DATOS AÑADIDOS ---\n",
        "                    \"energy_raw_kwh\": source_data.get('energy_raw_kwh'),\n",
        "                    \"energy_idle_kwh\": source_data.get('energy_idle_kwh'),\n",
        "                    \"total_new_tokens\": source_data.get('total_new_tokens'),\n",
        "                    \"inference_duration_sec\": source_data.get('hardware_metadata', {}).get('duration_sec',\n",
        "                          source_data.get('total_loop_time_sec')),\n",
        "                    \"memory_allocated_gb\": source_data.get('model_size_gb') # O 'memory_allocated_gb' si está disponible\n",
        "                }\n",
        "\n",
        "        # Calcular estadísticas\n",
        "        aggregated[workload_name] = {\n",
        "            \"energy_kwh_mean\": float(np.mean(energy_samples)),\n",
        "            \"energy_kwh_std\": float(np.std(energy_samples, ddof=1)) if len(energy_samples) > 1 else 0.0,\n",
        "            \"energy_kwh_min\": float(np.min(energy_samples)),\n",
        "            \"energy_kwh_max\": float(np.max(energy_samples)),\n",
        "\n",
        "            \"throughput_mean\": float(np.mean(throughput_samples)),\n",
        "            \"throughput_std\": float(np.std(throughput_samples, ddof=1)) if len(throughput_samples) > 1 else 0.0,\n",
        "            \"throughput_min\": float(np.min(throughput_samples)),\n",
        "            \"throughput_max\": float(np.max(throughput_samples)),\n",
        "\n",
        "            \"ttft_mean\": float(np.mean(ttft_samples)) if ttft_samples else None,\n",
        "            \"ttft_std\": float(np.std(ttft_samples, ddof=1)) if len(ttft_samples) > 1 else None,\n",
        "            \"ttft_min\": float(np.min(ttft_samples)) if ttft_samples else None,\n",
        "            \"ttft_max\": float(np.max(ttft_samples)) if ttft_samples else None,\n",
        "\n",
        "            \"joules_per_token_mean\": float(np.mean(joules_per_token_samples)),\n",
        "            \"joules_per_token_std\": float(np.std(joules_per_token_samples, ddof=1)) if len(joules_per_token_samples) > 1 else 0.0,\n",
        "            \"joules_per_token_min\": float(np.min(joules_per_token_samples)),\n",
        "            \"joules_per_token_max\": float(np.max(joules_per_token_samples)),\n",
        "\n",
        "            \"num_runs_aggregated\": len(energy_samples),\n",
        "            \"seeds_used\": valid_seeds_used,\n",
        "            \"model_size_gb\": run_data.get('model_size_gb', 0),\n",
        "            \"batch_size\": run_data.get('batch_size', 0),\n",
        "            \"num_prompts\": run_data.get('num_prompts', 0),\n",
        "\n",
        "            \"outliers_excluded\": outliers_detected if outliers_detected else None,\n",
        "            \"outliers_count\": len(outliers_detected),\n",
        "\n",
        "            \"raw_runs_data\": valid_raw_runs\n",
        "        }\n",
        "\n",
        "    return aggregated, outliers_log, problematic_runs\n",
        "\n",
        "# Aggregate results\n",
        "aggregated_results = {}\n",
        "all_outliers_log = {}\n",
        "all_problematic_runs = {}\n",
        "\n",
        "if 'all_results' not in globals():\n",
        "    print(\"❌ ERROR: 'all_results' not defined.\")\n",
        "else:\n",
        "    for model_key, runs_dict in all_results.items():\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Aggregating {model_key}...\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        aggregated, outliers_log, problematic_runs = aggregate_runs(runs_dict)\n",
        "        aggregated_results[model_key] = aggregated\n",
        "\n",
        "        if outliers_log:\n",
        "            all_outliers_log[model_key] = outliers_log\n",
        "        if problematic_runs:\n",
        "            all_problematic_runs[model_key] = problematic_runs\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"✅ AGGREGATION COMPLETE\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"   Models processed: {len(aggregated_results)}\")\n",
        "    print(f\"   Total problematic runs excluded: {sum(len(v) for v in all_problematic_runs.values())}\")\n",
        "    print(f\"   Total individual outliers: {sum(len(v) for v in all_outliers_log.values())}\")\n",
        "\n",
        "    # Reporte detallado\n",
        "    if all_problematic_runs:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"🚫 PROBLEMATIC RUNS EXCLUDED (>50% outliers)\")\n",
        "        print(f\"{'='*70}\")\n",
        "        for model_key, runs in all_problematic_runs.items():\n",
        "            print(f\"\\n{model_key}:\")\n",
        "            for run_key in runs.keys():\n",
        "                print(f\"  ❌ {run_key}\")\n",
        "\n",
        "    if all_outliers_log:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"🚫 INDIVIDUAL OUTLIERS EXCLUDED\")\n",
        "        print(f\"{'='*70}\")\n",
        "        for model_key, workloads in all_outliers_log.items():\n",
        "            print(f\"\\n{model_key}:\")\n",
        "            for workload_name, outliers in workloads.items():\n",
        "                print(f\"  {workload_name}:\")\n",
        "                for outlier in outliers:\n",
        "                    print(f\"    - {outlier['run_key']}: {outlier['joules_per_token']:.4f} J/token\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIdYGp9FUyei",
        "outputId": "f943234b-bf03-4080-d25e-8c8bbb9fb6e7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "📊  AGGREGATING RESULTS ACROSS RUNS\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Aggregating baseline...\n",
            "======================================================================\n",
            "\n",
            "    🔍 Run quality assessment:\n",
            "    run_1: 0/6 outliers (0%) - ✅ OK\n",
            "    run_2: 1/6 outliers (17%) - ✅ OK\n",
            "    run_3: 1/6 outliers (17%) - ✅ OK\n",
            "\n",
            "    ✅ Using 3/3 runs for aggregation\n",
            "\n",
            "    Processing hellaswag_latency_b1...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing mmlu_latency_b1...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing ifeval_latency_b1...\n",
            "      🚫 Outlier: run_2 = 0.3435 J/token\n",
            "      🚫 Outlier: run_3 = 0.3254 J/token\n",
            "      ✅ 1 valid measurements\n",
            "\n",
            "    Processing hellaswag_throughput_b8...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing mmlu_throughput_b8...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing ifeval_throughput_b8...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "======================================================================\n",
            "Aggregating 10pct...\n",
            "======================================================================\n",
            "\n",
            "    🔍 Run quality assessment:\n",
            "    run_1: 0/6 outliers (0%) - ✅ OK\n",
            "    run_2: 1/6 outliers (17%) - ✅ OK\n",
            "    run_3: 1/6 outliers (17%) - ✅ OK\n",
            "\n",
            "    ✅ Using 3/3 runs for aggregation\n",
            "\n",
            "    Processing hellaswag_latency_b1...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing mmlu_latency_b1...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing ifeval_latency_b1...\n",
            "      🚫 Outlier: run_2 = 0.3145 J/token\n",
            "      🚫 Outlier: run_3 = 0.3192 J/token\n",
            "      ✅ 1 valid measurements\n",
            "\n",
            "    Processing hellaswag_throughput_b8...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing mmlu_throughput_b8...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing ifeval_throughput_b8...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "======================================================================\n",
            "Aggregating 20pct...\n",
            "======================================================================\n",
            "\n",
            "    🔍 Run quality assessment:\n",
            "    run_1: 0/6 outliers (0%) - ✅ OK\n",
            "    run_2: 1/6 outliers (17%) - ✅ OK\n",
            "    run_3: 1/6 outliers (17%) - ✅ OK\n",
            "\n",
            "    ✅ Using 3/3 runs for aggregation\n",
            "\n",
            "    Processing hellaswag_latency_b1...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing mmlu_latency_b1...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing ifeval_latency_b1...\n",
            "      🚫 Outlier: run_2 = 0.3034 J/token\n",
            "      🚫 Outlier: run_3 = 0.3208 J/token\n",
            "      ✅ 1 valid measurements\n",
            "\n",
            "    Processing hellaswag_throughput_b8...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing mmlu_throughput_b8...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing ifeval_throughput_b8...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "======================================================================\n",
            "Aggregating 30pct...\n",
            "======================================================================\n",
            "\n",
            "    🔍 Run quality assessment:\n",
            "    run_1: 0/6 outliers (0%) - ✅ OK\n",
            "    run_2: 0/6 outliers (0%) - ✅ OK\n",
            "    run_3: 0/6 outliers (0%) - ✅ OK\n",
            "\n",
            "    ✅ Using 3/3 runs for aggregation\n",
            "\n",
            "    Processing hellaswag_latency_b1...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing mmlu_latency_b1...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing ifeval_latency_b1...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing hellaswag_throughput_b8...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing mmlu_throughput_b8...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing ifeval_throughput_b8...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "======================================================================\n",
            "Aggregating 40pct...\n",
            "======================================================================\n",
            "\n",
            "    🔍 Run quality assessment:\n",
            "    run_1: 0/6 outliers (0%) - ✅ OK\n",
            "    run_2: 0/6 outliers (0%) - ✅ OK\n",
            "    run_3: 0/6 outliers (0%) - ✅ OK\n",
            "\n",
            "    ✅ Using 3/3 runs for aggregation\n",
            "\n",
            "    Processing hellaswag_latency_b1...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing mmlu_latency_b1...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing ifeval_latency_b1...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing hellaswag_throughput_b8...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing mmlu_throughput_b8...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing ifeval_throughput_b8...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "======================================================================\n",
            "Aggregating 50pct...\n",
            "======================================================================\n",
            "\n",
            "    🔍 Run quality assessment:\n",
            "    run_1: 0/6 outliers (0%) - ✅ OK\n",
            "    run_2: 0/6 outliers (0%) - ✅ OK\n",
            "    run_3: 0/6 outliers (0%) - ✅ OK\n",
            "\n",
            "    ✅ Using 3/3 runs for aggregation\n",
            "\n",
            "    Processing hellaswag_latency_b1...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing mmlu_latency_b1...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing ifeval_latency_b1...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing hellaswag_throughput_b8...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing mmlu_throughput_b8...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing ifeval_throughput_b8...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "======================================================================\n",
            "Aggregating 60pct...\n",
            "======================================================================\n",
            "\n",
            "    🔍 Run quality assessment:\n",
            "    run_1: 0/6 outliers (0%) - ✅ OK\n",
            "    run_2: 0/6 outliers (0%) - ✅ OK\n",
            "    run_3: 0/6 outliers (0%) - ✅ OK\n",
            "\n",
            "    ✅ Using 3/3 runs for aggregation\n",
            "\n",
            "    Processing hellaswag_latency_b1...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing mmlu_latency_b1...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing ifeval_latency_b1...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing hellaswag_throughput_b8...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing mmlu_throughput_b8...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "    Processing ifeval_throughput_b8...\n",
            "      ✅ 3 valid measurements\n",
            "\n",
            "======================================================================\n",
            "✅ AGGREGATION COMPLETE\n",
            "======================================================================\n",
            "   Models processed: 7\n",
            "   Total problematic runs excluded: 0\n",
            "   Total individual outliers: 3\n",
            "\n",
            "======================================================================\n",
            "🚫 INDIVIDUAL OUTLIERS EXCLUDED\n",
            "======================================================================\n",
            "\n",
            "baseline:\n",
            "  ifeval_latency_b1:\n",
            "    - run_2: 0.2828 J/token\n",
            "    - run_3: 0.2828 J/token\n",
            "\n",
            "10pct:\n",
            "  ifeval_latency_b1:\n",
            "    - run_2: 0.2766 J/token\n",
            "    - run_3: 0.2766 J/token\n",
            "\n",
            "20pct:\n",
            "  ifeval_latency_b1:\n",
            "    - run_2: 0.2594 J/token\n",
            "    - run_3: 0.2594 J/token\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5RqknL6IvXH",
        "outputId": "c614a91e-5791-4560-b30e-0ab93ab95cd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "📊  CONSOLIDATING AGGREGATED RESULTS TO CSV/JSON\n",
            "======================================================================\n",
            "\n",
            "✅  Consolidated 42 result rows (with aggregated statistics)\n",
            "   Models: 7\n",
            "   Workloads: 6\n",
            "   Runs per configuration: 3\n",
            "\n",
            "DataFrame Preview (Aggregated Results):\n",
            "                     model                 workload  energy_kwh_mean  \\\n",
            "0  Llama-3.2-1B (baseline)     hellaswag_latency_b1         0.000039   \n",
            "1  Llama-3.2-1B (baseline)  hellaswag_throughput_b8         0.000024   \n",
            "2  Llama-3.2-1B (baseline)        ifeval_latency_b1         0.000171   \n",
            "3  Llama-3.2-1B (baseline)     ifeval_throughput_b8         0.000053   \n",
            "4  Llama-3.2-1B (baseline)          mmlu_latency_b1         0.000300   \n",
            "5  Llama-3.2-1B (baseline)       mmlu_throughput_b8         0.000064   \n",
            "6  Llama-3.2-1B-pruned-10%     hellaswag_latency_b1         0.000070   \n",
            "7  Llama-3.2-1B-pruned-10%  hellaswag_throughput_b8         0.000034   \n",
            "8  Llama-3.2-1B-pruned-10%        ifeval_latency_b1         0.000202   \n",
            "9  Llama-3.2-1B-pruned-10%     ifeval_throughput_b8         0.000063   \n",
            "\n",
            "   energy_kwh_std  throughput_mean  throughput_std  \n",
            "0    7.149760e-06        49.532543        0.375437  \n",
            "1    3.810401e-07       242.947984        5.047821  \n",
            "2    0.000000e+00        51.741510        0.000000  \n",
            "3    5.779119e-07       275.112167        1.419005  \n",
            "4    2.931726e-05        51.416682        0.630326  \n",
            "5    1.271385e-06       275.165578        6.133852  \n",
            "6    9.892758e-06        49.950425        0.273638  \n",
            "7    5.132676e-07       240.535543        4.406028  \n",
            "8    0.000000e+00        51.061296        0.000000  \n",
            "9    4.170777e-07       277.439242        3.031489  \n",
            "\n",
            "💾  Aggregated results saved to: /content/drive/MyDrive/glu_pruning/results/carbon_1b_aggregated_results_20251107_134504.csv\n",
            "💾  Raw runs data saved to: /content/drive/MyDrive/glu_pruning/results/carbon_1b_raw_runs_20251107_134504.json\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"📊  CONSOLIDATING AGGREGATED RESULTS TO CSV/JSON\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# Build consolidated data using AGGREGATED results\n",
        "consolidated_data = []\n",
        "\n",
        "for model_key, workload_results in aggregated_results.items():\n",
        "    display_name = model_names.get(model_key, model_key)\n",
        "    pruning_pct = model_pruning.get(model_key, 0)\n",
        "    is_star = model_is_star.get(model_key, False)\n",
        "\n",
        "    for workload_name, metrics in workload_results.items():\n",
        "        row = {\n",
        "            \"model\": display_name,\n",
        "            \"pruning_pct\": pruning_pct,\n",
        "            \"is_star\": is_star,\n",
        "            \"workload\": workload_name,\n",
        "\n",
        "            # Aggregated metrics (mean ± std)\n",
        "            \"energy_kwh_mean\": metrics[\"energy_kwh_mean\"],\n",
        "            \"energy_kwh_std\": metrics[\"energy_kwh_std\"],\n",
        "            \"throughput_mean\": metrics[\"throughput_mean\"],\n",
        "            \"throughput_std\": metrics[\"throughput_std\"],\n",
        "            \"joules_per_token_mean\": metrics[\"joules_per_token_mean\"],\n",
        "            \"joules_per_token_std\": metrics[\"joules_per_token_std\"],\n",
        "            \"ttft_mean\": metrics.get(\"ttft_mean\"),\n",
        "            \"ttft_std\": metrics.get(\"ttft_std\"),\n",
        "\n",
        "            # Metadata\n",
        "            \"num_runs\": metrics[\"num_runs_aggregated\"], # <--- LÍNEA CORREGIDA\n",
        "            \"model_size_gb\": metrics[\"model_size_gb\"],\n",
        "            \"batch_size\": metrics[\"batch_size\"],\n",
        "        }\n",
        "        consolidated_data.append(row)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(consolidated_data)\n",
        "df = df.sort_values(by=[\"pruning_pct\", \"workload\"]).reset_index(drop=True)\n",
        "\n",
        "print(f\"✅  Consolidated {len(df)} result rows (with aggregated statistics)\")\n",
        "print(f\"   Models: {df['model'].nunique()}\")\n",
        "print(f\"   Workloads: {df['workload'].nunique()}\")\n",
        "# Usamos el dataframe para asegurar que contamos los runs correctos\n",
        "print(f\"   Runs per configuration: {df['num_runs'].max()}\")\n",
        "\n",
        "# Display preview with mean ± std\n",
        "print(\"\\nDataFrame Preview (Aggregated Results):\")\n",
        "print(df[['model', 'workload', 'energy_kwh_mean', 'energy_kwh_std', 'throughput_mean', 'throughput_std']].head(10))\n",
        "\n",
        "# Save to CSV\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "csv_path = f\"{RESULTS_DIR}/carbon_1b_aggregated_results_{timestamp}.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\"\\n💾  Aggregated results saved to: {csv_path}\")\n",
        "\n",
        "# Also save raw runs for transparency\n",
        "raw_runs_path = f\"{RESULTS_DIR}/carbon_1b_raw_runs_{timestamp}.json\"\n",
        "with open(raw_runs_path, 'w') as f:\n",
        "    json.dump(all_results, f, indent=2, default=str)  # default=str handles numpy types\n",
        "\n",
        "print(f\"💾  Raw runs data saved to: {raw_runs_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"📈 CARBON & PERFORMANCE ANALYSIS\")\n",
        "print(f\"S{'='*70}\\n\")\n",
        "\n",
        "# Calculate averages per model across workloads\n",
        "summary_data = []\n",
        "\n",
        "# ===================================================================\n",
        "# START: FIX\n",
        "# ===================================================================\n",
        "\n",
        "# Use dynamic keys from aggregated_results (FIX 1)\n",
        "for model_key in aggregated_results.keys():\n",
        "    # Get the aggregated workload results for this model (FIX 2)\n",
        "    workload_results = aggregated_results[model_key]\n",
        "\n",
        "    # Aggregate metrics across workloads using the '_mean' keys (FIX 3)\n",
        "    total_energy = sum(m['energy_kwh_mean'] for m in workload_results.values())\n",
        "    avg_throughput = np.mean([m['throughput_mean'] for m in workload_results.values()])\n",
        "    ttft_values = [m['ttft_mean'] for m in workload_results.values() if m.get('ttft_mean') is not None]\n",
        "\n",
        "    # --- AÑADIDO ---\n",
        "    joules_per_token_values = [m['joules_per_token_mean'] for m in workload_results.values() if m.get('joules_per_token_mean') is not None]\n",
        "    avg_joules_per_token = np.mean(joules_per_token_values) if joules_per_token_values else 0.0\n",
        "    # --- FIN AÑADIDO ---\n",
        "# ===================================================================\n",
        "# END: FIX\n",
        "# ===================================================================\n",
        "\n",
        "    avg_ttft = np.mean(ttft_values) if ttft_values else 0.0 # Usar np.mean\n",
        "    model_size = list(workload_results.values())[0]['model_size_gb']  # Same for all workloads\n",
        "\n",
        "    summary = {\n",
        "        \"model\": model_names.get(model_key, model_key),\n",
        "        \"pruning_pct\": model_pruning.get(model_key, 0),\n",
        "        \"is_star\": model_is_star.get(model_key, False),\n",
        "        \"total_energy_kwh\": total_energy,\n",
        "        \"avg_throughput_tok_s\": avg_throughput,\n",
        "        \"avg_joules_per_token\": avg_joules_per_token, # <-- AÑADIDO\n",
        "        \"avg_ttft_ms\": avg_ttft,\n",
        "        \"model_size_gb\": model_size\n",
        "    }\n",
        "    summary_data.append(summary)\n",
        "\n",
        "# Sort by pruning_pct for consistent display\n",
        "summary_df = pd.DataFrame(summary_data).sort_values('pruning_pct').reset_index(drop=True)\n",
        "\n",
        "# --- ACTUALIZAR PRINT ---\n",
        "print(\"Performance Summary (Calculations Corrected):\")\n",
        "print(\"-\" * 110) # Ampliar tabla\n",
        "print(summary_df.to_string(index=False, float_format=\"%.4f\"))\n",
        "print(\"-\" * 110) # Ampliar tabla\n",
        "# --- FIN ACTUALIZAR PRINT ---\n",
        "\n",
        "# Calculate improvements vs baseline\n",
        "baseline_row = summary_df[summary_df['pruning_pct'] == 0].iloc[0]\n",
        "\n",
        "print(f\"\\n💡 Improvements vs. Baseline (Calculations Corrected):\")\n",
        "print(\"-\" * 90)\n",
        "\n",
        "for _, row in summary_df.iterrows():\n",
        "    if row['pruning_pct'] == 0:\n",
        "        continue\n",
        "\n",
        "    energy_reduction = ((baseline_row['total_energy_kwh'] - row['total_energy_kwh']) / baseline_row['total_energy_kwh']) * 100\n",
        "    throughput_change = ((row['avg_throughput_tok_s'] - baseline_row['avg_throughput_tok_s']) / baseline_row['avg_throughput_tok_s']) * 100\n",
        "    ttft_change = ((row['avg_ttft_ms'] - baseline_row['avg_ttft_ms']) / baseline_row['avg_ttft_ms']) * 100\n",
        "    size_reduction = ((baseline_row['model_size_gb'] - row['model_size_gb']) / baseline_row['model_size_gb']) * 100\n",
        "\n",
        "    # --- AÑADIDO ---\n",
        "    joules_change = 0.0\n",
        "    if baseline_row['avg_joules_per_token'] > 0:\n",
        "        # Nota: Para Joules/token, un número MENOR es mejor (menos energía).\n",
        "        # Por lo tanto, (baseline - actual) > 0 significa una mejora.\n",
        "        joules_change = ((baseline_row['avg_joules_per_token'] - row['avg_joules_per_token']) / baseline_row['avg_joules_per_token']) * 100\n",
        "    # --- FIN AÑADIDO ---\n",
        "\n",
        "    star_marker = \"⭐\" if row['is_star'] else \"\"\n",
        "    print(f\"\\n{row['model']} {star_marker}\")\n",
        "\n",
        "    # --- ACTUALIZAR PRINT ---\n",
        "    print(f\"  Energy (Total): {energy_reduction:+.2f}% ({'✅ Lower' if energy_reduction > 0 else '❌ Higher'})\")\n",
        "    print(f\"  Energy (J/Token): {joules_change:+.2f}% ({'✅ More Efficient' if joules_change > 0 else '❌ Less Efficient'})\") # <-- AÑADIDO\n",
        "    print(f\"  Throughput: {throughput_change:+.2f}% ({'✅ Faster' if throughput_change > 0 else '⚠️ Slower'})\")\n",
        "    print(f\"  Latency (TTFT): {ttft_change:+.2f}% ({'✅ Lower' if ttft_change < 0 else '⚠️ Higher'})\")\n",
        "    print(f\"  Model Size: {size_reduction:+.2f}% ({'✅ Smaller' if size_reduction > 0 else '❌ Larger'})\")\n",
        "    # --- FIN ACTUALIZAR PRINT ---\n",
        "\n",
        "print(\"\\n\" + \"-\" * 90)"
      ],
      "metadata": {
        "id": "TBPO5o8oteVs",
        "outputId": "464ca4f8-4f3f-44ae-8943-9a102b2242f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "📈 CARBON & PERFORMANCE ANALYSIS\n",
            "S======================================================================\n",
            "\n",
            "Performance Summary (Calculations Corrected):\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "                  model  pruning_pct  is_star  total_energy_kwh  avg_throughput_tok_s  avg_joules_per_token  avg_ttft_ms  model_size_gb\n",
            "Llama-3.2-1B (baseline)            0    False            0.0007              157.6527                0.1682     877.2974         2.3019\n",
            "Llama-3.2-1B-pruned-10%           10    False            0.0007              156.8793                0.1730    1006.6757         2.1519\n",
            "Llama-3.2-1B-pruned-20%           20    False            0.0008              159.3089                0.1636    1123.2391         2.0020\n",
            "Llama-3.2-1B-pruned-30%           30    False            0.0008              158.2259                0.1574    1312.3533         1.8520\n",
            "Llama-3.2-1B-pruned-40%           40     True            0.0008              159.6844                0.1437    1320.8938         1.7020\n",
            "Llama-3.2-1B-pruned-50%           50    False            0.0008              163.2632                0.1393    1400.9065         1.5519\n",
            "Llama-3.2-1B-pruned-60%           60    False            0.0008              162.3789                0.1352    1396.7862         1.4019\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "💡 Improvements vs. Baseline (Calculations Corrected):\n",
            "------------------------------------------------------------------------------------------\n",
            "\n",
            "Llama-3.2-1B-pruned-10% \n",
            "  Energy (Total): -12.53% (❌ Higher)\n",
            "  Energy (J/Token): -2.84% (❌ Less Efficient)\n",
            "  Throughput: -0.49% (⚠️ Slower)\n",
            "  Latency (TTFT): +14.75% (⚠️ Higher)\n",
            "  Model Size: +6.51% (✅ Smaller)\n",
            "\n",
            "Llama-3.2-1B-pruned-20% \n",
            "  Energy (Total): -15.79% (❌ Higher)\n",
            "  Energy (J/Token): +2.70% (✅ More Efficient)\n",
            "  Throughput: +1.05% (✅ Faster)\n",
            "  Latency (TTFT): +28.03% (⚠️ Higher)\n",
            "  Model Size: +13.03% (✅ Smaller)\n",
            "\n",
            "Llama-3.2-1B-pruned-30% \n",
            "  Energy (Total): -27.86% (❌ Higher)\n",
            "  Energy (J/Token): +6.42% (✅ More Efficient)\n",
            "  Throughput: +0.36% (✅ Faster)\n",
            "  Latency (TTFT): +49.59% (⚠️ Higher)\n",
            "  Model Size: +19.54% (✅ Smaller)\n",
            "\n",
            "Llama-3.2-1B-pruned-40% ⭐\n",
            "  Energy (Total): -17.97% (❌ Higher)\n",
            "  Energy (J/Token): +14.55% (✅ More Efficient)\n",
            "  Throughput: +1.29% (✅ Faster)\n",
            "  Latency (TTFT): +50.56% (⚠️ Higher)\n",
            "  Model Size: +26.06% (✅ Smaller)\n",
            "\n",
            "Llama-3.2-1B-pruned-50% \n",
            "  Energy (Total): -19.51% (❌ Higher)\n",
            "  Energy (J/Token): +17.16% (✅ More Efficient)\n",
            "  Throughput: +3.56% (✅ Faster)\n",
            "  Latency (TTFT): +59.68% (⚠️ Higher)\n",
            "  Model Size: +32.58% (✅ Smaller)\n",
            "\n",
            "Llama-3.2-1B-pruned-60% \n",
            "  Energy (Total): -15.49% (❌ Higher)\n",
            "  Energy (J/Token): +19.61% (✅ More Efficient)\n",
            "  Throughput: +3.00% (✅ Faster)\n",
            "  Latency (TTFT): +59.21% (⚠️ Higher)\n",
            "  Model Size: +39.10% (✅ Smaller)\n",
            "\n",
            "------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxHStHvMIvXH"
      },
      "source": [
        "# 6. Quick Analysis & Visualization\n",
        "\n",
        "Generate quick insights to decide which models merit uploading to HuggingFace Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Xo_bifVCIvXI",
        "outputId": "8eeb3a07-6a3f-41ff-9922-c8bfb2878275",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x1000 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Visualization saved to: /content/drive/MyDrive/glu_pruning/results/carbon_1b_analysis.png\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Create comprehensive visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "models = summary_df['model'].values\n",
        "pruning_levels = summary_df['pruning_pct'].values\n",
        "star_mask = summary_df['is_star'].values\n",
        "\n",
        "# Colors: baseline blue, star gold, others gray\n",
        "colors = ['#1f77b4' if p == 0 else '#FFD700' if s else '#95a5a6'\n",
        "          for p, s in zip(pruning_levels, star_mask)]\n",
        "\n",
        "# 1. Energy Consumption\n",
        "axes[0, 0].bar(range(len(models)), summary_df['total_energy_kwh'], color=colors, edgecolor='black', linewidth=1.5)\n",
        "axes[0, 0].set_xticks(range(len(models)))\n",
        "axes[0, 0].set_xticklabels([m.replace('Llama-3.2-', '') for m in models], rotation=0, ha='center')\n",
        "axes[0, 0].set_ylabel('Total Energy (kWh)', fontsize=11)\n",
        "axes[0, 0].set_title('Energy Consumption', fontsize=13, fontweight='bold')\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(summary_df['total_energy_kwh']):\n",
        "    axes[0, 0].text(i, v + 0.0001, f'{v:.4f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 2. Throughput\n",
        "axes[0, 1].bar(range(len(models)), summary_df['avg_throughput_tok_s'], color=colors, edgecolor='black', linewidth=1.5)\n",
        "axes[0, 1].set_xticks(range(len(models)))\n",
        "axes[0, 1].set_xticklabels([m.replace('Llama-3.2-', '') for m in models], rotation=0, ha='center')\n",
        "axes[0, 1].set_ylabel('Throughput (tokens/s)', fontsize=11)\n",
        "axes[0, 1].set_title('Inference Throughput', fontsize=13, fontweight='bold')\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(summary_df['avg_throughput_tok_s']):\n",
        "    axes[0, 1].text(i, v + 0.5, f'{v:.1f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 3. Latency (TTFT)\n",
        "axes[1, 0].bar(range(len(models)), summary_df['avg_ttft_ms'], color=colors, edgecolor='black', linewidth=1.5)\n",
        "axes[1, 0].set_xticks(range(len(models)))\n",
        "axes[1, 0].set_xticklabels([m.replace('Llama-3.2-', '') for m in models], rotation=0, ha='center')\n",
        "axes[1, 0].set_ylabel('Avg TTFT (ms)', fontsize=11)\n",
        "axes[1, 0].set_title('Latency (Time To First Token)', fontsize=13, fontweight='bold')\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(summary_df['avg_ttft_ms']):\n",
        "    axes[1, 0].text(i, v + 1, f'{v:.1f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 4. Model Size\n",
        "axes[1, 1].bar(range(len(models)), summary_df['model_size_gb'], color=colors, edgecolor='black', linewidth=1.5)\n",
        "axes[1, 1].set_xticks(range(len(models)))\n",
        "axes[1, 1].set_xticklabels([m.replace('Llama-3.2-', '') for m in models], rotation=0, ha='center')\n",
        "axes[1, 1].set_ylabel('Model Size (GB)', fontsize=11)\n",
        "axes[1, 1].set_title('Model Size', fontsize=13, fontweight='bold')\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(summary_df['model_size_gb']):\n",
        "    axes[1, 1].text(i, v + 0.02, f'{v:.2f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{RESULTS_DIR}/carbon_1b_analysis.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n📊 Visualization saved to: {RESULTS_DIR}/carbon_1b_analysis.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4H7r1PNIvXI"
      },
      "source": [
        "# 7. Agregating results."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NUEVO CÓDIGO PARA LA CELDA 42\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"📊 CONSTRUCTING FINAL JSON FROM AGGREGATED RESULTS\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# --- Validar variables globales ---\n",
        "# (Asegurarse de que las variables de celdas anteriores existen)\n",
        "\n",
        "if 'RESULTS_DIR' not in globals():\n",
        "    print(\"⚠️ Warning: RESULTS_DIR not set. Using default './results'\")\n",
        "    RESULTS_DIR = \"/content/drive/MyDrive/glu_pruning/results\"\n",
        "\n",
        "if 'BENCHMARKS_CARBON' not in globals():\n",
        "    print(\"❌ Error: BENCHMARKS_CARBON not defined. Recargando fallback.\")\n",
        "    BENCHMARKS_CARBON = [\n",
        "        {\"name\": \"gsm8k_latency_b1\", \"num_prompts\": 10, \"max_new_tokens\": 100, \"dataset\": \"gsm8k\", \"subset\": \"test\", \"description\": \"Math reasoning (Latency, TTFT, bsz=1)\", \"batch_size\": 1},\n",
        "        {\"name\": \"mmlu_latency_b1\", \"num_prompts\": 10, \"max_new_tokens\": 50, \"dataset\": \"mmlu\", \"subset\": \"test\", \"description\": \"Knowledge QA (Latency, TTFT, bsz=1)\", \"batch_size\": 1},\n",
        "        {\"name\": \"ifeval_latency_b1\", \"num_prompts\": 10, \"max_new_tokens\": 150, \"dataset\": \"IFEval\", \"subset\": \"train\", \"description\": \"Instruction (Latency, TTFT, bsz=1)\", \"batch_size\": 1},\n",
        "        {\"name\": \"gsm8k_throughput_b8\", \"num_prompts\": 10, \"max_new_tokens\": 100, \"dataset\": \"gsm8k\", \"subset\": \"test\", \"description\": \"Math reasoning (Throughput, bsz=8)\", \"batch_size\": 8},\n",
        "        {\"name\": \"mmlu_throughput_b8\", \"num_prompts\": 10, \"max_new_tokens\": 50, \"dataset\": \"mmlu\", \"subset\": \"test\", \"description\": \"Knowledge QA (Throughput, bsz=8)\", \"batch_size\": 8},\n",
        "        {\"name\": \"ifeval_throughput_b8\", \"num_prompts\": 10, \"max_new_tokens\": 150, \"dataset\": \"IFEval\", \"subset\": \"train\", \"description\": \"Instruction (Throughput, bsz=8)\", \"batch_size\": 8},\n",
        "    ]\n",
        "\n",
        "# --- Usar resultados ya agregados y filtrados ---\n",
        "if 'aggregated_results' not in globals() or 'all_results' not in globals():\n",
        "    print(\"❌ FATAL ERROR: 'aggregated_results' o 'all_results' no están definidos.\")\n",
        "    print(\"   Por favor, ejecute de nuevo las celdas 38 y 39 antes de esta.\")\n",
        "else:\n",
        "    print(f\"Usando 'aggregated_results' (calculados en Celda 39) para {len(aggregated_results)} modelos.\")\n",
        "\n",
        "    # --- Construir el JSON Completo ---\n",
        "\n",
        "    # 1. Construir mapeo de configuración\n",
        "    config_map = {}\n",
        "    model_family_name = \"Llama-3.2-1B\"\n",
        "\n",
        "    for cfg in EXPERIMENT_CONFIG_CARBON:\n",
        "        if model_family_name not in cfg[\"base_model\"]:\n",
        "            continue\n",
        "\n",
        "        pruning_pct = cfg['pruning_pct']\n",
        "        # La clave debe coincidir con la usada en 'aggregated_results' (p.ej. 'baseline', '20pct', '40pct')\n",
        "        key = \"baseline\" if pruning_pct == 0 else f\"{pruning_pct}pct\"\n",
        "        model_name_cfg = f\"{model_family_name}\" if pruning_pct == 0 else f\"{model_family_name}-pruned-{pruning_pct}%\"\n",
        "\n",
        "        hf_repo = cfg.get(\"hf_repo_id\", cfg[\"base_model\"]) if pruning_pct > 0 else cfg[\"base_model\"]\n",
        "\n",
        "        config_map[key] = {\n",
        "            \"name\": model_name_cfg,\n",
        "            \"pruning_pct\": int(pruning_pct),\n",
        "            \"is_star\": bool(cfg[\"is_star\"]),\n",
        "            \"hf_repo\": hf_repo,\n",
        "        }\n",
        "\n",
        "    print(f\"\\nDynamically built config map for {len(config_map)} models.\")\n",
        "\n",
        "    # 2. Construir 'models_evaluated'\n",
        "    models_evaluated = {}\n",
        "    summary_stats_list = []\n",
        "\n",
        "    for model_key, workload_results in aggregated_results.items():\n",
        "        if model_key in config_map:\n",
        "            config = config_map[model_key]\n",
        "\n",
        "            # Usar los resultados de workload_results (ya filtrados por outliers)\n",
        "            models_evaluated[model_key] = {\n",
        "                \"name\": config[\"name\"],\n",
        "                \"pruning_pct\": config[\"pruning_pct\"],\n",
        "                \"is_star\": config[\"is_star\"],\n",
        "                \"hf_repo\": config[\"hf_repo\"],\n",
        "                \"results\": workload_results\n",
        "            }\n",
        "\n",
        "            # Calcular estadísticas de resumen (basado en Celda 40)\n",
        "            valid_workloads = [w for w in workload_results.values() if 'error' not in w]\n",
        "            if valid_workloads:\n",
        "                joules_per_token_values = [m['joules_per_token_mean'] for m in valid_workloads if m.get('joules_per_token_mean') is not None]\n",
        "                avg_joules_per_token = float(np.mean(joules_per_token_values)) if joules_per_token_values else 0.0\n",
        "                ttft_values = [m['ttft_mean'] for m in valid_workloads if m.get('ttft_mean') is not None]\n",
        "                avg_ttft = float(np.mean(ttft_values)) if ttft_values else 0.0\n",
        "\n",
        "                summary_stats_list.append({\n",
        "                    \"model\": config[\"name\"],\n",
        "                    \"pruning_pct\": config[\"pruning_pct\"],\n",
        "                    \"is_star\": config[\"is_star\"],\n",
        "                    \"total_energy_kwh\": sum(m.get('energy_kwh_mean', 0) for m in valid_workloads),\n",
        "                    \"avg_throughput_tok_s\": np.mean([m.get('throughput_mean', 0) for m in valid_workloads]),\n",
        "                    \"avg_joules_per_token\": avg_joules_per_token,\n",
        "                    \"avg_ttft_ms\": avg_ttft,\n",
        "                    \"model_size_gb\": list(valid_workloads)[0].get('model_size_gb', 0)\n",
        "                })\n",
        "        else:\n",
        "            print(f\"⚠️ Warning: No config map entry found for key '{model_key}'.\")\n",
        "\n",
        "    # 3. Construir 'summary_statistics'\n",
        "    baseline_stats = next((s for s in summary_stats_list if s['pruning_pct'] == 0), None)\n",
        "    pruned_stats = sorted([s for s in summary_stats_list if s['pruning_pct'] > 0], key=lambda x: x['pruning_pct'])\n",
        "\n",
        "    summary_statistics = {\n",
        "        \"baseline\": baseline_stats,\n",
        "        \"pruned_models\": pruned_stats\n",
        "    }\n",
        "\n",
        "    # 4. Obtener hardware_info (del primer run del baseline, usando 'all_results')\n",
        "    hardware_info = {}\n",
        "    try:\n",
        "        # 1. CORRECCIÓN: Usar la clave correcta 'run_1' (generada en la Celda 9)\n",
        "        run_key_to_check = \"run_1\"\n",
        "\n",
        "        if \"baseline\" in all_results and run_key_to_check in all_results[\"baseline\"]:\n",
        "            # 2. Obtener la metadata del primer run/workload como fuente\n",
        "            first_workload_data = list(all_results[\"baseline\"][run_key_to_check].values())[0]\n",
        "\n",
        "            if \"hardware_metadata\" in first_workload_data:\n",
        "                hw_meta = first_workload_data[\"hardware_metadata\"]\n",
        "\n",
        "                # 3. CONSTRUCCIÓN CURADA: Crear el diccionario limpio que especificaste\n",
        "                hardware_info = {\n",
        "                    \"gpu_model\": hw_meta.get(\"gpu_name_torch\"),\n",
        "                    \"gpu_memory_gb\": round(hw_meta.get(\"gpu_total_memory_gb\", 0), 2),\n",
        "                    \"gpu_compute_capability\": hw_meta.get(\"gpu_compute_capability\"),\n",
        "                    \"cuda_version\": hw_meta.get(\"cuda_version\"),\n",
        "                    \"torch_version\": hw_meta.get(\"torch_version\"),\n",
        "                    \"gpu_temperature_celsius\": hw_meta.get(\"gpu_temperature_celsius\"),\n",
        "                }\n",
        "\n",
        "                # 4. AÑADIR DATOS DE IDLE: Obtener de la variable global 'idle_calibration' (de la Celda 8)\n",
        "                if 'idle_calibration' in globals():\n",
        "                    hardware_info[\"idle_power_watts\"] = round(idle_calibration.get(\"idle_power_watts\", 0), 2)\n",
        "                    hardware_info[\"idle_power_measurement_duration_sec\"] = idle_calibration.get(\"duration_seconds\")\n",
        "                    hardware_info[\"idle_power_measurement_timestamp\"] = idle_calibration.get(\"timestamp\")\n",
        "                else:\n",
        "                    print(\"    Warning: 'idle_calibration' no encontrada. Faltarán datos de idle power.\")\n",
        "\n",
        "                print(f\"    Successfully captured and curated hardware metadata (GPU: {hardware_info.get('gpu_model', 'N/A')})\")\n",
        "            else:\n",
        "                print(\"    'hardware_metadata' not found in first workload.\")\n",
        "        else:\n",
        "            print(f\"    Could not find '{run_key_to_check}' in 'all_results[\\\"baseline\\\"]'. Keys available: {list(all_results.get('baseline', {}).keys())}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    Error extracting hardware info: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "    # 5. Consolidar todo\n",
        "    complete_results = {\n",
        "        \"experiment_metadata\": {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"notebook\": \"03_Evaluate_1B_CARBON.ipynb\",\n",
        "            \"model_family\": model_family_name,\n",
        "            \"pruning_method\": \"MAW (Maximum Absolute Weight)\",\n",
        "            \"hardware_details\": hardware_info\n",
        "        },\n",
        "        \"benchmarks\": [\n",
        "            {\n",
        "                \"name\": task[\"name\"],\n",
        "                \"num_prompts\": task[\"num_prompts\"],\n",
        "                \"max_new_tokens\": task[\"max_new_tokens\"],\n",
        "                \"description\": task[\"description\"]\n",
        "            }\n",
        "            for task in BENCHMARKS_CARBON\n",
        "        ],\n",
        "        \"models_evaluated\": models_evaluated,\n",
        "        \"summary_statistics\": summary_statistics,\n",
        "        \"citation\": {\n",
        "            \"paper\": \"Exploring GLU Expansion Ratios: Structured Pruning in Llama-3.2 Models\",\n",
        "            \"author\": \"Pere Martra\",\n",
        "            \"doi\": \"https://doi.org/10.31219/osf.io/qgxea\",\n",
        "            \"github\": \"https://github.com/peremartra/llama-glu-expansion-pruning\",\n",
        "            \"note\": \"Results are freely available for research purposes. Please cite the paper if you use this data.\"\n",
        "        }\n",
        "    }\n",
        "    print(f\"\\nSuccessfully consolidated results for {len(models_evaluated)} models.\")\n",
        "\n",
        "    # --- Guardar en JSON ---\n",
        "    try:\n",
        "        os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        json_path = f\"{RESULTS_DIR}/llama_1b_carbon_complete_results_{timestamp}.json\"\n",
        "        with open(json_path, 'w') as f:\n",
        "            json.dump(complete_results, f, indent=2, ensure_ascii=False, default=str)\n",
        "\n",
        "        print(f\"\\n✅ Complete (and filtered) carbon results saved to:\")\n",
        "        print(f\"   {json_path}\")\n",
        "\n",
        "        latest_json = f\"{RESULTS_DIR}/llama_1b_carbon_complete_results_latest.json\"\n",
        "        with open(latest_json, 'w') as f:\n",
        "            json.dump(complete_results, f, indent=2, ensure_ascii=False, default=str)\n",
        "\n",
        "        print(f\"✅ Latest (and filtered) version:\")\n",
        "        print(f\"   {latest_json}\")\n",
        "\n",
        "        file_size_kb = Path(json_path).stat().st_size / 1024\n",
        "        print(f\"\\n📊 File size: {file_size_kb:.1f} KB\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error saving JSON files: {e}\")\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"✅ COMPLETE CARBON RESULTS SAVED - Ready for research sharing\")\n",
        "    print(f\"{'='*70}\\n\")"
      ],
      "metadata": {
        "id": "nfjYJe0yNNhK",
        "outputId": "77171c91-8955-4fe4-a9f5-2a9e21abb8c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "📊 CONSTRUCTING FINAL JSON FROM AGGREGATED RESULTS\n",
            "======================================================================\n",
            "\n",
            "Usando 'aggregated_results' (calculados en Celda 39) para 7 modelos.\n",
            "\n",
            "Dynamically built config map for 7 models.\n",
            "    Successfully captured and curated hardware metadata (GPU: NVIDIA L4)\n",
            "\n",
            "Successfully consolidated results for 7 models.\n",
            "\n",
            "✅ Complete (and filtered) carbon results saved to:\n",
            "   /content/drive/MyDrive/glu_pruning/results/llama_1b_carbon_complete_results_20251107_134516.json\n",
            "✅ Latest (and filtered) version:\n",
            "   /content/drive/MyDrive/glu_pruning/results/llama_1b_carbon_complete_results_latest.json\n",
            "\n",
            "📊 File size: 114.4 KB\n",
            "\n",
            "======================================================================\n",
            "✅ COMPLETE CARBON RESULTS SAVED - Ready for research sharing\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6JVP-5RIvXI"
      },
      "source": [
        "---\n",
        "\n",
        "## 🎓 Key Takeaways\n",
        "\n",
        "This notebook evaluated the Llama-3.2-1B model family across different datasets to determine:\n",
        "\n",
        "1. **Optimal pruning level** for GLU-MLP layers\n",
        "2. **Performance-Carbon emission trade-offs** at different expansion ratios\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Powered by [OptiPFair](https://github.com/peremartra/optipfair)** - Structured Pruning for GLU Architectures\n",
        "\n",
        "If this research helps your work:\n",
        "- ⭐ Star [the repo](https://github.com/peremartra/optipfair)\n",
        "- 📖 Read the [documentation](https://peremartra.github.io/optipfair/)\n",
        "- 🐛 Report issues or suggest features\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DvcQV-q0S_XI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}