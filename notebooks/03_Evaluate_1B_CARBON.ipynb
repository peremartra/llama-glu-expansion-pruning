{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/llama-glu-expansion-pruning/blob/main/notebooks/03_Evaluate_1B_CARBON.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzfjNg_SIvXD"
      },
      "source": [
        "# GLU Pruning Research - Llama-3.2-1B Evaluation\n",
        "## 03 - Measuring Environmental Impact & Inference Performance\n",
        "\n",
        "### Exploring energy efficiency of width-pruned GLU models\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "\n",
        "[![Paper](https://img.shields.io/badge/OSF-Paper-blue?logo=osf&logoColor=white)](https://doi.org/10.31219/osf.io/qgxea)\n",
        "[![GitHub](https://img.shields.io/badge/‚≠ê_Star-OptiPFair-orange?logo=github&logoColor=white)](https://github.com/peremartra/optipfair)\n",
        "[![PyPI](https://img.shields.io/pypi/v/optipfair?logo=python&logoColor=white&label=v)](https://pypi.org/project/optipfair/)\n",
        "\n",
        "**Repository:** [github.com/peremartra/llama-glu-expansion-pruning](https://github.com/peremartra/llama-glu-expansion-pruning)\n",
        "\n",
        "---\n",
        "\n",
        "**Colab Environment:** GPU L4 (or T4)\n",
        "\n",
        "**Estimated Runtime:** ~1-2 hours total\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Notebook Objective\n",
        "\n",
        "This notebook measures the inference performance and energy consumption of the Llama-3.2-1B model family (the base model and pruned versions from 10% to 60%). The goal is to quantify the efficiency gains (in speed and energy) achieved through structured pruning.\n",
        "\n",
        "### Key Features:\n",
        "‚úÖ Inference Measurement: Measures Latency (Time To First Token or TTFT) and Throughput (tokens/sec).\n",
        "\n",
        "‚úÖ Energy Tracking: Uses codecarbon to measure net energy (kWh) and Joules per token (J/token), factoring out idle power consumption.\n",
        "\n",
        "‚úÖ Robust Measurements: Executes 3 \"runs\" with random seeds ([42, 123, 456]) for each model, ensuring reliable averages.\n",
        "\n",
        "‚úÖ On-the-fly Pruning: Loads the base model and applies pruning in memory (using optipfair) for each run, no pre-generated models needed.\n",
        "\n",
        "‚úÖ Checkpoint Support: Saves the results of each individual run, allowing resumption if the notebook is interrupted.\n",
        "\n",
        "### Results will answer:\n",
        "- How much energy (Joules/token) does pruning actually save?\n",
        "\n",
        "- Does the 40% pruned model (the \"star model\" for accuracy) also have the best performance/energy profile?\n",
        "\n",
        "- What is the trade-off between pruning percentage, latency (TTFT), and throughput?\n",
        "\n",
        "- How does batch size (bsz=1 vs bsz=8) affect the efficiency of the pruned models?\n",
        "\n",
        "---\n",
        "\n",
        "**Note**: This notebook only measures inference performance and energy consumption. The accuracy evaluation (using lm-eval) is performed in the [02_Evaluate_1B.ipynb](https://github.com/peremartra/llama-glu-expansion-pruning/blob/main/notebooks/02_Evaluate_1B.ipynb) notebook.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkVFbeCMIvXF"
      },
      "source": [
        "# 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAo67s0lIvXF"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install -q optipfair\n",
        "!pip install -q lm-eval\n",
        "!pip install -q langdetect\n",
        "!pip install -q codecarbon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWIHQuIGIvXG"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive for checkpoint persistence\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DG2nO7YpIvXG"
      },
      "outputs": [],
      "source": [
        "# Download utils.py from GitHub repository\n",
        "!wget -q https://raw.githubusercontent.com/peremartra/llama-glu-expansion-pruning/main/utils.py\n",
        "\n",
        "# Verify download\n",
        "import os\n",
        "if os.path.exists('utils.py'):\n",
        "    print(\"‚úÖ utils.py downloaded successfully\")\n",
        "else:\n",
        "    print(\"‚ùå Failed to download utils.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHjkx6_QIvXG"
      },
      "outputs": [],
      "source": [
        "# Import core libraries and utilities\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import glob\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Import our utility functions\n",
        "from utils import (\n",
        "    EXPERIMENT_CONFIG_CARBON,\n",
        "    BENCHMARKS_CARBON,\n",
        "    load_or_create_model,\n",
        "    run_carbon_profiling,\n",
        "    run_robust_evaluation,\n",
        "    clear_gpu_cache,\n",
        "    get_model_stats,\n",
        "    format_results_table,\n",
        "    calibrate_idle_power  # ‚Üê NUEVA FUNCI√ìN\n",
        ")\n",
        "\n",
        "print(\"‚úÖ All imports successful\")\n",
        "print(f\"üì± Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# EXPERIMENTAL RUNS CONFIGURATION\n",
        "# =============================================================================\n",
        "NUM_EXPERIMENTAL_RUNS = 3\n",
        "RANDOM_SEEDS = [42, 123, 456]  # Fixed seeds for reproducibility\n",
        "\n",
        "print(f\"üîÑ Experiment Configuration:\")\n",
        "print(f\"   Number of runs: {NUM_EXPERIMENTAL_RUNS}\")\n",
        "print(f\"   Random seeds: {RANDOM_SEEDS}\")\n",
        "print(f\"   Total evaluations per model: {len(BENCHMARKS_CARBON)} workloads √ó {NUM_EXPERIMENTAL_RUNS} runs\")\n",
        "print()"
      ],
      "metadata": {
        "id": "n_36x9scTpv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LixoDuXJIvXG"
      },
      "source": [
        "# 2. Configuration & Planning\n",
        "\n",
        "This section filters the experiment configuration for 1B models and displays the profiling plan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5fDt6IxIvXG"
      },
      "outputs": [],
      "source": [
        "# Filter configuration for 1B models only\n",
        "models_1b = [\n",
        "    config for config in EXPERIMENT_CONFIG_CARBON\n",
        "    if \"1B\" in config[\"base_model\"] and \"3B\" not in config[\"base_model\"] and \"Instruct\" not in config[\"base_model\"]\n",
        "]\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"üìä EVALUATION PLAN: Llama-3.2-1B Family\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "print(f\"Total models to evaluate: {len(models_1b) + 1}\")  # +1 for base model\n",
        "print(f\"Benchmarks per model: {len(BENCHMARKS_CARBON)}\")\n",
        "print(f\"Total evaluations: {(len(models_1b) + 1) * len(BENCHMARKS_CARBON)}\")\n",
        "print(f\"Estimated runtime: ~1-1.5 hours\\n\")\n",
        "\n",
        "# Display models table\n",
        "print(\"Models to profile:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Model':<40} {'Pruning':<10} {'Star':<6}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for config in models_1b:\n",
        "    if config['pruning_pct'] == 0:\n",
        "        model_name = \"Llama-3.2-1B (baseline)\"\n",
        "    else:\n",
        "        model_name = f\"Llama-3.2-1B-pruned-{config['pruning_pct']}%\"\n",
        "\n",
        "    pruning = f\"{config['pruning_pct']}%\"\n",
        "    star = \"‚≠ê Yes\" if config['is_star'] else \"No\"\n",
        "    print(f\"{model_name:<40} {pruning:<10} {star:<6}\")\n",
        "\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Display workloads\n",
        "print(\"\\nWorkloads to run:\")\n",
        "print(\"-\" * 70)\n",
        "for i, workload in enumerate(BENCHMARKS_CARBON, 1):\n",
        "    name = workload['name']\n",
        "    prompts = workload['num_prompts']\n",
        "    tokens = workload['max_new_tokens']\n",
        "    desc = workload['description']\n",
        "    print(f\"{i}. {name:<25} {prompts} prompts, {tokens} tokens - {desc}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "print(\"\\n‚öôÔ∏è Configuration:\")\n",
        "print(f\"   - Neuron selection method: MAW (Maximum Absolute Weight)\")\n",
        "print(f\"   - Checkpointing: Enabled (per-workload granularity)\")\n",
        "print(f\"   - Model creation: On-the-fly pruning (no pre-creation needed)\")\n",
        "print(f\"   - GPU warm-up: First 5 prompts excluded from metrics\")\n",
        "print(f\"   - Energy tracking: CodeCarbon\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7U0stRUIvXG"
      },
      "outputs": [],
      "source": [
        "# Setup checkpoint paths (manual construction since helpers are internal)\n",
        "BASE_CHECKPOINT_DIR = \"/content/drive/MyDrive/glu_pruning/checkpoints\"\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/glu_pruning/results\"\n",
        "\n",
        "# Carbon-specific subdirectory\n",
        "CHECKPOINT_DIR = f\"{BASE_CHECKPOINT_DIR}/1b_carbon\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "Path(CHECKPOINT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Checkpoint directory: {CHECKPOINT_DIR}\")\n",
        "print(f\"‚úÖ Results directory: {RESULTS_DIR}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONSTRUCT CHECKPOINT PATHS DYNAMICALLY FROM models_1b\n",
        "# ============================================================================\n",
        "checkpoint_paths = {}\n",
        "\n",
        "for config in models_1b:\n",
        "    pruning_pct = config['pruning_pct']\n",
        "\n",
        "    # Create key: \"baseline\" for 0%, \"{X}pct\" for others\n",
        "    if pruning_pct == 0:\n",
        "        key = \"baseline\"\n",
        "        filename = \"llama_3.2_1b_baseline_carbon.json\"\n",
        "    else:\n",
        "        key = f\"{pruning_pct}pct\"\n",
        "        filename = f\"llama_3.2_1b_pruned_{pruning_pct}pct_carbon.json\"\n",
        "\n",
        "    checkpoint_paths[key] = f\"{CHECKPOINT_DIR}/{filename}\"\n",
        "\n",
        "print(f\"\\nüìÇ Auto-generated {len(checkpoint_paths)} checkpoint paths:\")\n",
        "for key, path in checkpoint_paths.items():\n",
        "    exists = \"‚úÖ Exists\" if Path(path).exists() else \"üÜï New\"\n",
        "    print(f\"  {key:<10}: {exists}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5yTl5gTIvXH"
      },
      "source": [
        "# 3. Baseline Evaluation\n",
        "\n",
        "Evaluate the original Llama-3.2-1B model to establish performance baseline."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================\n",
        "# CALIBRATE IDLE POWER (Run once at start)\n",
        "# ====================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"üîã STEP 0: IDLE POWER CALIBRATION\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "idle_calibration = calibrate_idle_power(\n",
        "    device=\"cuda\",\n",
        "    duration_seconds=30,  # 30s is enough for stable measurement\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Save calibration to drive for reproducibility\n",
        "calibration_path = f\"{RESULTS_DIR}/idle_power_calibration.json\"\n",
        "with open(calibration_path, 'w') as f:\n",
        "    json.dump(idle_calibration, f, indent=2)\n",
        "\n",
        "print(f\"\\nüíæ Calibration saved to: {calibration_path}\")\n",
        "print(\"=\"*70 + \"\\n\")"
      ],
      "metadata": {
        "id": "GHJPPtiHDtiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjcHGNrsIvXH"
      },
      "outputs": [],
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"üìä  PHASE 1: BASELINE PROFILING\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "BASE_MODEL_ID = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# ============================================================================\n",
        "# MULTIPLE RUNS LOOP FOR BASELINE\n",
        "# ============================================================================\n",
        "baseline_all_runs = {}  # Store results from all runs\n",
        "\n",
        "for run_idx, seed in enumerate(RANDOM_SEEDS, 1):\n",
        "    print(f\"\\n{''*70}\")\n",
        "    print(f\"üîÑ  BASELINE - RUN {run_idx}/{NUM_EXPERIMENTAL_RUNS} (seed={seed})\")\n",
        "    print(f\"{'‚îÄ'*70}\\n\")\n",
        "\n",
        "    # Load base model (fresh for each run to avoid state contamination)\n",
        "    print(f\"Loading base model: {BASE_MODEL_ID}...\")\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        BASE_MODEL_ID,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print(\"‚úÖ  Model loaded successfully\")\n",
        "\n",
        "    # Construct checkpoint path for this run\n",
        "    checkpoint_path_run = checkpoint_paths[\"baseline\"].replace(\".json\", f\"_run{run_idx}_seed{seed}.json\")\n",
        "\n",
        "    # Run carbon profiling with seed\n",
        "    baseline_results_run = run_carbon_profiling(\n",
        "        model=base_model,\n",
        "        tokenizer=tokenizer,\n",
        "        workloads=BENCHMARKS_CARBON,\n",
        "        checkpoint_path=checkpoint_path_run,\n",
        "        model_name=f\"Llama-3.2-1B-baseline-run{run_idx}\",\n",
        "        idle_power_calibration=idle_calibration,  # ‚Üê CAMBIO: usar None\n",
        "        device=\"cuda\",\n",
        "        random_seed=seed  # ‚Üê NUEVO: pasar seed\n",
        "    )\n",
        "\n",
        "    # Store results for this run\n",
        "    baseline_all_runs[f\"run_{run_idx}\"] = baseline_results_run\n",
        "\n",
        "    # Display run summary\n",
        "    print(f\"\\n‚úÖ Run {run_idx} completed\")\n",
        "    print(\"Results summary:\")\n",
        "    for workload_name, metrics in baseline_results_run.items():\n",
        "        print(f\"\\n{workload_name}:\")\n",
        "        print(f\"  Energy: {metrics['energy_kwh']:.6f} kWh\")\n",
        "        print(f\"  Throughput: {metrics['throughput_tokens_per_sec']:.2f} tok/s\")\n",
        "\n",
        "    # Clear memory before next run\n",
        "    del base_model\n",
        "    clear_gpu_cache()\n",
        "    print(f\"üßπ  Memory cleared\\n\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"‚úÖ  ALL BASELINE RUNS COMPLETED\")\n",
        "print(f\"{'='*70}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtbaAsxIIvXH"
      },
      "source": [
        "# 4. Pruned Models Evaluation Loop\n",
        "\n",
        "Profile the pruned variants using on-the-fly pruning with OptiPFair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPoZiAfjIvXH"
      },
      "outputs": [],
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"üìä  PHASE 2: PRUNED MODELS PROFILING\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# Store all results for final comparison\n",
        "# Structure: all_results[model_key][run_key] = results_dict\n",
        "all_results = {\n",
        "    \"baseline\": baseline_all_runs  # ‚Üê CAMBIO: ahora contiene m√∫ltiples runs\n",
        "}\n",
        "\n",
        "# Filter out baseline (already done)\n",
        "pruned_models = [m for m in models_1b if m['pruning_pct'] > 0]\n",
        "\n",
        "# Evaluate each pruned model\n",
        "for i, config in enumerate(pruned_models, 1):\n",
        "    model_name = f\"Llama-3.2-1B-pruned-{config['pruning_pct']}%\"\n",
        "    pruning_pct = config['pruning_pct']\n",
        "    is_star = config['is_star']\n",
        "\n",
        "    print(f\"\\n{'‚îÄ'*70}\")\n",
        "    print(f\"üîÑ  PROFILING MODEL {i}/{len(pruned_models)}: {model_name}\")\n",
        "    print(f\"   Pruning: {pruning_pct}% | Star: {'‚≠ê ' if is_star else '‚ùå'}\")\n",
        "    print(f\"{'‚îÄ'*70}\\n\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # MULTIPLE RUNS LOOP FOR THIS PRUNED MODEL\n",
        "    # ========================================================================\n",
        "    pruned_model_all_runs = {}\n",
        "\n",
        "    for run_idx, seed in enumerate(RANDOM_SEEDS, 1):\n",
        "        print(f\"\\n{'¬∑¬∑'*35}\")\n",
        "        print(f\"   RUN {run_idx}/{NUM_EXPERIMENTAL_RUNS} (seed={seed})\")\n",
        "        print(f\"{'¬∑¬∑'*35}\\n\")\n",
        "\n",
        "        try:\n",
        "            # Load or create model (fresh for each run)\n",
        "            model, tokenizer, stats = load_or_create_model(config, device=\"cuda\")\n",
        "\n",
        "            # Display model statistics (once per model, not per run)\n",
        "            if run_idx == 1:\n",
        "                print(f\"\\nüìà  Model Statistics:\")\n",
        "                print(f\"   Parameters: {stats['total_parameters']:,}\")\n",
        "                print(f\"   Size: {stats['size_gb']:.2f} GB\")\n",
        "                if 'pruning_stats' in stats:\n",
        "                    print(f\"   Reduction: {stats['pruning_stats']['percentage_reduction']:.1f}%\")\n",
        "                print(f\"   Source: {stats['source']}\\n\")\n",
        "\n",
        "            # Construct checkpoint path for this run\n",
        "            checkpoint_key = f\"{pruning_pct}pct\"\n",
        "            checkpoint_path_run = checkpoint_paths[checkpoint_key].replace(\".json\", f\"_run{run_idx}_seed{seed}.json\")\n",
        "\n",
        "            # Run profiling with seed\n",
        "            results_run = run_carbon_profiling(\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                workloads=BENCHMARKS_CARBON,\n",
        "                checkpoint_path=checkpoint_path_run,\n",
        "                model_name=f\"{model_name}-run{run_idx}\",\n",
        "                idle_power_calibration=idle_calibration,  # ‚Üê CAMBIO: usar None\n",
        "                device=\"cuda\",\n",
        "                random_seed=seed  # ‚Üê NUEVO: pasar seed\n",
        "            )\n",
        "\n",
        "            # Store results for this run\n",
        "            pruned_model_all_runs[f\"run_{run_idx}\"] = results_run\n",
        "\n",
        "            print(f\"\\n‚úÖ Run {run_idx} completed for {model_name}\")\n",
        "\n",
        "            # Clear memory before next run\n",
        "            del model\n",
        "            clear_gpu_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå  ERROR in run {run_idx}: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            clear_gpu_cache()\n",
        "            continue\n",
        "\n",
        "    # Store all runs for this model\n",
        "    all_results[checkpoint_key] = pruned_model_all_runs\n",
        "\n",
        "    print(f\"\\n‚úÖ All runs completed for {model_name}\\n\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"‚úÖ  ALL MODELS PROFILED\")\n",
        "print(f\"{'='*70}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================================\n",
        "# CREATE GLOBAL MAPPINGS (FOR LATER CELLS)\n",
        "# ===========================================================================\n",
        "# These dicts are used by cells 17, 18, and 21 for aggregation and display.\n",
        "\n",
        "model_names = {}\n",
        "model_pruning = {}\n",
        "model_is_star = {}\n",
        "\n",
        "print(\"\\nüõ†Ô∏è  Creating global model mappings (model_names, model_pruning, model_is_star)...\")\n",
        "\n",
        "for config in models_1b:\n",
        "    pruning_pct = config['pruning_pct']\n",
        "\n",
        "    # Determine the key (must match Cell 10's logic: 'baseline' or '40pct')\n",
        "    if pruning_pct == 0:\n",
        "        key = \"baseline\"\n",
        "        model_name = \"Llama-3.2-1B (baseline)\"\n",
        "    else:\n",
        "        key = f\"{pruning_pct}pct\"\n",
        "        model_name = f\"Llama-3.2-1B-pruned-{pruning_pct}%\"\n",
        "\n",
        "    # Populate the dictionaries\n",
        "    model_names[key] = model_name\n",
        "    model_pruning[key] = pruning_pct\n",
        "    model_is_star[key] = config['is_star']\n",
        "\n",
        "print(f\"   ...Mappings created for {len(model_names)} models: {list(model_names.keys())}\")"
      ],
      "metadata": {
        "id": "ZgFIvjfSUxyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1MptG-dIvXH"
      },
      "source": [
        "# 5. Results Consolidation & Export\n",
        "\n",
        "Consolidate all evaluation results and export to CSV for analysis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"üìä  AGGREGATING RESULTS ACROSS RUNS\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "JOULES_THRESHOLD = 0.30  # 300 mJ/token\n",
        "OUTLIER_RUN_THRESHOLD = 0.3  # Si >80% de benchmarks son outliers, descartar run completo\n",
        "\n",
        "def detect_problematic_runs(runs_dict):\n",
        "    \"\"\"\n",
        "    Detecta runs donde la mayor√≠a de benchmarks son outliers.\n",
        "    Retorna un dict con el % de outliers por run.\n",
        "    \"\"\"\n",
        "    run_outlier_stats = {}\n",
        "\n",
        "    for run_key, workloads in runs_dict.items():\n",
        "        outlier_count = 0\n",
        "        total_count = len(workloads)\n",
        "\n",
        "        for workload_name, data in workloads.items():\n",
        "            joules = data.get('joules_per_token', 0.0)\n",
        "            if joules > JOULES_THRESHOLD:\n",
        "                outlier_count += 1\n",
        "\n",
        "        outlier_percentage = outlier_count / total_count if total_count > 0 else 0\n",
        "        run_outlier_stats[run_key] = {\n",
        "            'outlier_count': outlier_count,\n",
        "            'total_count': total_count,\n",
        "            'outlier_percentage': outlier_percentage,\n",
        "            'is_problematic': outlier_percentage > OUTLIER_RUN_THRESHOLD\n",
        "        }\n",
        "\n",
        "    return run_outlier_stats\n",
        "\n",
        "def aggregate_runs(runs_dict):\n",
        "    \"\"\"\n",
        "    Aggregate metrics from multiple runs with intelligent outlier handling.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    # PASO 1: Detectar runs completamente problem√°ticos\n",
        "    run_stats = detect_problematic_runs(runs_dict)\n",
        "\n",
        "    print(\"\\n    üîç Run quality assessment:\")\n",
        "    for run_key, stats in run_stats.items():\n",
        "        status = \"‚ùå PROBLEMATIC\" if stats['is_problematic'] else \"‚úÖ OK\"\n",
        "        print(f\"    {run_key}: {stats['outlier_count']}/{stats['total_count']} outliers ({stats['outlier_percentage']*100:.0f}%) - {status}\")\n",
        "\n",
        "    # Filtrar runs problem√°ticos\n",
        "    valid_runs = {k: v for k, v in runs_dict.items()\n",
        "                  if not run_stats[k]['is_problematic']}\n",
        "    problematic_runs = {k: v for k, v in runs_dict.items()\n",
        "                       if run_stats[k]['is_problematic']}\n",
        "\n",
        "    if not valid_runs:\n",
        "        print(\"\\n    ‚ö†Ô∏è WARNING: All runs are problematic. Using all runs with per-benchmark filtering.\")\n",
        "        valid_runs = runs_dict  # Usar todos si no hay ninguno v√°lido\n",
        "    else:\n",
        "        print(f\"\\n    ‚úÖ Using {len(valid_runs)}/{len(runs_dict)} runs for aggregation\")\n",
        "        if problematic_runs:\n",
        "            print(f\"    üö´ Excluding {len(problematic_runs)} problematic runs: {list(problematic_runs.keys())}\")\n",
        "\n",
        "    # PASO 2: Agregar por workload\n",
        "    aggregated = {}\n",
        "    outliers_log = {}\n",
        "\n",
        "    first_valid_run_key = next((key for key, val in valid_runs.items() if val), None)\n",
        "    if not first_valid_run_key:\n",
        "        print(\"    ‚ö†Ô∏è Error: All runs dictionaries are empty.\")\n",
        "        return {}, {}, problematic_runs\n",
        "\n",
        "    workload_names = list(valid_runs[first_valid_run_key].keys())\n",
        "\n",
        "    for workload_name in workload_names:\n",
        "        energy_samples = []\n",
        "        throughput_samples = []\n",
        "        ttft_samples = []\n",
        "        joules_per_token_samples = []\n",
        "\n",
        "        print(f\"\\n    Processing {workload_name}...\")\n",
        "        run_data = {}\n",
        "        valid_seeds_used = []\n",
        "        outliers_detected = []\n",
        "\n",
        "        for run_key in valid_runs.keys():\n",
        "            if workload_name not in valid_runs[run_key]:\n",
        "                print(f\"      ‚ö†Ô∏è Warning: '{workload_name}' missing in '{run_key}'\")\n",
        "                continue\n",
        "\n",
        "            run_data_current = valid_runs[run_key][workload_name]\n",
        "            joules = run_data_current.get('joules_per_token', 0.0)\n",
        "\n",
        "            # Filtrado por benchmark\n",
        "            if joules > JOULES_THRESHOLD:\n",
        "                outlier_info = {\n",
        "                    \"run_key\": run_key,\n",
        "                    \"joules_per_token\": source_data.get('joules_per_token', 0.0),\n",
        "                    \"energy_kwh\": run_data_current['energy_kwh'],\n",
        "                    \"reason\": f\">{JOULES_THRESHOLD:.4f} J/token\"\n",
        "                }\n",
        "                outliers_detected.append(outlier_info)\n",
        "                print(f\"      üö´ Outlier: {run_key} = {joules:.4f} J/token\")\n",
        "            else:\n",
        "                # Valor v√°lido\n",
        "                energy_samples.append(run_data_current['energy_kwh'])\n",
        "                throughput_samples.append(run_data_current['throughput_tokens_per_sec'])\n",
        "                joules_per_token_samples.append(joules)\n",
        "\n",
        "                if run_data_current.get('avg_ttft_ms') is not None:\n",
        "                    ttft_samples.append(run_data_current['avg_ttft_ms'])\n",
        "\n",
        "                try:\n",
        "                    run_index = int(run_key.split('_')[-1]) - 1\n",
        "                    if 0 <= run_index < len(RANDOM_SEEDS):\n",
        "                        valid_seeds_used.append(RANDOM_SEEDS[run_index])\n",
        "                except ValueError:\n",
        "                    print(f\"      ‚ö†Ô∏è Could not parse seed from '{run_key}'\")\n",
        "\n",
        "                run_data = run_data_current\n",
        "\n",
        "        if outliers_detected:\n",
        "            outliers_log[workload_name] = outliers_detected\n",
        "\n",
        "        # Verificar datos v√°lidos\n",
        "        if not energy_samples:\n",
        "            print(f\"      ‚ùå No valid data for {workload_name}\")\n",
        "            aggregated[workload_name] = {\n",
        "                \"error\": \"All measurements were outliers\",\n",
        "                \"outliers_excluded\": outliers_detected\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        print(f\"      ‚úÖ {len(energy_samples)} valid measurements\")\n",
        "\n",
        "        # Raw runs solo con v√°lidos\n",
        "        valid_raw_runs = {}\n",
        "        for run_key in valid_runs.keys():\n",
        "            if workload_name in valid_runs[run_key]:\n",
        "                source_data = valid_runs[run_key][workload_name]\n",
        "\n",
        "                valid_raw_runs[run_key] = {\n",
        "                    # --- Datos existentes ---\n",
        "                    \"energy_kwh\": source_data['energy_kwh'], # Net (actual)\n",
        "                    \"throughput\": source_data['throughput_tokens_per_sec'],\n",
        "                    \"ttft_ms\": source_data.get('avg_ttft_ms'),\n",
        "                    \"joules_per_token\": source_data.get('joules_per_token', 0.0),\n",
        "\n",
        "                    # --- NUEVOS DATOS A√ëADIDOS ---\n",
        "                    \"energy_raw_kwh\": source_data.get('energy_raw_kwh'),\n",
        "                    \"energy_idle_kwh\": source_data.get('energy_idle_kwh'),\n",
        "                    \"total_new_tokens\": source_data.get('total_new_tokens'),\n",
        "                    \"inference_duration_sec\": source_data.get('hardware_metadata', {}).get('duration_sec',\n",
        "                          source_data.get('total_loop_time_sec')),\n",
        "                    \"memory_allocated_gb\": source_data.get('model_size_gb') # O 'memory_allocated_gb' si est√° disponible\n",
        "                }\n",
        "\n",
        "        # Calcular estad√≠sticas\n",
        "        aggregated[workload_name] = {\n",
        "            \"energy_kwh_mean\": float(np.mean(energy_samples)),\n",
        "            \"energy_kwh_std\": float(np.std(energy_samples, ddof=1)) if len(energy_samples) > 1 else 0.0,\n",
        "            \"energy_kwh_min\": float(np.min(energy_samples)),\n",
        "            \"energy_kwh_max\": float(np.max(energy_samples)),\n",
        "\n",
        "            \"throughput_mean\": float(np.mean(throughput_samples)),\n",
        "            \"throughput_std\": float(np.std(throughput_samples, ddof=1)) if len(throughput_samples) > 1 else 0.0,\n",
        "            \"throughput_min\": float(np.min(throughput_samples)),\n",
        "            \"throughput_max\": float(np.max(throughput_samples)),\n",
        "\n",
        "            \"ttft_mean\": float(np.mean(ttft_samples)) if ttft_samples else None,\n",
        "            \"ttft_std\": float(np.std(ttft_samples, ddof=1)) if len(ttft_samples) > 1 else None,\n",
        "            \"ttft_min\": float(np.min(ttft_samples)) if ttft_samples else None,\n",
        "            \"ttft_max\": float(np.max(ttft_samples)) if ttft_samples else None,\n",
        "\n",
        "            \"joules_per_token_mean\": float(np.mean(joules_per_token_samples)),\n",
        "            \"joules_per_token_std\": float(np.std(joules_per_token_samples, ddof=1)) if len(joules_per_token_samples) > 1 else 0.0,\n",
        "            \"joules_per_token_min\": float(np.min(joules_per_token_samples)),\n",
        "            \"joules_per_token_max\": float(np.max(joules_per_token_samples)),\n",
        "\n",
        "            \"num_runs_aggregated\": len(energy_samples),\n",
        "            \"seeds_used\": valid_seeds_used,\n",
        "            \"model_size_gb\": run_data.get('model_size_gb', 0),\n",
        "            \"batch_size\": run_data.get('batch_size', 0),\n",
        "            \"num_prompts\": run_data.get('num_prompts', 0),\n",
        "\n",
        "            \"outliers_excluded\": outliers_detected if outliers_detected else None,\n",
        "            \"outliers_count\": len(outliers_detected),\n",
        "\n",
        "            \"raw_runs_data\": valid_raw_runs\n",
        "        }\n",
        "\n",
        "    return aggregated, outliers_log, problematic_runs\n",
        "\n",
        "# Aggregate results\n",
        "aggregated_results = {}\n",
        "all_outliers_log = {}\n",
        "all_problematic_runs = {}\n",
        "\n",
        "if 'all_results' not in globals():\n",
        "    print(\"‚ùå ERROR: 'all_results' not defined.\")\n",
        "else:\n",
        "    for model_key, runs_dict in all_results.items():\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Aggregating {model_key}...\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        aggregated, outliers_log, problematic_runs = aggregate_runs(runs_dict)\n",
        "        aggregated_results[model_key] = aggregated\n",
        "\n",
        "        if outliers_log:\n",
        "            all_outliers_log[model_key] = outliers_log\n",
        "        if problematic_runs:\n",
        "            all_problematic_runs[model_key] = problematic_runs\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"‚úÖ AGGREGATION COMPLETE\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"   Models processed: {len(aggregated_results)}\")\n",
        "    print(f\"   Total problematic runs excluded: {sum(len(v) for v in all_problematic_runs.values())}\")\n",
        "    print(f\"   Total individual outliers: {sum(len(v) for v in all_outliers_log.values())}\")\n",
        "\n",
        "    # Reporte detallado\n",
        "    if all_problematic_runs:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"üö´ PROBLEMATIC RUNS EXCLUDED (>50% outliers)\")\n",
        "        print(f\"{'='*70}\")\n",
        "        for model_key, runs in all_problematic_runs.items():\n",
        "            print(f\"\\n{model_key}:\")\n",
        "            for run_key in runs.keys():\n",
        "                print(f\"  ‚ùå {run_key}\")\n",
        "\n",
        "    if all_outliers_log:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"üö´ INDIVIDUAL OUTLIERS EXCLUDED\")\n",
        "        print(f\"{'='*70}\")\n",
        "        for model_key, workloads in all_outliers_log.items():\n",
        "            print(f\"\\n{model_key}:\")\n",
        "            for workload_name, outliers in workloads.items():\n",
        "                print(f\"  {workload_name}:\")\n",
        "                for outlier in outliers:\n",
        "                    print(f\"    - {outlier['run_key']}: {outlier['joules_per_token']:.4f} J/token\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")"
      ],
      "metadata": {
        "id": "gIdYGp9FUyei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5RqknL6IvXH"
      },
      "outputs": [],
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"üìä  CONSOLIDATING AGGREGATED RESULTS TO CSV/JSON\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# Build consolidated data using AGGREGATED results\n",
        "consolidated_data = []\n",
        "\n",
        "for model_key, workload_results in aggregated_results.items():\n",
        "    display_name = model_names.get(model_key, model_key)\n",
        "    pruning_pct = model_pruning.get(model_key, 0)\n",
        "    is_star = model_is_star.get(model_key, False)\n",
        "\n",
        "    for workload_name, metrics in workload_results.items():\n",
        "        row = {\n",
        "            \"model\": display_name,\n",
        "            \"pruning_pct\": pruning_pct,\n",
        "            \"is_star\": is_star,\n",
        "            \"workload\": workload_name,\n",
        "\n",
        "            # Aggregated metrics (mean ¬± std)\n",
        "            \"energy_kwh_mean\": metrics[\"energy_kwh_mean\"],\n",
        "            \"energy_kwh_std\": metrics[\"energy_kwh_std\"],\n",
        "            \"throughput_mean\": metrics[\"throughput_mean\"],\n",
        "            \"throughput_std\": metrics[\"throughput_std\"],\n",
        "            \"joules_per_token_mean\": metrics[\"joules_per_token_mean\"],\n",
        "            \"joules_per_token_std\": metrics[\"joules_per_token_std\"],\n",
        "            \"ttft_mean\": metrics.get(\"ttft_mean\"),\n",
        "            \"ttft_std\": metrics.get(\"ttft_std\"),\n",
        "\n",
        "            # Metadata\n",
        "            \"num_runs\": metrics[\"num_runs_aggregated\"], # <--- L√çNEA CORREGIDA\n",
        "            \"model_size_gb\": metrics[\"model_size_gb\"],\n",
        "            \"batch_size\": metrics[\"batch_size\"],\n",
        "        }\n",
        "        consolidated_data.append(row)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(consolidated_data)\n",
        "df = df.sort_values(by=[\"pruning_pct\", \"workload\"]).reset_index(drop=True)\n",
        "\n",
        "print(f\"‚úÖ  Consolidated {len(df)} result rows (with aggregated statistics)\")\n",
        "print(f\"   Models: {df['model'].nunique()}\")\n",
        "print(f\"   Workloads: {df['workload'].nunique()}\")\n",
        "# Usamos el dataframe para asegurar que contamos los runs correctos\n",
        "print(f\"   Runs per configuration: {df['num_runs'].max()}\")\n",
        "\n",
        "# Display preview with mean ¬± std\n",
        "print(\"\\nDataFrame Preview (Aggregated Results):\")\n",
        "print(df[['model', 'workload', 'energy_kwh_mean', 'energy_kwh_std', 'throughput_mean', 'throughput_std']].head(10))\n",
        "\n",
        "# Save to CSV\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "csv_path = f\"{RESULTS_DIR}/carbon_1b_aggregated_results_{timestamp}.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\"\\nüíæ  Aggregated results saved to: {csv_path}\")\n",
        "\n",
        "# Also save raw runs for transparency\n",
        "raw_runs_path = f\"{RESULTS_DIR}/carbon_1b_raw_runs_{timestamp}.json\"\n",
        "with open(raw_runs_path, 'w') as f:\n",
        "    json.dump(all_results, f, indent=2, default=str)  # default=str handles numpy types\n",
        "\n",
        "print(f\"üíæ  Raw runs data saved to: {raw_runs_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"üìà CARBON & PERFORMANCE ANALYSIS\")\n",
        "print(f\"S{'='*70}\\n\")\n",
        "\n",
        "# Calculate averages per model across workloads\n",
        "summary_data = []\n",
        "\n",
        "# ===================================================================\n",
        "# START: FIX\n",
        "# ===================================================================\n",
        "\n",
        "# Use dynamic keys from aggregated_results (FIX 1)\n",
        "for model_key in aggregated_results.keys():\n",
        "    # Get the aggregated workload results for this model (FIX 2)\n",
        "    workload_results = aggregated_results[model_key]\n",
        "\n",
        "    # Aggregate metrics across workloads using the '_mean' keys (FIX 3)\n",
        "    total_energy = sum(m['energy_kwh_mean'] for m in workload_results.values())\n",
        "    avg_throughput = np.mean([m['throughput_mean'] for m in workload_results.values()])\n",
        "    ttft_values = [m['ttft_mean'] for m in workload_results.values() if m.get('ttft_mean') is not None]\n",
        "\n",
        "    # --- A√ëADIDO ---\n",
        "    joules_per_token_values = [m['joules_per_token_mean'] for m in workload_results.values() if m.get('joules_per_token_mean') is not None]\n",
        "    avg_joules_per_token = np.mean(joules_per_token_values) if joules_per_token_values else 0.0\n",
        "    # --- FIN A√ëADIDO ---\n",
        "# ===================================================================\n",
        "# END: FIX\n",
        "# ===================================================================\n",
        "\n",
        "    avg_ttft = np.mean(ttft_values) if ttft_values else 0.0 # Usar np.mean\n",
        "    model_size = list(workload_results.values())[0]['model_size_gb']  # Same for all workloads\n",
        "\n",
        "    summary = {\n",
        "        \"model\": model_names.get(model_key, model_key),\n",
        "        \"pruning_pct\": model_pruning.get(model_key, 0),\n",
        "        \"is_star\": model_is_star.get(model_key, False),\n",
        "        \"total_energy_kwh\": total_energy,\n",
        "        \"avg_throughput_tok_s\": avg_throughput,\n",
        "        \"avg_joules_per_token\": avg_joules_per_token, # <-- A√ëADIDO\n",
        "        \"avg_ttft_ms\": avg_ttft,\n",
        "        \"model_size_gb\": model_size\n",
        "    }\n",
        "    summary_data.append(summary)\n",
        "\n",
        "# Sort by pruning_pct for consistent display\n",
        "summary_df = pd.DataFrame(summary_data).sort_values('pruning_pct').reset_index(drop=True)\n",
        "\n",
        "# --- ACTUALIZAR PRINT ---\n",
        "print(\"Performance Summary (Calculations Corrected):\")\n",
        "print(\"-\" * 110) # Ampliar tabla\n",
        "print(summary_df.to_string(index=False, float_format=\"%.4f\"))\n",
        "print(\"-\" * 110) # Ampliar tabla\n",
        "# --- FIN ACTUALIZAR PRINT ---\n",
        "\n",
        "# Calculate improvements vs baseline\n",
        "baseline_row = summary_df[summary_df['pruning_pct'] == 0].iloc[0]\n",
        "\n",
        "print(f\"\\nüí° Performance vs. Baseline:\")\n",
        "print(\"-\" * 90)\n",
        "\n",
        "for _, row in summary_df.iterrows():\n",
        "    if row['pruning_pct'] == 0:\n",
        "        continue\n",
        "\n",
        "    # === METRICS WHERE LESS IS BETTER (calculate REDUCTION) ===\n",
        "    # Positive = improvement (less consumption/latency)\n",
        "    # Negative = degradation (more consumption/latency)\n",
        "\n",
        "    energy_total_reduction = ((baseline_row['total_energy_kwh'] - row['total_energy_kwh']) / baseline_row['total_energy_kwh']) * 100\n",
        "\n",
        "    joules_per_token_reduction = ((baseline_row['avg_joules_per_token'] - row['avg_joules_per_token']) / baseline_row['avg_joules_per_token']) * 100\n",
        "\n",
        "    ttft_reduction = ((baseline_row['avg_ttft_ms'] - row['avg_ttft_ms']) / baseline_row['avg_ttft_ms']) * 100\n",
        "\n",
        "    size_reduction = ((baseline_row['model_size_gb'] - row['model_size_gb']) / baseline_row['model_size_gb']) * 100\n",
        "\n",
        "    # === METRICS WHERE MORE IS BETTER (calculate INCREASE) ===\n",
        "    # Positive = improvement (more throughput/tokens)\n",
        "    # Negative = degradation (less throughput/tokens)\n",
        "\n",
        "    throughput_increase = ((row['avg_throughput_tok_s'] - baseline_row['avg_throughput_tok_s']) / baseline_row['avg_throughput_tok_s']) * 100\n",
        "\n",
        "    tokens_increase = ((row['total_generated_tokens'] - baseline_row['total_generated_tokens']) / baseline_row['total_generated_tokens']) * 100\n",
        "\n",
        "    # Display results\n",
        "    star_marker = \"‚≠ê\" if row['is_star'] else \"\"\n",
        "    print(f\"\\n{row['model']} {star_marker}\")\n",
        "\n",
        "    # Energy (Total) - LESS IS BETTER\n",
        "    print(f\"  Energy (Total): {energy_total_reduction:+.2f}% ({'‚úÖ Lower' if energy_total_reduction > 0 else '‚ùå Higher'})\")\n",
        "\n",
        "    # Tokens Generated - MORE IS BETTER\n",
        "    print(f\"  Tokens Generated: {tokens_increase:+.2f}% ({'‚úÖ More Output' if tokens_increase > 0 else '‚ö†Ô∏è Less Output'})\")\n",
        "\n",
        "    # Energy per Token - LESS IS BETTER\n",
        "    print(f\"  Energy (J/Token): {joules_per_token_reduction:+.2f}% ({'‚úÖ More Efficient' if joules_per_token_reduction > 0 else '‚ùå Less Efficient'})\")\n",
        "\n",
        "    # Throughput - MORE IS BETTER\n",
        "    print(f\"  Throughput: {throughput_increase:+.2f}% ({'‚úÖ Faster' if throughput_increase > 0 else '‚ö†Ô∏è Slower'})\")\n",
        "\n",
        "    # Latency (TTFT) - LESS IS BETTER\n",
        "    print(f\"  Latency (TTFT): {ttft_reduction:+.2f}% ({'‚úÖ Lower' if ttft_reduction > 0 else '‚ö†Ô∏è Higher'})\")\n",
        "\n",
        "    # Model Size - LESS IS BETTER\n",
        "    print(f\"  Model Size: {size_reduction:+.2f}% ({'‚úÖ Smaller' if size_reduction > 0 else '‚ö†Ô∏è Larger'})\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 90)"
      ],
      "metadata": {
        "id": "TBPO5o8oteVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxHStHvMIvXH"
      },
      "source": [
        "# 6. Quick Analysis & Visualization\n",
        "\n",
        "Generate quick insights to decide which models merit uploading to HuggingFace Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xo_bifVCIvXI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Create comprehensive visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "models = summary_df['model'].values\n",
        "pruning_levels = summary_df['pruning_pct'].values\n",
        "star_mask = summary_df['is_star'].values\n",
        "\n",
        "# Colors: baseline blue, star gold, others gray\n",
        "colors = ['#1f77b4' if p == 0 else '#FFD700' if s else '#95a5a6'\n",
        "          for p, s in zip(pruning_levels, star_mask)]\n",
        "\n",
        "# 1. Energy Consumption\n",
        "axes[0, 0].bar(range(len(models)), summary_df['total_energy_kwh'], color=colors, edgecolor='black', linewidth=1.5)\n",
        "axes[0, 0].set_xticks(range(len(models)))\n",
        "axes[0, 0].set_xticklabels([m.replace('Llama-3.2-', '') for m in models], rotation=0, ha='center')\n",
        "axes[0, 0].set_ylabel('Total Energy (kWh)', fontsize=11)\n",
        "axes[0, 0].set_title('Energy Consumption', fontsize=13, fontweight='bold')\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(summary_df['total_energy_kwh']):\n",
        "    axes[0, 0].text(i, v + 0.0001, f'{v:.4f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 2. Throughput\n",
        "axes[0, 1].bar(range(len(models)), summary_df['avg_throughput_tok_s'], color=colors, edgecolor='black', linewidth=1.5)\n",
        "axes[0, 1].set_xticks(range(len(models)))\n",
        "axes[0, 1].set_xticklabels([m.replace('Llama-3.2-', '') for m in models], rotation=0, ha='center')\n",
        "axes[0, 1].set_ylabel('Throughput (tokens/s)', fontsize=11)\n",
        "axes[0, 1].set_title('Inference Throughput', fontsize=13, fontweight='bold')\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(summary_df['avg_throughput_tok_s']):\n",
        "    axes[0, 1].text(i, v + 0.5, f'{v:.1f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 3. Latency (TTFT)\n",
        "axes[1, 0].bar(range(len(models)), summary_df['avg_ttft_ms'], color=colors, edgecolor='black', linewidth=1.5)\n",
        "axes[1, 0].set_xticks(range(len(models)))\n",
        "axes[1, 0].set_xticklabels([m.replace('Llama-3.2-', '') for m in models], rotation=0, ha='center')\n",
        "axes[1, 0].set_ylabel('Avg TTFT (ms)', fontsize=11)\n",
        "axes[1, 0].set_title('Latency (Time To First Token)', fontsize=13, fontweight='bold')\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(summary_df['avg_ttft_ms']):\n",
        "    axes[1, 0].text(i, v + 1, f'{v:.1f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 4. Model Size\n",
        "axes[1, 1].bar(range(len(models)), summary_df['model_size_gb'], color=colors, edgecolor='black', linewidth=1.5)\n",
        "axes[1, 1].set_xticks(range(len(models)))\n",
        "axes[1, 1].set_xticklabels([m.replace('Llama-3.2-', '') for m in models], rotation=0, ha='center')\n",
        "axes[1, 1].set_ylabel('Model Size (GB)', fontsize=11)\n",
        "axes[1, 1].set_title('Model Size', fontsize=13, fontweight='bold')\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(summary_df['model_size_gb']):\n",
        "    axes[1, 1].text(i, v + 0.02, f'{v:.2f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{RESULTS_DIR}/carbon_1b_analysis.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìä Visualization saved to: {RESULTS_DIR}/carbon_1b_analysis.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4H7r1PNIvXI"
      },
      "source": [
        "# 7. Agregating results."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NUEVO C√ìDIGO PARA LA CELDA 42\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"üìä CONSTRUCTING FINAL JSON FROM AGGREGATED RESULTS\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# --- Validar variables globales ---\n",
        "# (Asegurarse de que las variables de celdas anteriores existen)\n",
        "\n",
        "if 'RESULTS_DIR' not in globals():\n",
        "    print(\"‚ö†Ô∏è Warning: RESULTS_DIR not set. Using default './results'\")\n",
        "    RESULTS_DIR = \"/content/drive/MyDrive/glu_pruning/results\"\n",
        "\n",
        "if 'BENCHMARKS_CARBON' not in globals():\n",
        "    print(\"‚ùå Error: BENCHMARKS_CARBON not defined. Recargando fallback.\")\n",
        "    BENCHMARKS_CARBON = [\n",
        "        {\"name\": \"gsm8k_latency_b1\", \"num_prompts\": 10, \"max_new_tokens\": 100, \"dataset\": \"gsm8k\", \"subset\": \"test\", \"description\": \"Math reasoning (Latency, TTFT, bsz=1)\", \"batch_size\": 1},\n",
        "        {\"name\": \"mmlu_latency_b1\", \"num_prompts\": 10, \"max_new_tokens\": 50, \"dataset\": \"mmlu\", \"subset\": \"test\", \"description\": \"Knowledge QA (Latency, TTFT, bsz=1)\", \"batch_size\": 1},\n",
        "        {\"name\": \"ifeval_latency_b1\", \"num_prompts\": 10, \"max_new_tokens\": 150, \"dataset\": \"IFEval\", \"subset\": \"train\", \"description\": \"Instruction (Latency, TTFT, bsz=1)\", \"batch_size\": 1},\n",
        "        {\"name\": \"gsm8k_throughput_b8\", \"num_prompts\": 10, \"max_new_tokens\": 100, \"dataset\": \"gsm8k\", \"subset\": \"test\", \"description\": \"Math reasoning (Throughput, bsz=8)\", \"batch_size\": 8},\n",
        "        {\"name\": \"mmlu_throughput_b8\", \"num_prompts\": 10, \"max_new_tokens\": 50, \"dataset\": \"mmlu\", \"subset\": \"test\", \"description\": \"Knowledge QA (Throughput, bsz=8)\", \"batch_size\": 8},\n",
        "        {\"name\": \"ifeval_throughput_b8\", \"num_prompts\": 10, \"max_new_tokens\": 150, \"dataset\": \"IFEval\", \"subset\": \"train\", \"description\": \"Instruction (Throughput, bsz=8)\", \"batch_size\": 8},\n",
        "    ]\n",
        "\n",
        "# --- Usar resultados ya agregados y filtrados ---\n",
        "if 'aggregated_results' not in globals() or 'all_results' not in globals():\n",
        "    print(\"‚ùå FATAL ERROR: 'aggregated_results' o 'all_results' no est√°n definidos.\")\n",
        "    print(\"   Por favor, ejecute de nuevo las celdas 38 y 39 antes de esta.\")\n",
        "else:\n",
        "    print(f\"Usando 'aggregated_results' (calculados en Celda 39) para {len(aggregated_results)} modelos.\")\n",
        "\n",
        "    # --- Construir el JSON Completo ---\n",
        "\n",
        "    # 1. Construir mapeo de configuraci√≥n\n",
        "    config_map = {}\n",
        "    model_family_name = \"Llama-3.2-1B\"\n",
        "\n",
        "    for cfg in EXPERIMENT_CONFIG_CARBON:\n",
        "        if model_family_name not in cfg[\"base_model\"]:\n",
        "            continue\n",
        "\n",
        "        pruning_pct = cfg['pruning_pct']\n",
        "        # La clave debe coincidir con la usada en 'aggregated_results' (p.ej. 'baseline', '20pct', '40pct')\n",
        "        key = \"baseline\" if pruning_pct == 0 else f\"{pruning_pct}pct\"\n",
        "        model_name_cfg = f\"{model_family_name}\" if pruning_pct == 0 else f\"{model_family_name}-pruned-{pruning_pct}%\"\n",
        "\n",
        "        hf_repo = cfg.get(\"hf_repo_id\", cfg[\"base_model\"]) if pruning_pct > 0 else cfg[\"base_model\"]\n",
        "\n",
        "        config_map[key] = {\n",
        "            \"name\": model_name_cfg,\n",
        "            \"pruning_pct\": int(pruning_pct),\n",
        "            \"is_star\": bool(cfg[\"is_star\"]),\n",
        "            \"hf_repo\": hf_repo,\n",
        "        }\n",
        "\n",
        "    print(f\"\\nDynamically built config map for {len(config_map)} models.\")\n",
        "\n",
        "    # 2. Construir 'models_evaluated'\n",
        "    models_evaluated = {}\n",
        "    summary_stats_list = []\n",
        "\n",
        "    for model_key, workload_results in aggregated_results.items():\n",
        "        if model_key in config_map:\n",
        "            config = config_map[model_key]\n",
        "\n",
        "            # Usar los resultados de workload_results (ya filtrados por outliers)\n",
        "            models_evaluated[model_key] = {\n",
        "                \"name\": config[\"name\"],\n",
        "                \"pruning_pct\": config[\"pruning_pct\"],\n",
        "                \"is_star\": config[\"is_star\"],\n",
        "                \"hf_repo\": config[\"hf_repo\"],\n",
        "                \"results\": workload_results\n",
        "            }\n",
        "\n",
        "            # Calcular estad√≠sticas de resumen (basado en Celda 40)\n",
        "            valid_workloads = [w for w in workload_results.values() if 'error' not in w]\n",
        "            if valid_workloads:\n",
        "                joules_per_token_values = [m['joules_per_token_mean'] for m in valid_workloads if m.get('joules_per_token_mean') is not None]\n",
        "                avg_joules_per_token = float(np.mean(joules_per_token_values)) if joules_per_token_values else 0.0\n",
        "                ttft_values = [m['ttft_mean'] for m in valid_workloads if m.get('ttft_mean') is not None]\n",
        "                avg_ttft = float(np.mean(ttft_values)) if ttft_values else 0.0\n",
        "\n",
        "                summary_stats_list.append({\n",
        "                    \"model\": config[\"name\"],\n",
        "                    \"pruning_pct\": config[\"pruning_pct\"],\n",
        "                    \"is_star\": config[\"is_star\"],\n",
        "                    \"total_energy_kwh\": sum(m.get('energy_kwh_mean', 0) for m in valid_workloads),\n",
        "                    \"avg_throughput_tok_s\": np.mean([m.get('throughput_mean', 0) for m in valid_workloads]),\n",
        "                    \"avg_joules_per_token\": avg_joules_per_token,\n",
        "                    \"avg_ttft_ms\": avg_ttft,\n",
        "                    \"model_size_gb\": list(valid_workloads)[0].get('model_size_gb', 0)\n",
        "                })\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Warning: No config map entry found for key '{model_key}'.\")\n",
        "\n",
        "    # 3. Construir 'summary_statistics'\n",
        "    baseline_stats = next((s for s in summary_stats_list if s['pruning_pct'] == 0), None)\n",
        "    pruned_stats = sorted([s for s in summary_stats_list if s['pruning_pct'] > 0], key=lambda x: x['pruning_pct'])\n",
        "\n",
        "    summary_statistics = {\n",
        "        \"baseline\": baseline_stats,\n",
        "        \"pruned_models\": pruned_stats\n",
        "    }\n",
        "\n",
        "    # 4. Obtener hardware_info (del primer run del baseline, usando 'all_results')\n",
        "    hardware_info = {}\n",
        "    try:\n",
        "        # 1. CORRECCI√ìN: Usar la clave correcta 'run_1' (generada en la Celda 9)\n",
        "        run_key_to_check = \"run_1\"\n",
        "\n",
        "        if \"baseline\" in all_results and run_key_to_check in all_results[\"baseline\"]:\n",
        "            # 2. Obtener la metadata del primer run/workload como fuente\n",
        "            first_workload_data = list(all_results[\"baseline\"][run_key_to_check].values())[0]\n",
        "\n",
        "            if \"hardware_metadata\" in first_workload_data:\n",
        "                hw_meta = first_workload_data[\"hardware_metadata\"]\n",
        "\n",
        "                # 3. CONSTRUCCI√ìN CURADA: Crear el diccionario limpio que especificaste\n",
        "                hardware_info = {\n",
        "                    \"gpu_model\": hw_meta.get(\"gpu_name_torch\"),\n",
        "                    \"gpu_memory_gb\": round(hw_meta.get(\"gpu_total_memory_gb\", 0), 2),\n",
        "                    \"gpu_compute_capability\": hw_meta.get(\"gpu_compute_capability\"),\n",
        "                    \"cuda_version\": hw_meta.get(\"cuda_version\"),\n",
        "                    \"torch_version\": hw_meta.get(\"torch_version\"),\n",
        "                    \"gpu_temperature_celsius\": hw_meta.get(\"gpu_temperature_celsius\"),\n",
        "                }\n",
        "\n",
        "                # 4. A√ëADIR DATOS DE IDLE: Obtener de la variable global 'idle_calibration' (de la Celda 8)\n",
        "                if 'idle_calibration' in globals():\n",
        "                    hardware_info[\"idle_power_watts\"] = round(idle_calibration.get(\"idle_power_watts\", 0), 2)\n",
        "                    hardware_info[\"idle_power_measurement_duration_sec\"] = idle_calibration.get(\"duration_seconds\")\n",
        "                    hardware_info[\"idle_power_measurement_timestamp\"] = idle_calibration.get(\"timestamp\")\n",
        "                else:\n",
        "                    print(\"    Warning: 'idle_calibration' no encontrada. Faltar√°n datos de idle power.\")\n",
        "\n",
        "                print(f\"    Successfully captured and curated hardware metadata (GPU: {hardware_info.get('gpu_model', 'N/A')})\")\n",
        "            else:\n",
        "                print(\"    'hardware_metadata' not found in first workload.\")\n",
        "        else:\n",
        "            print(f\"    Could not find '{run_key_to_check}' in 'all_results[\\\"baseline\\\"]'. Keys available: {list(all_results.get('baseline', {}).keys())}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    Error extracting hardware info: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "    # 5. Consolidar todo\n",
        "    complete_results = {\n",
        "        \"experiment_metadata\": {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"notebook\": \"03_Evaluate_1B_CARBON.ipynb\",\n",
        "            \"model_family\": model_family_name,\n",
        "            \"pruning_method\": \"MAW (Maximum Absolute Weight)\",\n",
        "            \"hardware_details\": hardware_info\n",
        "        },\n",
        "        \"benchmarks\": [\n",
        "            {\n",
        "                \"name\": task[\"name\"],\n",
        "                \"num_prompts\": task[\"num_prompts\"],\n",
        "                \"max_new_tokens\": task[\"max_new_tokens\"],\n",
        "                \"description\": task[\"description\"]\n",
        "            }\n",
        "            for task in BENCHMARKS_CARBON\n",
        "        ],\n",
        "        \"models_evaluated\": models_evaluated,\n",
        "        \"summary_statistics\": summary_statistics,\n",
        "        \"citation\": {\n",
        "            \"paper\": \"Exploring GLU Expansion Ratios: Structured Pruning in Llama-3.2 Models\",\n",
        "            \"author\": \"Pere Martra\",\n",
        "            \"doi\": \"https://doi.org/10.31219/osf.io/qgxea\",\n",
        "            \"github\": \"https://github.com/peremartra/llama-glu-expansion-pruning\",\n",
        "            \"note\": \"Results are freely available for research purposes. Please cite the paper if you use this data.\"\n",
        "        }\n",
        "    }\n",
        "    print(f\"\\nSuccessfully consolidated results for {len(models_evaluated)} models.\")\n",
        "\n",
        "    # --- Guardar en JSON ---\n",
        "    try:\n",
        "        os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        json_path = f\"{RESULTS_DIR}/llama_1b_carbon_complete_results_{timestamp}.json\"\n",
        "        with open(json_path, 'w') as f:\n",
        "            json.dump(complete_results, f, indent=2, ensure_ascii=False, default=str)\n",
        "\n",
        "        print(f\"\\n‚úÖ Complete (and filtered) carbon results saved to:\")\n",
        "        print(f\"   {json_path}\")\n",
        "\n",
        "        latest_json = f\"{RESULTS_DIR}/llama_1b_carbon_complete_results_latest.json\"\n",
        "        with open(latest_json, 'w') as f:\n",
        "            json.dump(complete_results, f, indent=2, ensure_ascii=False, default=str)\n",
        "\n",
        "        print(f\"‚úÖ Latest (and filtered) version:\")\n",
        "        print(f\"   {latest_json}\")\n",
        "\n",
        "        file_size_kb = Path(json_path).stat().st_size / 1024\n",
        "        print(f\"\\nüìä File size: {file_size_kb:.1f} KB\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error saving JSON files: {e}\")\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"‚úÖ COMPLETE CARBON RESULTS SAVED - Ready for research sharing\")\n",
        "    print(f\"{'='*70}\\n\")"
      ],
      "metadata": {
        "id": "nfjYJe0yNNhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6JVP-5RIvXI"
      },
      "source": [
        "---\n",
        "\n",
        "## üéì Key Takeaways\n",
        "\n",
        "This notebook evaluated the Llama-3.2-1B model family across different datasets to determine:\n",
        "\n",
        "1. **Optimal pruning level** for GLU-MLP layers\n",
        "2. **Performance-Carbon emission trade-offs** at different expansion ratios\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Powered by [OptiPFair](https://github.com/peremartra/optipfair)** - Structured Pruning for GLU Architectures\n",
        "\n",
        "If this research helps your work:\n",
        "- ‚≠ê Star [the repo](https://github.com/peremartra/optipfair)\n",
        "- üìñ Read the [documentation](https://peremartra.github.io/optipfair/)\n",
        "- üêõ Report issues or suggest features\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DvcQV-q0S_XI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}