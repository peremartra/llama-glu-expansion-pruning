{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "93f8209749b340a3a19de9afd8115cc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7dada4dab53b4769890141d06a65613e",
              "IPY_MODEL_fff1a9ee634148348cf24395046b2879",
              "IPY_MODEL_15239a9b2c744fd4a65c3c5b64047311"
            ],
            "layout": "IPY_MODEL_7d6076a7bb5345459c06b644d4f991e1"
          }
        },
        "7dada4dab53b4769890141d06a65613e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e368237697d4f478122891335f2e4a0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_986ed0bd03c5462d9234300caa0ffad4",
            "value": "config.json:‚Äá100%"
          }
        },
        "fff1a9ee634148348cf24395046b2879": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69f9c2b3560646478fa85829768d9d23",
            "max": 877,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6d2ca964c7749ca99d5f038802cdb44",
            "value": 877
          }
        },
        "15239a9b2c744fd4a65c3c5b64047311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bad59835e2b41a2be9a09f3d142ae48",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9c9337664aa44ec699fffa6117d65b69",
            "value": "‚Äá877/877‚Äá[00:00&lt;00:00,‚Äá111kB/s]"
          }
        },
        "7d6076a7bb5345459c06b644d4f991e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e368237697d4f478122891335f2e4a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "986ed0bd03c5462d9234300caa0ffad4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69f9c2b3560646478fa85829768d9d23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6d2ca964c7749ca99d5f038802cdb44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8bad59835e2b41a2be9a09f3d142ae48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c9337664aa44ec699fffa6117d65b69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98b3ae78de164ae794fcada2547928e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1a40ad56028847d6bbfb60eb7a97cd80",
              "IPY_MODEL_3fdca068796a4419bff7612d53e9e871",
              "IPY_MODEL_ecf1e80d063744028c348c0422c39a07"
            ],
            "layout": "IPY_MODEL_0505592b9c9441f8b5d31f4251cce914"
          }
        },
        "1a40ad56028847d6bbfb60eb7a97cd80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_826242079df34663a6ff06f55cf4fd3d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a547d433d6fa41ec820e27cda2b1465c",
            "value": "model.safetensors:‚Äá‚Äá63%"
          }
        },
        "3fdca068796a4419bff7612d53e9e871": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2a589ca98784b42b39e244463b367f2",
            "max": 2471645608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5fac6575b7384d899f89cc3a78bf52c3",
            "value": 1562880888
          }
        },
        "ecf1e80d063744028c348c0422c39a07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cb344c08b27474f9b1876884ac6e593",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_961c998cd92c4c4eab4c9d40c546a388",
            "value": "‚Äá1.56G/2.47G‚Äá[00:03&lt;00:01,‚Äá506MB/s]"
          }
        },
        "0505592b9c9441f8b5d31f4251cce914": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "826242079df34663a6ff06f55cf4fd3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a547d433d6fa41ec820e27cda2b1465c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2a589ca98784b42b39e244463b367f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fac6575b7384d899f89cc3a78bf52c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7cb344c08b27474f9b1876884ac6e593": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "961c998cd92c4c4eab4c9d40c546a388": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/llama-glu-expansion-pruning/blob/main/notebooks/02_Evaluate_1B_Instruct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzfjNg_SIvXD"
      },
      "source": [
        "# GLU Pruning Research - Llama-3.2-1B-Instruct Evaluation\n",
        "## 02 - Comprehensive Benchmark Suite Evaluation\n",
        "\n",
        "### Exploring GLU Expansion Ratios in Llama-3.2 Models\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "\n",
        "[![Paper](https://img.shields.io/badge/OSF-Paper-blue?logo=osf&logoColor=white)](https://doi.org/10.31219/osf.io/qgxea)\n",
        "[![GitHub](https://img.shields.io/badge/‚≠ê_Star-OptiPFair-orange?logo=github&logoColor=white)](https://github.com/peremartra/optipfair)\n",
        "[![PyPI](https://img.shields.io/pypi/v/optipfair?logo=python&logoColor=white&label=v)](https://pypi.org/project/optipfair/)\n",
        "\n",
        "**Repository:** [github.com/peremartra/llama-glu-expansion-pruning](https://github.com/peremartra/llama-glu-expansion-pruning)\n",
        "\n",
        "---\n",
        "\n",
        "**Colab Environment:** GPU L4 (or T4)\n",
        "\n",
        "**Models to Evaluate:**\n",
        "* Llama-3.2-1B-Instruct (base) - Baseline\n",
        "* Llama-3.2-1B-I-pruned-40% (140% expansion) ‚≠ê Star model\n",
        "* Llama-3.2-1B-I-pruned-60% (60% expansion)\n",
        "\n",
        "**Benchmarks (5 total):**\n",
        "* Lambada-OpenAI (0-shot)\n",
        "* MMLU (5-shot)\n",
        "* TruthfulQA MC1/MC2 (0-shot)\n",
        "* GSM8K (5-shot CoT)\n",
        "* IfEval (0-shot)\n",
        "* leaderboard_musr (0-shot)\n",
        "\n",
        "**Estimated Runtime:** ~4-5 hours total\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Notebook Objective\n",
        "\n",
        "GLU expansion pruning (using the MAW method) appears to remove the \"capacity for digression\" or \"computational noise\" from the base model. This harms memorization (mmlu/gsm8k) but improves tasks requiring literal adherence to instructions (ifeval/musr).\n",
        "\n",
        "---\n",
        "\n",
        "**Note:** This evaluation uses the MAW (Maximum Absolute Weight) neuron selection method, validated in Notebook 00 as the optimal approach for GLU architectures.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkVFbeCMIvXF"
      },
      "source": [
        "# 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nAo67s0lIvXF",
        "outputId": "3794529b-aff4-4873-dab6-7e5fcc327f03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install -q optipfair\n",
        "!pip install -q lm-eval\n",
        "!pip install -q langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWIHQuIGIvXG",
        "outputId": "ccf83adf-e3fa-4b06-d0c6-8faaee7d5385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive for checkpoint persistence\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG2nO7YpIvXG",
        "outputId": "cfc0ac4f-fc11-43bf-e9c9-040712900dd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ utils.py downloaded successfully\n"
          ]
        }
      ],
      "source": [
        "# Download utils.py from GitHub repository\n",
        "!wget -q https://raw.githubusercontent.com/peremartra/llama-glu-expansion-pruning/main/utils.py\n",
        "\n",
        "# Verify download\n",
        "import os\n",
        "if os.path.exists('utils.py'):\n",
        "    print(\"‚úÖ utils.py downloaded successfully\")\n",
        "else:\n",
        "    print(\"‚ùå Failed to download utils.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHjkx6_QIvXG",
        "outputId": "ed7e267a-c9d4-447a-a086-4d108add52de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All imports successful\n",
            "üì± Device: GPU\n",
            "   GPU: NVIDIA L4\n",
            "   Memory: 23.8 GB\n"
          ]
        }
      ],
      "source": [
        "# Import core libraries and utilities\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# Import our utility functions\n",
        "from utils import (\n",
        "    EXPERIMENT_CONFIG,\n",
        "    BENCHMARKS_INSTRUCT,\n",
        "    load_or_create_model,\n",
        "    run_robust_evaluation,\n",
        "    clear_gpu_cache,\n",
        "    get_model_stats,\n",
        "    format_results_table\n",
        ")\n",
        "\n",
        "print(\"‚úÖ All imports successful\")\n",
        "print(f\"üì± Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LixoDuXJIvXG"
      },
      "source": [
        "# 2. Configuration & Planning\n",
        "\n",
        "This section filters the experiment configuration for 1B models and displays the evaluation plan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5fDt6IxIvXG",
        "outputId": "90fb7c93-d837-4279-bd95-bf4ff6eb0a5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "üìä EVALUATION PLAN: Llama-3.2-1B-Instruct Family\n",
            "======================================================================\n",
            "\n",
            "Total models to evaluate: 4\n",
            "Benchmarks per model: 7\n",
            "Total evaluations: 28\n",
            "Estimated runtime: ~4-5 hours\n",
            "\n",
            "Models to evaluate:\n",
            "----------------------------------------------------------------------\n",
            "Model                          Pruning    Star  \n",
            "----------------------------------------------------------------------\n",
            "Llama-3.2-1B-Instruct (baseline) 0%         300%         N/A   \n",
            "Llama-3.2-1B-I-pruned-10pct    10%        No    \n",
            "Llama-3.2-1B-I-pruned-40pct    40%        ‚≠ê Yes \n",
            "Llama-3.2-1B-I-pruned-60pct    60%        No    \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Benchmarks to run:\n",
            "----------------------------------------------------------------------\n",
            " 1. leaderboard_musr          0-shot\n",
            " 2. truthfulqa_mc1            0-shot\n",
            " 3. truthfulqa_mc2            0-shot\n",
            " 4. lambada_openai            0-shot\n",
            " 5. mmlu                      5-shot\n",
            " 6. gsm8k                     5-shot\n",
            " 7. ifeval                    0-shot\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "‚öôÔ∏è  Configuration:\n",
            "   - Neuron selection method: MAW (Maximum Absolute Weight)\n",
            "   - Checkpointing: Enabled (per-task granularity)\n",
            "   - Model creation: On-the-fly pruning (no pre-creation needed)\n",
            "   - Error handling: Skip failed tasks and continue\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Filter configuration for 1B models only\n",
        "models_1b = [\n",
        "    config for config in EXPERIMENT_CONFIG\n",
        "    if \"1B-I\" in config[\"base_model\"] and \"3B\" not in config[\"base_model\"]\n",
        "]\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"üìä EVALUATION PLAN: Llama-3.2-1B-Instruct Family\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "print(f\"Total models to evaluate: {len(models_1b) + 1}\")  # +1 for base model\n",
        "print(f\"Benchmarks per model: {len(BENCHMARKS_INSTRUCT)}\")\n",
        "print(f\"Total evaluations: {(len(models_1b) + 1) * len(BENCHMARKS_INSTRUCT)}\")\n",
        "print(f\"Estimated runtime: ~4-5 hours\\n\")\n",
        "\n",
        "# Display models table\n",
        "print(\"Models to evaluate:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Model':<30} {'Pruning':<10} {'Star':<6}\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Llama-3.2-1B-Instruct (baseline)':<30} {'0%':<10} {'300%':<12} {'N/A':<6}\")\n",
        "for config in models_1b:\n",
        "    model_name = config['hf_repo_id'].split('/')[-1]\n",
        "    pruning = f\"{config['pruning_pct']}%\"\n",
        "    star = \"‚≠ê Yes\" if config['is_star'] else \"No\"\n",
        "    print(f\"{model_name:<30} {pruning:<10} {star:<6}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Display benchmarks\n",
        "print(\"\\nBenchmarks to run:\")\n",
        "print(\"-\" * 70)\n",
        "for i, task in enumerate(BENCHMARKS_INSTRUCT, 1):\n",
        "    task_name = task['name']\n",
        "    fewshot = f\"{task['num_fewshot']}-shot\"\n",
        "    print(f\"{i:2d}. {task_name:<25} {fewshot}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "print(\"\\n‚öôÔ∏è  Configuration:\")\n",
        "print(f\"   - Neuron selection method: MAW (Maximum Absolute Weight)\")\n",
        "print(f\"   - Checkpointing: Enabled (per-task granularity)\")\n",
        "print(f\"   - Model creation: On-the-fly pruning (no pre-creation needed)\")\n",
        "print(f\"   - Error handling: Skip failed tasks and continue\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7U0stRUIvXG",
        "outputId": "b8720398-f3a0-4988-f165-b403c8c7b22f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Checkpoint directory: /content/drive/MyDrive/glu_pruning/checkpoints/1bI\n",
            "‚úÖ Results directory: /content/drive/MyDrive/glu_pruning/results\n",
            "\n",
            "Checkpoint files:\n",
            "   baseline  : ‚úÖ Exists\n",
            "   40pct     : ‚úÖ Exists\n",
            "   60pct     : ‚úÖ Exists\n"
          ]
        }
      ],
      "source": [
        "# Setup checkpoint paths\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/glu_pruning/checkpoints/1bI\"\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/glu_pruning/results\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "Path(CHECKPOINT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Checkpoint directory: {CHECKPOINT_DIR}\")\n",
        "print(f\"‚úÖ Results directory: {RESULTS_DIR}\")\n",
        "\n",
        "# Define checkpoint paths for each model\n",
        "checkpoint_paths = {\n",
        "    \"baseline\": f\"{CHECKPOINT_DIR}/llama_3.2_1b_I_baseline.json\",\n",
        "    \"40pct\": f\"{CHECKPOINT_DIR}/llama_3.2_1b_I_pruned_40pct.json\",\n",
        "    \"60pct\": f\"{CHECKPOINT_DIR}/llama_3.2_1b_I_pruned_60pct.json\",\n",
        "}\n",
        "\n",
        "print(\"\\nCheckpoint files:\")\n",
        "for key, path in checkpoint_paths.items():\n",
        "    exists = \"‚úÖ Exists\" if Path(path).exists() else \"üÜï New\"\n",
        "    print(f\"   {key:<10}: {exists}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5yTl5gTIvXH"
      },
      "source": [
        "# 3. Baseline Evaluation\n",
        "\n",
        "Evaluate the original Llama-3.2-1B model to establish performance baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjcHGNrsIvXH",
        "outputId": "4bc9b8cf-66db-4711-ef2c-69b00198b607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "93f8209749b340a3a19de9afd8115cc8",
            "7dada4dab53b4769890141d06a65613e",
            "fff1a9ee634148348cf24395046b2879",
            "15239a9b2c744fd4a65c3c5b64047311",
            "7d6076a7bb5345459c06b644d4f991e1",
            "5e368237697d4f478122891335f2e4a0",
            "986ed0bd03c5462d9234300caa0ffad4",
            "69f9c2b3560646478fa85829768d9d23",
            "a6d2ca964c7749ca99d5f038802cdb44",
            "8bad59835e2b41a2be9a09f3d142ae48",
            "9c9337664aa44ec699fffa6117d65b69",
            "98b3ae78de164ae794fcada2547928e7",
            "1a40ad56028847d6bbfb60eb7a97cd80",
            "3fdca068796a4419bff7612d53e9e871",
            "ecf1e80d063744028c348c0422c39a07",
            "0505592b9c9441f8b5d31f4251cce914",
            "826242079df34663a6ff06f55cf4fd3d",
            "a547d433d6fa41ec820e27cda2b1465c",
            "b2a589ca98784b42b39e244463b367f2",
            "5fac6575b7384d899f89cc3a78bf52c3",
            "7cb344c08b27474f9b1876884ac6e593",
            "961c998cd92c4c4eab4c9d40c546a388"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "üìä PHASE 1: BASELINE EVALUATION\n",
            "======================================================================\n",
            "\n",
            "Loading base model: meta-llama/Llama-3.2-1B-Instruct...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93f8209749b340a3a19de9afd8115cc8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98b3ae78de164ae794fcada2547928e7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"üìä PHASE 1: BASELINE EVALUATION\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "BASE_MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "# Load base model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "print(f\"Loading base model: {BASE_MODEL_ID}...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    #dtype=torch.float16, #T4\n",
        "    dtype=torch.bfloat16, #A100 <- Used for experiments\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"‚úÖ Model loaded successfully\")\n",
        "\n",
        "# Display model statistics\n",
        "base_stats = get_model_stats(base_model)\n",
        "print(f\"\\nüìà Model Statistics:\")\n",
        "print(f\"   Parameters: {base_stats['total_parameters']:,}\")\n",
        "print(f\"   Size: {base_stats['size_gb']:.2f} GB\")\n",
        "\n",
        "# Run evaluation with checkpointing\n",
        "baseline_results = run_robust_evaluation(\n",
        "    model=base_model,\n",
        "    tokenizer=tokenizer,\n",
        "    tasks=BENCHMARKS_INSTRUCT,\n",
        "    checkpoint_path=checkpoint_paths[\"baseline\"],\n",
        "    model_name=\"Llama-3.2-1B-I-baseline\"\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"‚úÖ BASELINE EVALUATION COMPLETED\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# Display results summary\n",
        "print(\"Results Preview:\")\n",
        "print(format_results_table(baseline_results))\n",
        "\n",
        "# Clear memory\n",
        "del base_model\n",
        "clear_gpu_cache()\n",
        "\n",
        "print(\"\\nüßπ Memory cleared, ready for pruned models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtbaAsxIIvXH"
      },
      "source": [
        "# 4. Pruned Models Evaluation Loop\n",
        "\n",
        "Evaluate the three pruned variants (20%, 40%, 60%) using on-the-fly pruning with OptiPFair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPoZiAfjIvXH"
      },
      "outputs": [],
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"üìä PHASE 2: PRUNED MODELS EVALUATION\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# Store all results for final comparison\n",
        "all_results = {\n",
        "    \"baseline\": baseline_results\n",
        "}\n",
        "\n",
        "# Evaluate each pruned model\n",
        "for i, config in enumerate(models_1b, 1):\n",
        "    model_name = config['hf_repo_id'].split('/')[-1]\n",
        "    pruning_pct = config['pruning_pct']\n",
        "    is_star = config['is_star']\n",
        "\n",
        "    print(f\"\\n{'‚îÄ'*70}\")\n",
        "    print(f\"üîÑ EVALUATING MODEL {i}/{len(models_1b)}: {model_name}{pruning_pct} \")\n",
        "    print(f\"   Pruning: {pruning_pct}% |  Star: {'‚≠ê' if is_star else 'No'}\")\n",
        "    print(f\"{'‚îÄ'*70}\\n\")\n",
        "\n",
        "    try:\n",
        "        # Load or create model using utility function\n",
        "        model, tokenizer, stats = load_or_create_model(config, device=\"auto\")\n",
        "\n",
        "        # Display model statistics\n",
        "        print(f\"\\nüìà Model Statistics:\")\n",
        "        print(f\"   Parameters: {stats['total_parameters']:,}\")\n",
        "        print(f\"   Size: {stats['size_gb']:.2f} GB\")\n",
        "        if 'pruning_stats' in stats:\n",
        "            print(f\"   Reduction: {stats['pruning_stats']['percentage_reduction']:.2f}%\")\n",
        "        print(f\"   Source: {stats['source']}\\n\")\n",
        "\n",
        "        # Determine checkpoint key\n",
        "        checkpoint_key = f\"{pruning_pct}pct\"\n",
        "\n",
        "        # Run evaluation with checkpointing\n",
        "        results = run_robust_evaluation(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            tasks=BENCHMARKS_INSTRUCT,\n",
        "            checkpoint_path=checkpoint_paths[checkpoint_key],\n",
        "            model_name=model_name\n",
        "        )\n",
        "\n",
        "        # Store results\n",
        "        all_results[checkpoint_key] = results\n",
        "\n",
        "        print(f\"\\n‚úÖ {model_name}{pruning_pct} evaluation completed\")\n",
        "        print(\"\\nResults Preview:\")\n",
        "        print(format_results_table(results))\n",
        "\n",
        "        # Clear memory before next model\n",
        "        del model\n",
        "        clear_gpu_cache()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå ERROR evaluating {model_name}: {str(e)}\")\n",
        "        print(\"   Continuing with next model...\\n\")\n",
        "        clear_gpu_cache()\n",
        "        continue\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"‚úÖ ALL PRUNED MODELS EVALUATED\")\n",
        "print(f\"{'='*70}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1MptG-dIvXH"
      },
      "source": [
        "# 5. Results Consolidation & Export\n",
        "\n",
        "Consolidate all evaluation results and export to CSV for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5RqknL6IvXH"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for this cell\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "import re\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"üìä CONSOLIDATING RESULTS (DYNAMIC FILE-BASED)\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# --- Directory Setup ---\n",
        "# Ensure CHECKPOINT_DIR is defined (it should be defined in a previous cell)\n",
        "# This is where the individual JSON results are.\n",
        "# Example: CHECKPOINT_DIR = \"/content/drive/MyDrive/glu_pruning/checkpoints/1b\"\n",
        "if 'CHECKPOINT_DIR' not in globals():\n",
        "    print(\"‚ö†Ô∏è Warning: CHECKPOINT_DIR not set. Using default './checkpoints/1bI'\")\n",
        "    CHECKPOINT_DIR = \"./checkpoints/1bI\"\n",
        "\n",
        "# Ensure RESULTS_DIR is defined (it should be defined in a previous cell)\n",
        "# This is where the consolidated CSV will be saved.\n",
        "# Example: RESULTS_DIR = \"/content/drive/MyDrive/glu_pruning/results\"\n",
        "if 'RESULTS_DIR' not in globals():\n",
        "    print(\"‚ö†Ô∏è Warning: RESULTS_DIR not set. Using default './results'\")\n",
        "    RESULTS_DIR = \"./results\"\n",
        "# --- End Directory Setup ---\n",
        "\n",
        "\n",
        "# Prepare data for DataFrame\n",
        "consolidated_data = []\n",
        "\n",
        "# --- Dynamic Loading ---\n",
        "# 1. Find all individual 1B model result files\n",
        "# *** THIS IS THE CORRECTED LINE: Using CHECKPOINT_DIR ***\n",
        "json_files = glob.glob(f\"{CHECKPOINT_DIR}/llama_3.2_1b_I_*.json\")\n",
        "\n",
        "# 2. Exclude any aggregate/summary files\n",
        "json_files = [\n",
        "    f for f in json_files\n",
        "    if \"results\" not in os.path.basename(f) and \"complete\" not in os.path.basename(f)\n",
        "]\n",
        "\n",
        "print(f\"Searching for results in: {CHECKPOINT_DIR}\")\n",
        "print(f\"Found {len(json_files)} individual result files to process:\")\n",
        "\n",
        "# 3. Process each model's result file\n",
        "for json_path in sorted(json_files):\n",
        "    print(f\"  -> Processing: {os.path.basename(json_path)}\")\n",
        "    try:\n",
        "        with open(json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ö†Ô∏è Warning: Could not read or parse file. Error: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Extract metadata and results\n",
        "    metadata = data.get(\"metadata\", {})\n",
        "    model_name_from_file = metadata.get(\"model_name\", \"Unknown Model\")\n",
        "\n",
        "    results = data.get(\"results\", {})\n",
        "    if not results:\n",
        "        print(f\"    ‚ö†Ô∏è Warning: No 'results' found in file. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # --- Dynamically derive info from metadata ---\n",
        "    pruning_pct = 0\n",
        "    is_star = False\n",
        "\n",
        "    # Parse model name to get pruning percentage and display name\n",
        "    if \"baseline\" in model_name_from_file:\n",
        "        display_name = \"Llama-3.2-1B-Instruct\"\n",
        "        pruning_pct = 0\n",
        "    else:\n",
        "        # Use regex to find pruning percentage\n",
        "        match = re.search(r'pruned-(\\d+)pct', model_name_from_file)\n",
        "        if match:\n",
        "            pruning_pct = int(match.group(1))\n",
        "            display_name = f\"Llama-3.2-1B-I-pruned-{pruning_pct}%\"\n",
        "        else:\n",
        "            display_name = model_name_from_file # Fallback\n",
        "\n",
        "    # Star logic (as per original project spec/hardcoded cell)\n",
        "    # 40% is the \"star\" model for the 1B variant\n",
        "    if pruning_pct == 40:\n",
        "        is_star = True\n",
        "    # --- End dynamic info derivation ---\n",
        "\n",
        "    # Process each task for this model\n",
        "    for task_name, metrics in results.items():\n",
        "        row = {\n",
        "            \"model\": display_name,\n",
        "            \"pruning_pct\": pruning_pct,\n",
        "            \"is_star\": is_star,\n",
        "            \"task\": task_name,\n",
        "        }\n",
        "\n",
        "        # Add all metrics from this task\n",
        "        for metric_name, value in metrics.items():\n",
        "            # Convert string values to float where possible\n",
        "            try:\n",
        "                row[metric_name] = float(value)\n",
        "            except (ValueError, TypeError):\n",
        "                row[metric_name] = value\n",
        "\n",
        "        consolidated_data.append(row)\n",
        "\n",
        "# --- End Dynamic Loading ---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame\n",
        "df = pd.DataFrame(consolidated_data)\n",
        "\n",
        "# Sort by pruning_pct and then task to ensure consistent order\n",
        "if not df.empty:\n",
        "    df = df.sort_values(by=[\"pruning_pct\", \"task\"]).reset_index(drop=True)\n",
        "\n",
        "print(f\"\\n‚úÖ Consolidated {len(df)} result rows\")\n",
        "print(f\"   Models: {df['model'].nunique()}\")\n",
        "print(f\"   Tasks: {df['task'].nunique()}\")\n",
        "if 'model' in df.columns:\n",
        "    print(f\"   Metrics per task: {len(df.columns) - 4}\")  # Exclude metadata columns\n",
        "else:\n",
        "    print(\"   No data consolidated.\")\n",
        "\n",
        "# Display summary\n",
        "print(\"\\nDataFrame Preview:\")\n",
        "print(df.head(10))\n",
        "\n",
        "# Save to CSV\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "csv_path = f\"{RESULTS_DIR}/llama_1b_I_results_{timestamp}.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\"\\nüíæ Results saved to: {csv_path}\")\n",
        "\n",
        "# Also save a \"latest\" version for easy access\n",
        "latest_path = f\"{RESULTS_DIR}/llama_1b_I_results_latest.csv\"\n",
        "df.to_csv(latest_path, index=False)\n",
        "print(f\"üíæ Latest results: {latest_path}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"‚úÖ EVALUATION COMPLETE - ALL RESULTS SAVED\")\n",
        "print(f\"{'='*70}\\n\")"
      ],
      "metadata": {
        "id": "TBPO5o8oteVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxHStHvMIvXH"
      },
      "source": [
        "# 6. Quick Analysis & Visualization\n",
        "\n",
        "Generate quick insights to decide which models merit uploading to HuggingFace Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xo_bifVCIvXI"
      },
      "outputs": [],
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"üìà QUICK ANALYSIS: Performance vs. Pruning Level\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# This cell assumes the 'df' DataFrame was created in the previous cell.\n",
        "\n",
        "# Calculate average performance degradation per model\n",
        "# Focus on key metrics: accuracy for classification, perplexity for generation\n",
        "\n",
        "summary_metrics = []\n",
        "\n",
        "# --- Dynamic Analysis ---\n",
        "# Group by the model-level info we created in the previous cell\n",
        "# This replaces the hardcoded 'model_info' dictionary\n",
        "try:\n",
        "    grouped = df.groupby(['model', 'pruning_pct', 'is_star'])\n",
        "except KeyError:\n",
        "    print(\"‚ùå Error: 'df' DataFrame not found or is missing required columns.\")\n",
        "    print(\"   Please ensure the previous consolidation cell was run successfully.\")\n",
        "    # Create an empty df to avoid crashing the rest of the cell\n",
        "    grouped = pd.DataFrame().groupby(['model', 'pruning_pct', 'is_star'])\n",
        "\n",
        "print(f\"Analyzing {len(grouped)} unique models found in the DataFrame...\")\n",
        "\n",
        "for (model_name, pruning, is_star_bool), model_df in grouped:\n",
        "    # 'model_df' is now the DataFrame for this specific model\n",
        "\n",
        "    # Extract key metrics\n",
        "    # We look for 'accuracy' (from boolq, mmlu, etc.)\n",
        "    accuracies = model_df['accuracy'].dropna()\n",
        "    # We look for 'perplexity' (from lambada_openai)\n",
        "    # Note: wikitext 'word_perplexity,none' is not used here for simplicity\n",
        "    perplexities = model_df['perplexity'].dropna()\n",
        "\n",
        "    summary = {\n",
        "        \"model\": model_name,\n",
        "        \"pruning\": pruning,\n",
        "        \"star\": \"‚≠ê\" if is_star_bool else \"\",\n",
        "        \"avg_accuracy\": accuracies.mean() if len(accuracies) > 0 else None,\n",
        "        \"avg_perplexity\": perplexities.mean() if len(perplexities) > 0 else None,\n",
        "        \"num_tasks\": len(model_df),\n",
        "    }\n",
        "\n",
        "    summary_metrics.append(summary)\n",
        "\n",
        "# --- End Dynamic Analysis ---\n",
        "\n",
        "if not summary_metrics:\n",
        "    print(\"\\nNo summary metrics to display. Skipping analysis.\")\n",
        "else:\n",
        "    summary_df = pd.DataFrame(summary_metrics)\n",
        "    # Sort by pruning percentage to ensure a logical order\n",
        "    summary_df = summary_df.sort_values(by=\"pruning\").reset_index(drop=True)\n",
        "\n",
        "    print(\"\\nPerformance Summary:\")\n",
        "    print(\"-\" * 90)\n",
        "    print(summary_df.to_string(index=False, float_format=\"%.4f\"))\n",
        "    print(\"-\" * 90)\n",
        "\n",
        "    # Calculate degradation vs baseline\n",
        "    # Find baseline row (pruning == 0)\n",
        "    baseline_row = summary_df.loc[summary_df['pruning'] == 0]\n",
        "\n",
        "    if baseline_row.empty:\n",
        "        print(\"\\n‚ö†Ô∏è Baseline model (pruning=0) not found. Cannot calculate degradation.\")\n",
        "    else:\n",
        "        baseline_acc = baseline_row['avg_accuracy'].values[0]\n",
        "        baseline_ppl = baseline_row['avg_perplexity'].values[0]\n",
        "\n",
        "        print(f\"\\nDegradation vs. Baseline (Acc: {baseline_acc:.4f}, PPL: {baseline_ppl:.2f}):\")\n",
        "        print(\"-\" * 90)\n",
        "\n",
        "        for _, row in summary_df.iterrows():\n",
        "            if row['pruning'] == 0:\n",
        "                continue\n",
        "\n",
        "            acc_delta_str = \"N/A\"\n",
        "            if row['avg_accuracy'] is not None and baseline_acc is not None and baseline_acc != 0:\n",
        "                acc_delta = ((row['avg_accuracy'] - baseline_acc) / baseline_acc * 100)\n",
        "                acc_delta_str = f\"{acc_delta:+.2f}%\"\n",
        "\n",
        "            ppl_delta_str = \"N/A\"\n",
        "            if row['avg_perplexity'] is not None and baseline_ppl is not None and baseline_ppl != 0:\n",
        "                ppl_delta = ((row['avg_perplexity'] - baseline_ppl) / baseline_ppl * 100)\n",
        "                ppl_delta_str = f\"{ppl_delta:+.2f}%\"\n",
        "\n",
        "            print(f\"{row['model']:<35} {row['star']:>2}\")\n",
        "            print(f\"   Accuracy:   {acc_delta_str}\")\n",
        "            print(f\"   Perplexity: {ppl_delta_str}\")\n",
        "            print()\n",
        "\n",
        "        print(\"-\" * 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcveKgykIvXI"
      },
      "outputs": [],
      "source": [
        "# Visualization: Performance across pruning levels\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy plot\n",
        "axes[0].plot(summary_df['pruning'], summary_df['avg_accuracy'], marker='o', linewidth=2, markersize=8)\n",
        "axes[0].axhline(y=baseline_acc, color='r', linestyle='--', alpha=0.5, label='Baseline')\n",
        "axes[0].set_xlabel('Pruning Level (%)', fontsize=12)\n",
        "axes[0].set_ylabel('Average Accuracy', fontsize=12)\n",
        "axes[0].set_title('Accuracy vs. Pruning Level', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].legend()\n",
        "\n",
        "# Highlight star model\n",
        "star_idx = summary_df[summary_df['star'] == '‚≠ê'].index[0]\n",
        "axes[0].plot(summary_df.loc[star_idx, 'pruning'], summary_df.loc[star_idx, 'avg_accuracy'],\n",
        "             marker='*', markersize=20, color='gold', markeredgecolor='black', markeredgewidth=1.5)\n",
        "\n",
        "# Perplexity plot\n",
        "axes[1].plot(summary_df['pruning'], summary_df['avg_perplexity'], marker='o', linewidth=2, markersize=8, color='orange')\n",
        "axes[1].axhline(y=baseline_ppl, color='r', linestyle='--', alpha=0.5, label='Baseline')\n",
        "axes[1].set_xlabel('Pruning Level (%)', fontsize=12)\n",
        "axes[1].set_ylabel('Average Perplexity', fontsize=12)\n",
        "axes[1].set_title('Perplexity vs. Pruning Level', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].legend()\n",
        "\n",
        "# Highlight star model\n",
        "axes[1].plot(summary_df.loc[star_idx, 'pruning'], summary_df.loc[star_idx, 'avg_perplexity'],\n",
        "             marker='*', markersize=20, color='gold', markeredgecolor='black', markeredgewidth=1.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{RESULTS_DIR}/llama_1b_I_performance_analysis.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìä Visualization saved to: {RESULTS_DIR}/llama_1b_I_performance_analysis.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4H7r1PNIvXI"
      },
      "source": [
        "# 7. Decision Matrix: Which Models to Upload?\n",
        "\n",
        "Based on the evaluation results, determine which models should be uploaded to HuggingFace Hub for Phase 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5sHLEjwIvXI"
      },
      "outputs": [],
      "source": [
        "import numpy as np # Need numpy for nan checks\n",
        "import pandas as pd # Ensure pandas is imported\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"üéØ DECISION MATRIX: Models for HuggingFace Hub Upload\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "print(\"Evaluation Criteria:\")\n",
        "print(\"  1. Performance degradation (avg_accuracy) < 15% vs baseline\")\n",
        "print(\"  2. Outperforms or matches baseline in at least 3 tasks (Primary Metric)\")\n",
        "print(\"  3. Accuracy degradation (avg_accuracy) < 50% vs baseline\")\n",
        "print(\"  4. Sufficient parameter reduction to justify storage\\n\")\n",
        "\n",
        "# --- Setup: Load BENCHMARKS_BASE and Baseline Scores (as before) ---\n",
        "print(\"Building primary metric map from BENCHMARKS_INSTRUCT...\")\n",
        "TASK_PRIMARY_METRICS = {}\n",
        "if 'BENCHMARKS_INSTRUCT' not in globals():\n",
        "    print(\"=\"*70)\n",
        "    print(\"‚ùå Error: BENCHMARKS_INSTRUCT variable not found in global scope.\")\n",
        "    print(\"   Please ensure you have run the setup cell that imports from utils.py:\")\n",
        "    print(\"   >>> from utils import BENCHMARKS_INSTRUCT\")\n",
        "    print(\"=\"*70)\n",
        "    raise NameError(\"BENCHMARKS_INSTRUCT is not defined.\")\n",
        "else:\n",
        "    print(f\"‚úÖ Found BENCHMARKS_INSTRUCT with {len(BENCHMARKS_INSTRUCT)} tasks.\")\n",
        "\n",
        "for task_spec in BENCHMARKS_INSTRUCT:\n",
        "    task_name = task_spec[\"name\"]\n",
        "    if task_name == 'wikitext':\n",
        "        TASK_PRIMARY_METRICS[task_name] = ('word_perplexity,none', False) # Lower is better\n",
        "    elif task_name == 'lambada_openai':\n",
        "        TASK_PRIMARY_METRICS[task_name] = ('perplexity', False) # Lower is better\n",
        "    elif task_name == 'gsm8k':\n",
        "        TASK_PRIMARY_METRICS[task_name] = ('exact_match,strict-match', True) # Higher is better\n",
        "    else:\n",
        "        TASK_PRIMARY_METRICS[task_name] = ('accuracy', True) # Higher is better\n",
        "print(f\"Built primary metric map for {len(TASK_PRIMARY_METRICS)} tasks.\")\n",
        "\n",
        "try:\n",
        "    df_baseline = df[df['pruning_pct'] == 0].set_index('task')\n",
        "    baseline_scores = {}\n",
        "    for task, (metric, _) in TASK_PRIMARY_METRICS.items():\n",
        "        if task in df_baseline.index and metric in df_baseline.columns:\n",
        "            score = df_baseline.loc[task, metric]\n",
        "            if not pd.isna(score):\n",
        "                baseline_scores[task] = score\n",
        "    print(f\"Captured {len(baseline_scores)} valid scores from baseline model.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: Could not get baseline scores from 'df'. {e}\")\n",
        "    baseline_scores = {}\n",
        "# --- End Setup ---\n",
        "\n",
        "\n",
        "# Decision logic\n",
        "decisions = []\n",
        "if 'summary_df' not in globals():\n",
        "     print(\"‚ùå Error: 'summary_df' not found. Please run the previous analysis cell.\")\n",
        "else:\n",
        "    for _, row in summary_df.iterrows():\n",
        "        if row['pruning'] == 0:\n",
        "            continue  # Skip baseline\n",
        "\n",
        "        decision = {\n",
        "            \"model\": row['model'],\n",
        "            \"pruning\": row['pruning'],\n",
        "            \"star\": row['star'],\n",
        "        }\n",
        "\n",
        "        acc_degradation = abs((row['avg_accuracy'] - baseline_acc) / baseline_acc * 100) if baseline_acc and not pd.isna(row['avg_accuracy']) else 999\n",
        "        ppl_degradation = abs((row['avg_perplexity'] - baseline_ppl) / baseline_ppl * 100) if baseline_ppl and not pd.isna(row['avg_perplexity']) else 999\n",
        "\n",
        "        # --- MODIFIED: Check Criterion 2 AND store task names ---\n",
        "        outperform_count = 0\n",
        "        outperforming_tasks = []  # <-- NEW: List to store names\n",
        "        model_tasks_df = df[df['model'] == row['model']].set_index('task')\n",
        "\n",
        "        for task, (metric, higher_is_better) in TASK_PRIMARY_METRICS.items():\n",
        "            if task not in baseline_scores or task not in model_tasks_df.index:\n",
        "                continue\n",
        "\n",
        "            model_score = model_tasks_df.loc[task, metric]\n",
        "            if pd.isna(model_score):\n",
        "                continue\n",
        "\n",
        "            baseline_score = baseline_scores[task]\n",
        "\n",
        "            if higher_is_better:\n",
        "                if model_score >= baseline_score:\n",
        "                    outperform_count += 1\n",
        "                    outperforming_tasks.append(task) # <-- NEW: Store task name\n",
        "            else: # Lower is better\n",
        "                if model_score <= baseline_score:\n",
        "                    outperform_count += 1\n",
        "                    outperforming_tasks.append(task) # <-- NEW: Store task name\n",
        "\n",
        "        decision['outperform_count'] = outperform_count\n",
        "        decision['outperforming_tasks'] = outperforming_tasks # <-- NEW: Store list\n",
        "        tasks_str = \", \".join(outperforming_tasks) if outperforming_tasks else \"None\"\n",
        "        # --- End Criterion 2 Check ---\n",
        "\n",
        "\n",
        "        # --- MODIFIED: Updated Decision Logic with task list ---\n",
        "        is_star = row['star'] == '‚≠ê'\n",
        "        crit_1_low_degrad = (acc_degradation < 15)\n",
        "        crit_2_tasks = (outperform_count >= 3)\n",
        "        crit_3_not_catastrophic = (acc_degradation < 50)\n",
        "\n",
        "        if is_star:\n",
        "            decision['upload'] = True\n",
        "            decision['reason'] = f\"Star model. Won/Tied: {tasks_str}\"\n",
        "        elif crit_1_low_degrad and crit_2_tasks:\n",
        "            decision['upload'] = True\n",
        "            decision['reason'] = f\"Low degradation (acc: {acc_degradation:.1f}%) AND Won/Tied: {tasks_str}\"\n",
        "        elif crit_3_not_catastrophic and crit_2_tasks:\n",
        "            decision['upload'] = True\n",
        "            decision['reason'] = f\"Acceptable degradation (acc: {acc_degradation:.1f}%) AND Won/Tied: {tasks_str}\"\n",
        "        else:\n",
        "            decision['upload'] = False\n",
        "            reason_parts = []\n",
        "            if not crit_3_not_catastrophic:\n",
        "                reason_parts.append(f\"High acc degradation ({acc_degradation:.1f}%)\")\n",
        "            if not crit_2_tasks:\n",
        "                reason_parts.append(f\"Only {outperform_count} tasks won ({tasks_str})\")\n",
        "\n",
        "            if not reason_parts:\n",
        "                 reason_parts.append(f\"Degradation (acc: {acc_degradation:.1f}%) or task count ({outperform_count}) too low\")\n",
        "\n",
        "            decision['reason'] = \" AND \".join(reason_parts)\n",
        "\n",
        "        decisions.append(decision)\n",
        "\n",
        "# Display decision table\n",
        "print(\"\\nUpload Decisions (Updated Logic with Task Details):\")\n",
        "print(\"-\" * 140) # <-- Widen table\n",
        "# <-- MODIFIED: Widen Reason column\n",
        "print(f\"{'Model':<35} {'Pruning':<10} {'Star':<6} {'Tasks Won/Tied':<16} {'Upload?':<10} {'Reason'}\")\n",
        "print(\"-\" * 140) # <-- Widen table\n",
        "\n",
        "for dec in decisions:\n",
        "    upload_status = \"‚úÖ YES\" if dec['upload'] else \"‚ùå NO\"\n",
        "    print(f\"{dec['model']:<35} {dec['pruning']:<10}% {dec['star']:<6} {dec.get('outperform_count', 'N/A'):<16} {upload_status:<10} {dec['reason']}\")\n",
        "\n",
        "print(\"-\" * 140) # <-- Widen table\n",
        "\n",
        "# Summary\n",
        "models_to_upload = sum(1 for d in decisions if d['upload'])\n",
        "print(f\"\\nüì¶ Total models to upload to HF Hub: {models_to_upload}/{len(decisions)}\")\n",
        "print(f\"\\n‚úÖ PHASE 3 COMPLETE - Ready for Phase 2 (Model Factory)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(\"üíæ SAVING COMPLETE RESULTS FOR RESEARCH SHARING\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# --- Dynamic Model Info Setup ---\n",
        "# Build a lookup for expansion rates from EXPERIMENT_CONFIG (if it exists)\n",
        "exp_rate_map = {}\n",
        "if 'EXPERIMENT_CONFIG' in globals():\n",
        "    try:\n",
        "        # --- ROBUST PARSING ---\n",
        "        # Only add if all keys are present\n",
        "        for cfg in EXPERIMENT_CONFIG:\n",
        "            if (cfg.get(\"base_model\", \"\") == \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "                and 'pruning_pct' in cfg\n",
        "                and 'expansion_rate' in cfg):\n",
        "                 exp_rate_map[cfg['pruning_pct']] = cfg['expansion_rate']\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Warning: Could not parse EXPERIMENT_CONFIG. {e}\")\n",
        "exp_rate_map[0] = 300 # Baseline expansion rate\n",
        "\n",
        "print(f\"Loaded {len(exp_rate_map)} expansion rates from config (Baseline included).\")\n",
        "\n",
        "# Build 'models_evaluated' section dynamically from the 'df'\n",
        "models_evaluated = {}\n",
        "if 'df' not in globals():\n",
        "    print(\"‚ùå Error: 'df' DataFrame not found. Cannot build results.\")\n",
        "    raise NameError(\"'df' is not defined. Please run consolidation cell.\")\n",
        "\n",
        "# Group by model to rebuild the nested result structure\n",
        "grouped_by_model = df.groupby(['model', 'pruning_pct', 'is_star'])\n",
        "\n",
        "for (model_name, pruning_pct, is_star_bool), model_df in grouped_by_model:\n",
        "    model_key = f\"pruned_{int(pruning_pct)}pct\" if pruning_pct > 0 else \"baseline\"\n",
        "\n",
        "    # Rebuild the nested results dict\n",
        "    model_results_dict = {}\n",
        "    for _, row in model_df.iterrows():\n",
        "        task_name = row['task']\n",
        "        metrics = row.drop(['model', 'pruning_pct', 'is_star', 'task'])\n",
        "\n",
        "        task_metrics = {}\n",
        "        for col, val in metrics.dropna().items():\n",
        "            # Convert numpy types to native Python types\n",
        "            if isinstance(val, (np.integer, np.int64)):\n",
        "                task_metrics[col] = int(val)\n",
        "            elif isinstance(val, (np.floating, np.float64)):\n",
        "                task_metrics[col] = float(val)\n",
        "            else:\n",
        "                task_metrics[col] = val\n",
        "        model_results_dict[task_name] = task_metrics\n",
        "\n",
        "    # Assemble the entry for this model\n",
        "    models_evaluated[model_key] = {\n",
        "        \"name\": model_name,\n",
        "        \"pruning_pct\": int(pruning_pct), # Cast to native int\n",
        "        \"expansion_rate\": exp_rate_map.get(int(pruning_pct), None), # Get from map\n",
        "        \"is_star\": bool(is_star_bool), # <--- *** CRITICAL FIX: Cast to native bool ***\n",
        "        \"hf_repo\": (\n",
        "            \"meta-llama/Llama-3.2-1B-Instruct\" if pruning_pct == 0\n",
        "            else f\"peremartra/Llama-3.2-1B-I-pruned-{int(pruning_pct)}pct\"\n",
        "        ),\n",
        "        \"results\": model_results_dict\n",
        "    }\n",
        "print(f\"Dynamically built {len(models_evaluated)} model entries from 'df'.\")\n",
        "# --- End Dynamic Model Setup ---\n",
        "\n",
        "\n",
        "# Consolidate all data into a comprehensive JSON\n",
        "complete_results = {\n",
        "    \"experiment_metadata\": {\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"notebook\": \"03_Evaluate_1B_Instruct.ipynb\",\n",
        "        \"model_family\": \"Llama-3.2-1B-Instruct\",\n",
        "        \"pruning_method\": \"MAW (Maximum Absolute Weight)\",\n",
        "        #\"library_versions\": library_versions,\n",
        "        \"hardware\": {\n",
        "            \"device\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\",\n",
        "            \"gpu_memory_gb\": torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else None\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"benchmarks\": [\n",
        "        {\"name\": task[\"name\"], \"num_fewshot\": task[\"num_fewshot\"]}\n",
        "        for task in BENCHMARKS_INSTRUCT\n",
        "    ],\n",
        "\n",
        "    \"models_evaluated\": models_evaluated, # Use dynamic model data\n",
        "\n",
        "    \"summary_statistics\": {\n",
        "        \"baseline\": {\n",
        "            \"avg_accuracy\": float(summary_df.loc[summary_df['pruning'] == 0, 'avg_accuracy'].values[0]),\n",
        "            \"avg_perplexity\": float(summary_df.loc[summary_df['pruning'] == 0, 'avg_perplexity'].values[0]),\n",
        "        },\n",
        "        \"pruned_models\": [\n",
        "            {\n",
        "                \"pruning_pct\": int(row['pruning']),\n",
        "                \"is_star\": row['star'] == '‚≠ê', # This is native bool, so it's fine\n",
        "                \"avg_accuracy\": float(row['avg_accuracy']) if pd.notna(row['avg_accuracy']) else None,\n",
        "                \"avg_perplexity\": float(row['avg_perplexity']) if pd.notna(row['avg_perplexity']) else None,\n",
        "                \"accuracy_degradation_pct\": float(((row['avg_accuracy'] - baseline_acc) / baseline_acc * 100)) if baseline_acc and pd.notna(row['avg_accuracy']) else None,\n",
        "                \"perplexity_degradation_pct\": float(((row['avg_perplexity'] - baseline_ppl) / baseline_ppl * 100)) if baseline_ppl and pd.notna(row['avg_perplexity']) else None\n",
        "            }\n",
        "            for _, row in summary_df.iterrows() if row['pruning'] > 0\n",
        "        ]\n",
        "    },\n",
        "\n",
        "    \"upload_decisions\": decisions, # Use dynamic decisions\n",
        "\n",
        "    \"citation\": {\n",
        "        \"paper\": \"Exploring GLU Expansion Ratios: Structured Pruning in Llama-3.2 Models\",\n",
        "        \"author\": \"Pere Martra\",\n",
        "        \"doi\": \"https://doi.org/10.31219/osf.io/qgxea\",\n",
        "        \"github\": \"https://github.com/peremartra/llama-glu-expansion-pruning\",\n",
        "        \"note\": \"Results are freely available for research purposes. Please cite the paper if you use this data.\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- Save to JSON ---\n",
        "try:\n",
        "    # Ensure RESULTS_DIR is defined\n",
        "    if 'RESULTS_DIR' not in globals():\n",
        "        print(\"‚ùå Error: RESULTS_DIR not defined. Defaulting to './results'\")\n",
        "        RESULTS_DIR = \"./results\"\n",
        "\n",
        "    # Create RESULTS_DIR if it doesn't exist\n",
        "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    json_path = f\"{RESULTS_DIR}/llama_1b_I_complete_results_{timestamp}.json\"\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(complete_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"‚úÖ Complete results saved to:\")\n",
        "    print(f\"   {json_path}\")\n",
        "\n",
        "    # Also save a \"latest\" version\n",
        "    latest_json = f\"{RESULTS_DIR}/llama_1b_I_complete_results_latest.json\"\n",
        "    with open(latest_json, 'w') as f:\n",
        "        json.dump(complete_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"‚úÖ Latest version:\")\n",
        "    print(f\"   {latest_json}\")\n",
        "\n",
        "    # Display file size\n",
        "    file_size_kb = Path(json_path).stat().st_size / 1024\n",
        "    print(f\"\\nüìä File size: {file_size_kb:.1f} KB\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error saving JSON files: {e}\")\n",
        "    print(f\"   Please ensure RESULTS_DIR is defined and writeable: {RESULTS_DIR}\")\n",
        "\n",
        "\n",
        "print(f\"\\nüì¶ Models included: {len(complete_results['models_evaluated'])}\")\n",
        "print(f\"üìã Benchmarks per model: {len(BENCHMARKS_INSTRUCT)}\")\n",
        "print(f\"üî¨ Total result entries: {len(df)}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"‚úÖ COMPLETE RESULTS SAVED - Ready for research sharing\")\n",
        "# --- THIS IS THE CORRECTED LINE ---\n",
        "print(f\"{'='*70}\\n\")"
      ],
      "metadata": {
        "id": "nfjYJe0yNNhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6JVP-5RIvXI"
      },
      "source": [
        "---\n",
        "\n",
        "## üéì Key Takeaways\n",
        "\n",
        "This notebook evaluated the Llama-3.2-1B model family across 10 comprehensive benchmarks to determine:\n",
        "\n",
        "1. **Optimal pruning level** for GLU-MLP layers\n",
        "2. **Performance-efficiency trade-offs** at different expansion ratios\n",
        "3. **Which models justify upload** to HuggingFace Hub\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Powered by OptiPFair** - Structured Pruning for GLU Architectures\n",
        "\n",
        "If this research helps your work:\n",
        "- ‚≠ê Star [the repo](https://github.com/peremartra/optipfair)\n",
        "- üìñ Read the [documentation](https://peremartra.github.io/optipfair/)\n",
        "- üêõ Report issues or suggest features\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2oEyjdt9E42b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}