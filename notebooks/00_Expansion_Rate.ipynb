{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üî¨ Expansion Rate Documentation - GLU Width Pruning\n",
        "\n",
        "**Research Paper:** [Width Pruning in GLU-MLP Layers](https://doi.org/10.31219/osf.io/qgxea)\n",
        "\n",
        "[![Paper](https://img.shields.io/badge/OSF-Paper-blue?logo=osf&logoColor=white)](https://doi.org/10.31219/osf.io/qgxea)\n",
        "[![GitHub](https://img.shields.io/badge/‚≠ê_Star-OptiPFair-orange?logo=github&logoColor=white)](https://github.com/peremartra/optipfair)\n",
        "[![PyPI](https://img.shields.io/pypi/v/optipfair?logo=python&logoColor=white&label=v)](https://pypi.org/project/optipfair/)\n",
        "\n",
        "**Repository:** [github.com/peremartra/llama-glu-expansion-pruning](https://github.com/peremartra/llama-glu-expansion-pruning)\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Notebook Objective\n",
        "\n",
        "This notebook documents the **actual expansion rates** achieved after applying width pruning to GLU-MLP layers across all model configurations. It serves as a reference for:\n",
        "\n",
        "1. **Verifying pruning accuracy:** Confirm that each `pruning_pct` produces the expected expansion rate\n",
        "2. **Architecture documentation:** Record detailed layer dimensions for reproducibility\n",
        "3. **Parameter reduction analysis:** Calculate exact parameter savings per model\n",
        "4. **Cross-model comparison:** Compare expansion rates across 1B, 3B, and Instruct variants\n",
        "\n",
        "### Key Features:\n",
        "- ‚úÖ **Automated calculation:** Uses OptIFAIR to recreate all pruned models on-the-fly\n",
        "- ‚úÖ **Complete documentation:** Records all architecture details in structured JSON\n",
        "- ‚úÖ **No external dependencies:** Self-contained, no need for pre-existing model files\n",
        "- ‚úÖ **Reproducibility:** Establishes baseline for all future experiments\n",
        "\n",
        "### Output:\n",
        "- `expansion_rates.json`: Complete architecture details for all 18 model configurations\n",
        "- Summary table with expansion rates and parameter reductions\n",
        "\n",
        "---\n",
        "\n",
        "**Colab Environment:** CPU sufficient (no GPU needed for architecture inspection)\n",
        "\n",
        "**Estimated Runtime:** ~30-45 minutes (depends on download speeds)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install OptIFAIR library for structured GLU pruning\n",
        "!pip install -q optipfair\n",
        "\n",
        "print(\"‚úÖ OptIFAIR installed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download utils.py from GitHub repository\n",
        "!wget -q https://raw.githubusercontent.com/peremartra/llama-glu-expansion-pruning/main/utils.py\n",
        "\n",
        "# Verify download\n",
        "import os\n",
        "if os.path.exists('utils.py'):\n",
        "    print(\"‚úÖ utils.py downloaded successfully\")\n",
        "else:\n",
        "    print(\"‚ùå Failed to download utils.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import json\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from transformers import AutoModelForCausalLM\n",
        "from optipfair import prune_model\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Import experiment configuration\n",
        "from utils import EXPERIMENT_CONFIG\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(EXPERIMENT_CONFIG)} model configurations\")\n",
        "print(f\"üìÖ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Configuration & Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Output configuration\n",
        "OUTPUT_FILE = \"expansion_rates.json\"\n",
        "\n",
        "# OptIFAIR pruning parameters (matching paper methodology)\n",
        "PRUNING_CONFIG = {\n",
        "    \"pruning_type\": \"MLP_GLU\",\n",
        "    \"neuron_selection_method\": \"MAW\",  # Maximum Absolute Weight (optimal for GLU)\n",
        "    \"return_stats\": True\n",
        "}\n",
        "\n",
        "# Model loading configuration\n",
        "MODEL_LOAD_CONFIG = {\n",
        "    \"torch_dtype\": torch.bfloat16,\n",
        "    \"device_map\": \"auto\",\n",
        "    \"low_cpu_mem_usage\": True\n",
        "}\n",
        "\n",
        "print(\"‚öôÔ∏è Configuration:\")\n",
        "print(f\"   Output file: {OUTPUT_FILE}\")\n",
        "print(f\"   Pruning method: {PRUNING_CONFIG['neuron_selection_method']}\")\n",
        "print(f\"   Pruning type: {PRUNING_CONFIG['pruning_type']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Core Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model_architecture_info(model):\n",
        "    \"\"\"\n",
        "    Extract architecture information from a Llama model.\n",
        "    \n",
        "    Args:\n",
        "        model: HuggingFace model instance\n",
        "        \n",
        "    Returns:\n",
        "        dict: Architecture details including dimensions and expansion rate\n",
        "    \"\"\"\n",
        "    config = model.config\n",
        "    \n",
        "    # Get dimensions from model config\n",
        "    hidden_size = config.hidden_size\n",
        "    intermediate_size = config.intermediate_size\n",
        "    num_hidden_layers = config.num_hidden_layers\n",
        "    \n",
        "    # Calculate expansion rate\n",
        "    expansion_rate = intermediate_size / hidden_size\n",
        "    \n",
        "    # Count total parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    \n",
        "    return {\n",
        "        \"hidden_size\": hidden_size,\n",
        "        \"intermediate_size\": intermediate_size,\n",
        "        \"num_hidden_layers\": num_hidden_layers,\n",
        "        \"expansion_rate\": round(expansion_rate, 2),\n",
        "        \"expansion_rate_percentage\": round(expansion_rate * 100, 1),\n",
        "        \"total_parameters\": total_params,\n",
        "        \"total_parameters_millions\": round(total_params / 1e6, 2)\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_expansion_rate_for_config(config_entry):\n",
        "    \"\"\"\n",
        "    Load model, apply pruning, and extract expansion rate information.\n",
        "    \n",
        "    Args:\n",
        "        config_entry: Dictionary from EXPERIMENT_CONFIG\n",
        "        \n",
        "    Returns:\n",
        "        dict: Complete model information including original and pruned architectures\n",
        "    \"\"\"\n",
        "    base_model_name = config_entry[\"base_model\"]\n",
        "    pruning_pct = config_entry[\"pruning_pct\"]\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Processing: {base_model_name} @ {pruning_pct}% pruning\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    try:\n",
        "        # Load base model\n",
        "        print(f\"üì• Loading base model: {base_model_name}...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            base_model_name,\n",
        "            **MODEL_LOAD_CONFIG\n",
        "        )\n",
        "        \n",
        "        # Get original architecture info\n",
        "        print(\"üìä Analyzing original architecture...\")\n",
        "        original_arch = get_model_architecture_info(model)\n",
        "        print(f\"   Original expansion rate: {original_arch['expansion_rate_percentage']}%\")\n",
        "        print(f\"   Original parameters: {original_arch['total_parameters_millions']}M\")\n",
        "        \n",
        "        # Apply pruning\n",
        "        print(f\"‚úÇÔ∏è Applying {pruning_pct}% pruning with MAW method...\")\n",
        "        pruned_model, stats = prune_model(\n",
        "            model=model,\n",
        "            pruning_percentage=pruning_pct,\n",
        "            **PRUNING_CONFIG\n",
        "        )\n",
        "        \n",
        "        # Get pruned architecture info\n",
        "        print(\"üìä Analyzing pruned architecture...\")\n",
        "        pruned_arch = get_model_architecture_info(pruned_model)\n",
        "        print(f\"   Pruned expansion rate: {pruned_arch['expansion_rate_percentage']}%\")\n",
        "        print(f\"   Pruned parameters: {pruned_arch['total_parameters_millions']}M\")\n",
        "        \n",
        "        # Calculate reductions\n",
        "        param_reduction_pct = (\n",
        "            (original_arch['total_parameters'] - pruned_arch['total_parameters']) \n",
        "            / original_arch['total_parameters'] * 100\n",
        "        )\n",
        "        \n",
        "        expansion_reduction_pct = (\n",
        "            (original_arch['expansion_rate'] - pruned_arch['expansion_rate'])\n",
        "            / original_arch['expansion_rate'] * 100\n",
        "        )\n",
        "        \n",
        "        print(f\"üìâ Parameter reduction: {param_reduction_pct:.2f}%\")\n",
        "        print(f\"üìâ Expansion reduction: {expansion_reduction_pct:.2f}%\")\n",
        "        \n",
        "        # Clean up to free memory\n",
        "        del model\n",
        "        del pruned_model\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        # Return complete info\n",
        "        return {\n",
        "            \"base_model\": base_model_name,\n",
        "            \"pruning_pct\": pruning_pct,\n",
        "            \"original_architecture\": original_arch,\n",
        "            \"pruned_architecture\": pruned_arch,\n",
        "            \"reductions\": {\n",
        "                \"parameter_reduction_pct\": round(param_reduction_pct, 2),\n",
        "                \"expansion_reduction_pct\": round(expansion_reduction_pct, 2),\n",
        "                \"parameters_saved_millions\": round(\n",
        "                    (original_arch['total_parameters'] - pruned_arch['total_parameters']) / 1e6, 2\n",
        "                )\n",
        "            },\n",
        "            \"optipfair_stats\": stats,\n",
        "            \"status\": \"success\"\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing {base_model_name} @ {pruning_pct}%: {str(e)}\")\n",
        "        return {\n",
        "            \"base_model\": base_model_name,\n",
        "            \"pruning_pct\": pruning_pct,\n",
        "            \"status\": \"failed\",\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ Functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Process All Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "print(f\"\\nüöÄ Starting expansion rate calculation for {len(EXPERIMENT_CONFIG)} models...\\n\")\n",
        "\n",
        "# Process each configuration\n",
        "for i, config in enumerate(tqdm(EXPERIMENT_CONFIG, desc=\"Processing models\")):\n",
        "    print(f\"\\n[{i+1}/{len(EXPERIMENT_CONFIG)}] Processing configuration...\")\n",
        "    \n",
        "    result = calculate_expansion_rate_for_config(config)\n",
        "    all_results.append(result)\n",
        "    \n",
        "    # Brief status update\n",
        "    if result['status'] == 'success':\n",
        "        print(f\"‚úÖ Success: {result['pruned_architecture']['expansion_rate_percentage']}% expansion rate\")\n",
        "    else:\n",
        "        print(f\"‚ùå Failed: {result.get('error', 'Unknown error')}\")\n",
        "\n",
        "print(f\"\\n\\n{'='*80}\")\n",
        "print(f\"‚úÖ Completed processing all {len(EXPERIMENT_CONFIG)} models\")\n",
        "print(f\"{'='*80}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Save Results to JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare final JSON structure\n",
        "output_data = {\n",
        "    \"metadata\": {\n",
        "        \"generated_at\": datetime.now().isoformat(),\n",
        "        \"optipfair_version\": \"latest\",  # Could get actual version if needed\n",
        "        \"total_models\": len(all_results),\n",
        "        \"successful\": sum(1 for r in all_results if r['status'] == 'success'),\n",
        "        \"failed\": sum(1 for r in all_results if r['status'] == 'failed'),\n",
        "        \"pruning_method\": PRUNING_CONFIG['neuron_selection_method'],\n",
        "        \"pruning_type\": PRUNING_CONFIG['pruning_type']\n",
        "    },\n",
        "    \"models\": all_results\n",
        "}\n",
        "\n",
        "# Save to JSON file\n",
        "with open(OUTPUT_FILE, 'w') as f:\n",
        "    json.dump(output_data, f, indent=2)\n",
        "\n",
        "print(f\"\\nüíæ Results saved to: {OUTPUT_FILE}\")\n",
        "print(f\"   Total models: {output_data['metadata']['total_models']}\")\n",
        "print(f\"   Successful: {output_data['metadata']['successful']}\")\n",
        "print(f\"   Failed: {output_data['metadata']['failed']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Summary Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary dataframe for successful models\n",
        "successful_results = [r for r in all_results if r['status'] == 'success']\n",
        "\n",
        "summary_data = []\n",
        "for result in successful_results:\n",
        "    # Extract model family (1B, 3B, 1B-I, 3B-I)\n",
        "    base_name = result['base_model'].split('/')[-1]\n",
        "    if '1B-Instruct' in base_name:\n",
        "        family = '1B-Instruct'\n",
        "    elif '3B-Instruct' in base_name:\n",
        "        family = '3B-Instruct'\n",
        "    elif '1B' in base_name:\n",
        "        family = '1B'\n",
        "    else:\n",
        "        family = '3B'\n",
        "    \n",
        "    summary_data.append({\n",
        "        'Model Family': family,\n",
        "        'Pruning %': result['pruning_pct'],\n",
        "        'Original Expansion %': result['original_architecture']['expansion_rate_percentage'],\n",
        "        'Pruned Expansion %': result['pruned_architecture']['expansion_rate_percentage'],\n",
        "        'Expansion Reduction %': result['reductions']['expansion_reduction_pct'],\n",
        "        'Original Params (M)': result['original_architecture']['total_parameters_millions'],\n",
        "        'Pruned Params (M)': result['pruned_architecture']['total_parameters_millions'],\n",
        "        'Param Reduction %': result['reductions']['parameter_reduction_pct'],\n",
        "        'Params Saved (M)': result['reductions']['parameters_saved_millions']\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "# Sort by model family and pruning percentage\n",
        "summary_df = summary_df.sort_values(['Model Family', 'Pruning %'])\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"üìä EXPANSION RATE SUMMARY\")\n",
        "print(\"=\"*100)\n",
        "print(summary_df.to_string(index=False))\n",
        "print(\"=\"*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Model Family Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group by model family for comparison\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"üìä COMPARISON BY MODEL FAMILY\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "for family in sorted(summary_df['Model Family'].unique()):\n",
        "    family_df = summary_df[summary_df['Model Family'] == family]\n",
        "    \n",
        "    print(f\"\\nüîπ {family} Model Family:\")\n",
        "    print(f\"   Base expansion rate: {family_df.iloc[0]['Original Expansion %']}%\")\n",
        "    print(f\"   Base parameters: {family_df.iloc[0]['Original Params (M)']}M\")\n",
        "    print(f\"\\n   Pruned variants:\")\n",
        "    \n",
        "    for _, row in family_df.iterrows():\n",
        "        print(f\"      {row['Pruning %']:2d}% pruning ‚Üí {row['Pruned Expansion %']:5.1f}% expansion \"\n",
        "              f\"({row['Pruned Params (M):5.0f']}M params, {row['Param Reduction %']:.1f}% reduction)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. Target Expansion Rate Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify which models achieve the target 140% expansion rate\n",
        "TARGET_EXPANSION = 140.0\n",
        "\n",
        "print(f\"\\n{'='*100}\")\n",
        "print(f\"üéØ TARGET EXPANSION RATE: {TARGET_EXPANSION}%\")\n",
        "print(f\"{'='*100}\\n\")\n",
        "\n",
        "# Find models closest to target\n",
        "summary_df['Distance from Target'] = abs(summary_df['Pruned Expansion %'] - TARGET_EXPANSION)\n",
        "\n",
        "for family in sorted(summary_df['Model Family'].unique()):\n",
        "    family_df = summary_df[summary_df['Model Family'] == family]\n",
        "    closest = family_df.loc[family_df['Distance from Target'].idxmin()]\n",
        "    \n",
        "    print(f\"üîπ {family}:\")\n",
        "    print(f\"   Closest model: {closest['Pruning %']}% pruning\")\n",
        "    print(f\"   Achieved expansion: {closest['Pruned Expansion %']}%\")\n",
        "    print(f\"   Distance from target: {closest['Distance from Target']:.1f}%\")\n",
        "    print()\n",
        "\n",
        "print(f\"{'='*100}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9. Export Summary to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save summary table to CSV for easy reference\n",
        "csv_filename = \"expansion_rates_summary.csv\"\n",
        "summary_df.drop('Distance from Target', axis=1, inplace=True)  # Remove helper column\n",
        "summary_df.to_csv(csv_filename, index=False)\n",
        "\n",
        "print(f\"üìä Summary table exported to: {csv_filename}\")\n",
        "print(f\"\\n‚úÖ All analysis complete!\")\n",
        "print(f\"\\nüìÅ Output files:\")\n",
        "print(f\"   - {OUTPUT_FILE} (complete architecture details)\")\n",
        "print(f\"   - {csv_filename} (summary table)\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
