{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNEDaThPAnKsxXpOCmqJZK0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/llama-glu-expansion-pruning/blob/main/notebooks/00_Neuron_Selection_Method_Comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GLU Pruning Research\n",
        "## 00 - Neuron Selection Method Comparison: MAW vs VOW vs PON\n",
        "\n",
        "### Exploring GLU Expansion Ratios in Llama-3.2 Models\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "\n",
        "[![Paper](https://img.shields.io/badge/OSF-Paper-blue?logo=osf&logoColor=white)](https://doi.org/10.31219/osf.io/qgxea)\n",
        "[![GitHub](https://img.shields.io/badge/⭐_Star-OptiPFair-orange?logo=github&logoColor=white)](https://github.com/peremartra/optipfair)\n",
        "[![PyPI](https://img.shields.io/pypi/v/optipfair?logo=python&logoColor=white&label=v)](https://pypi.org/project/optipfair/)\n",
        "**Repository:** [github.com/peremartra/llama-glu-expansion-pruning](https://github.com/peremartra/llama-glu-expansion-pruning)\n",
        "\n",
        "---\n",
        "\n",
        "**Colab Environment:** CPU (model creation) + GPU T4 (evaluation)\n",
        "\n",
        "**Models:**\n",
        "* Llama-3.2-1B (base)\n",
        "\n",
        "**Benchmarks (in order):**\n",
        "* BoolQ (0-shot) - ~15 min\n",
        "* Lambada (0-shot) - ~15 min\n",
        "* IFEval (0-shot) - ~20 min\n",
        "* GSM8K (5-shot CoT) - ~30 min *(if needed)*\n",
        "\n",
        "---\n",
        "\n",
        "## 📋 Notebook Objective\n",
        "\n",
        "This preliminary experiment compares three neuron importance metrics for GLU-based pruning:\n",
        "- **MAW** (Maximum Absolute Weight) - Default method in OptiPFair\n",
        "- **VOW** (Variance of Weights)\n",
        "- **PON** (Product of Norms)\n",
        "\n",
        "**Goal:** Empirically determine which method best preserves model capabilities under pruning before conducting the main experiments. We will create three versions of Llama-3.2-1B pruned at 10% (one per method) and evaluate them sequentially on progressively demanding benchmarks.\n",
        "\n",
        "**Evaluation Strategy:** We'll start with the fastest benchmarks (BoolQ, Lambada) to quickly identify clear winners or losers. If methods show similar performance, we'll proceed to more demanding benchmarks (IFEval, GSM8K) for differentiation.\n",
        "\n",
        "**Why this matters:** The choice of neuron selection method fundamentally affects which neurons are removed during pruning. This \"tournament\" ensures our main experiments use the most effective approach, with results documented in the paper's methodology section.\n",
        "\n",
        "**Output:** The winning method will be used exclusively for all subsequent pruning experiments (01 and 02 notebooks).\n",
        "\n",
        "---\n",
        "\n",
        "## 📦 Required Libraries\n",
        "\n",
        "```python\n",
        "# Install OptiPFair for GLU pruning\n",
        "!pip install optipfair\n",
        "\n",
        "# Install LM Evaluation Harness for benchmarking\n",
        "!pip install lm-eval\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Note:** This notebook is part of a larger research project on width pruning in GLU architectures. See the full paper and codebase at the links above.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "KDsNzh0Edz4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup & Install"
      ],
      "metadata": {
        "id": "-YaGDutmijI0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYIJswmUdQqk",
        "outputId": "36b8e68c-4bf0-40e6-f657-6bb5adce88c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries.\n",
        "!pip install -q optipfair\n",
        "!pip install -q lm-eval\n",
        "!pip install -q langdetect"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from optipfair import prune_model\n",
        "from datasets import load_dataset\n",
        "import copy\n",
        "import json\n",
        "import gc\n",
        "from datetime import datetime\n",
        "from lm_eval import evaluator\n",
        "from lm_eval.models.huggingface import HFLM"
      ],
      "metadata": {
        "id": "MqC86Q0-iuK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_gpu_cache():\n",
        "    \"\"\"Limpia completamente la cache de GPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "    gc.collect()\n"
      ],
      "metadata": {
        "id": "tEXrIRSTdsDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"meta-llama/Llama-3.2-1B\"\n",
        "PRUNING_PERCENTAGE = 10\n",
        "PERPLEXITY_SAMPLES=500\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "RF_Vg3t7lhM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Base Model"
      ],
      "metadata": {
        "id": "p4KtFoaBkbt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "  MODEL_ID,\n",
        "  dtype=torch.float16,\n",
        "  device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "bU6rkLzvkdzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "if tokenizer.pad_token is None:\n",
        "  tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "xNRsCAFujdGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluations"
      ],
      "metadata": {
        "id": "mZQkno1cmK3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluations Code"
      ],
      "metadata": {
        "id": "5VACHwy93Sdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluation(model_obj, tokenizer, tasks, limit=None):\n",
        "    \"\"\"\n",
        "    Runs lm-eval on a model and tokenizer object already in memory.\n",
        "\n",
        "    Args:\n",
        "        model_obj: The PyTorch model object to evaluate.\n",
        "        tokenizer_obj: The tokenizer object.\n",
        "        tasks (list): A list of task names for lm-eval.\n",
        "        limit (int): The number of samples per task.\n",
        "    \"\"\"\n",
        "    model_name = getattr(model_obj.config, '_name_or_path', 'unknown')\n",
        "    limit_str = f\"(limit={limit})\" if limit else \"(full dataset)\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Starting lm-eval on model '{model_name}'\")\n",
        "    print(f\"Tasks: {tasks} {limit_str}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Wrap the local model object and tokenizer for lm-eval\n",
        "    model_wrapper = HFLM(\n",
        "        pretrained=model_obj,\n",
        "        tokenizer=tokenizer,\n",
        "        device=str(DEVICE)\n",
        "    )\n",
        "\n",
        "    # Run evaluation\n",
        "    results = evaluator.simple_evaluate(\n",
        "        model=model_wrapper,\n",
        "        tasks=tasks,\n",
        "        num_fewshot=0,\n",
        "        limit=limit,\n",
        "        device=str(DEVICE)\n",
        "    )\n",
        "\n",
        "    # Format results for clean display\n",
        "    formatted_results = {}\n",
        "    for task_name, res in results[\"results\"].items():\n",
        "        # Extract relevant metrics based on task type\n",
        "        if 'perplexity,none' in res:\n",
        "            # For perplexity tasks (wikitext)\n",
        "            formatted_results[task_name] = {\n",
        "                'perplexity': f\"{res.get('perplexity,none', 0):.2f}\",\n",
        "                'word_perplexity': f\"{res.get('word_perplexity,none', 0):.2f}\",\n",
        "                'bits_per_byte': f\"{res.get('bits_per_byte,none', 0):.4f}\"\n",
        "            }\n",
        "        elif 'acc,none' in res:\n",
        "            # For accuracy tasks (boolq, etc.)\n",
        "            formatted_results[task_name] = {\n",
        "                'accuracy': f\"{res.get('acc,none', 0):.4f}\",\n",
        "                'acc_norm': f\"{res.get('acc_norm,none', 0):.4f}\"\n",
        "            }\n",
        "        else:\n",
        "            # Fallback: store all available metrics\n",
        "            formatted_results[task_name] = {k: f\"{v:.4f}\" for k, v in res.items() if isinstance(v, (int, float))}\n",
        "\n",
        "    return formatted_results"
      ],
      "metadata": {
        "id": "372ftmpVt08W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TASKS = [\"wikitext\", \"boolq\", \"lambada_openai\"]\n"
      ],
      "metadata": {
        "id": "HNN1IbF2l3e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_results = model_evaluation(\n",
        "  model_obj=base_model,\n",
        "  tokenizer=tokenizer,\n",
        "  tasks=TASKS,\n",
        "  limit=None  # Full dataset\n",
        ")"
      ],
      "metadata": {
        "id": "BNwOoxgSmvcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "087a3701-49dd-468b-f013-0336c8cedb75",
        "id": "FAaXGou_mvcw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boolq': {'accuracy': '0.6391', 'acc_norm': '0.0000'},\n",
              " 'lambada_openai': {'perplexity': '5.72',\n",
              "  'word_perplexity': '0.00',\n",
              "  'bits_per_byte': '0.0000'},\n",
              " 'wikitext': {'word_perplexity,none': '11.5688',\n",
              "  'byte_perplexity,none': '1.5807',\n",
              "  'bits_per_byte,none': '0.6605'}}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAW selecion model\n"
      ],
      "metadata": {
        "id": "zd05349RmS-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Pruned Model with MAW\n",
        "pruned_model_maw, stats = prune_model(\n",
        "    model=copy.deepcopy(base_model),\n",
        "    pruning_type=\"MLP_GLU\",\n",
        "    neuron_selection_method=\"MAW\",\n",
        "    pruning_percentage=PRUNING_PERCENTAGE,\n",
        "    show_progress=True,\n",
        "    return_stats=True\n",
        ")\n",
        "stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GdvV3jglOSY",
        "outputId": "01c307fb-9f14-413a-9737-6b0cde2e29d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning layers: 100%|██████████| 16/16 [00:06<00:00,  2.47it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'original_parameters': 1235814400,\n",
              " 'pruned_parameters': 1155303424,\n",
              " 'reduction': 80510976,\n",
              " 'percentage_reduction': 6.5148112855781575,\n",
              " 'expansion_rate': 360.009765625}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAW_results = model_evaluation(\n",
        "  model_obj=pruned_model_maw,\n",
        "  tokenizer=tokenizer,\n",
        "  tasks=TASKS,\n",
        "  limit=None  # Full dataset\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZpAut-2uw7B",
        "outputId": "639bbd1f-22a9-4993-c825-b0ac578b69db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
            "WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Starting lm-eval on model 'meta-llama/Llama-3.2-1B'\n",
            "Tasks: ['wikitext', 'boolq', 'lambada_openai'] (full dataset)\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:lm_eval.api.task:[Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity\n",
            "WARNING:lm_eval.api.task:[Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False\n",
            "WARNING:lm_eval.api.task:[Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity\n",
            "WARNING:lm_eval.api.task:[Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False\n",
            "WARNING:lm_eval.api.task:[Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte\n",
            "WARNING:lm_eval.api.task:[Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False\n",
            "WARNING:lm_eval.api.task:[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
            "WARNING:lm_eval.api.task:[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of lambada_openai from None to 0\n",
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of boolq from None to 0\n",
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wikitext from None to 0\n",
            "100%|██████████| 5153/5153 [00:09<00:00, 544.42it/s]\n",
            "100%|██████████| 3270/3270 [00:01<00:00, 1799.24it/s]\n",
            "100%|██████████| 62/62 [00:00<00:00, 548.11it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 11693/11693 [03:05<00:00, 63.04it/s]\n",
            "100%|██████████| 62/62 [00:00<00:00, 90.36it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  4.58it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  5.64it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  4.86it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  5.06it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  6.61it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  3.85it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 11.01it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  5.17it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 19.88it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  6.52it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 10.35it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.83it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.93it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  8.45it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.83it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  4.73it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  9.45it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 38.98it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 41.93it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  7.23it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  6.66it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  7.53it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.56s/it]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.68it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  4.63it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  9.91it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.00it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 10.48it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  8.94it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  7.99it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  7.86it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  4.19it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  4.16it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 10.73it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 10.38it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 11.08it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 11.28it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 18.61it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  3.61it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.87it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.47it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  3.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bootstrapping for stddev: perplexity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:08<00:00, 11.53it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAW_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBc3oZm2pxR1",
        "outputId": "e2b4e521-75b2-4076-b497-c54f5a6316fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boolq': {'accuracy': '0.6251', 'acc_norm': '0.0000'},\n",
              " 'lambada_openai': {'perplexity': '20.54',\n",
              "  'word_perplexity': '0.00',\n",
              "  'bits_per_byte': '0.0000'},\n",
              " 'wikitext': {'word_perplexity,none': '17.4945',\n",
              "  'byte_perplexity,none': '1.7078',\n",
              "  'bits_per_byte,none': '0.7721'}}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Delete MAW model\n",
        "del pruned_model_maw\n",
        "clear_gpu_cache()"
      ],
      "metadata": {
        "id": "LdF8nznicqbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PON Selection Model\n"
      ],
      "metadata": {
        "id": "bExwACzx0sl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Pruned Model with PON\n",
        "pruned_model_pon, stats = prune_model(\n",
        "    model=copy.deepcopy(base_model),\n",
        "    pruning_type=\"MLP_GLU\",\n",
        "    neuron_selection_method=\"PON\",\n",
        "    pruning_percentage=PRUNING_PERCENTAGE,\n",
        "    show_progress=True,\n",
        "    return_stats=True\n",
        ")\n",
        "stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b88d5661-4d5b-4bb7-c61a-27f9d7254b64",
        "id": "Nu6tHMBIeiP-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning layers: 100%|██████████| 16/16 [00:05<00:00,  2.71it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'original_parameters': 1235814400,\n",
              " 'pruned_parameters': 1155303424,\n",
              " 'reduction': 80510976,\n",
              " 'percentage_reduction': 6.5148112855781575,\n",
              " 'expansion_rate': 360.009765625}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PON_results = model_evaluation(\n",
        "  model_obj=pruned_model_pon,\n",
        "  tokenizer=tokenizer,\n",
        "  tasks=TASKS,\n",
        "  limit=None  # Full dataset\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqzNbvbefOQt",
        "outputId": "9b14b959-2145-4afb-b5b0-b1a3160fdf66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
            "WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Starting lm-eval on model 'meta-llama/Llama-3.2-1B'\n",
            "Tasks: ['wikitext', 'boolq', 'lambada_openai'] (full dataset)\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:lm_eval.api.task:[Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity\n",
            "WARNING:lm_eval.api.task:[Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False\n",
            "WARNING:lm_eval.api.task:[Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity\n",
            "WARNING:lm_eval.api.task:[Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False\n",
            "WARNING:lm_eval.api.task:[Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte\n",
            "WARNING:lm_eval.api.task:[Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False\n",
            "WARNING:lm_eval.api.task:[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
            "WARNING:lm_eval.api.task:[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of lambada_openai from None to 0\n",
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of boolq from None to 0\n",
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wikitext from None to 0\n",
            "100%|██████████| 5153/5153 [00:09<00:00, 542.49it/s]\n",
            "100%|██████████| 3270/3270 [00:01<00:00, 1707.71it/s]\n",
            "100%|██████████| 62/62 [00:00<00:00, 539.10it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 11693/11693 [03:05<00:00, 63.14it/s]\n",
            "100%|██████████| 62/62 [00:00<00:00, 84.10it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 12.07it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  4.59it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  5.67it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  4.89it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  4.98it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  6.73it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  3.85it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 11.40it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  5.23it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 18.94it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  6.44it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 10.29it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.78it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.99it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.44it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  8.24it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.82it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  4.70it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  9.36it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 44.45it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 42.30it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  7.24it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  6.63it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  7.44it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.80s/it]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.72it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  4.61it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 10.11it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.99it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  4.99it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 10.41it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  8.70it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  7.86it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  8.08it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  4.17it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  4.22it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 10.85it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 10.38it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 11.03it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 12.09it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 18.19it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  3.63it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.87it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.50it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  3.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bootstrapping for stddev: perplexity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:08<00:00, 11.64it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PON_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7xvX-IpfjUO",
        "outputId": "fba011af-f11a-47e1-be56-1495614bff06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boolq': {'accuracy': '0.6220', 'acc_norm': '0.0000'},\n",
              " 'lambada_openai': {'perplexity': '2032.80',\n",
              "  'word_perplexity': '0.00',\n",
              "  'bits_per_byte': '0.0000'},\n",
              " 'wikitext': {'word_perplexity,none': '72.5170',\n",
              "  'byte_perplexity,none': '2.2280',\n",
              "  'bits_per_byte,none': '1.1557'}}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del pruned_model_pon\n",
        "clear_gpu_cache()"
      ],
      "metadata": {
        "id": "aOOOzqhbiNvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VOW Selection Model\n"
      ],
      "metadata": {
        "id": "VrqLDCm6h_0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Pruned Model with PON\n",
        "pruned_model_vow, stats = prune_model(\n",
        "    model=copy.deepcopy(base_model),\n",
        "    pruning_type=\"MLP_GLU\",\n",
        "    neuron_selection_method=\"VOW\",\n",
        "    pruning_percentage=PRUNING_PERCENTAGE,\n",
        "    show_progress=True,\n",
        "    return_stats=True\n",
        ")\n",
        "stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45746521-0c30-4e14-a875-b7ca92848c49",
        "id": "YV2ENt2NiR2W"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning layers: 100%|██████████| 16/16 [00:05<00:00,  2.68it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'original_parameters': 1235814400,\n",
              " 'pruned_parameters': 1155303424,\n",
              " 'reduction': 80510976,\n",
              " 'percentage_reduction': 6.5148112855781575,\n",
              " 'expansion_rate': 360.009765625}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VOW_results = model_evaluation(\n",
        "  model_obj=pruned_model_vow,\n",
        "  tokenizer=tokenizer,\n",
        "  tasks=TASKS,\n",
        "  limit=None  # Full dataset\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e56964c8-00ab-430a-ad1c-81cfce5c15f3",
        "id": "RljqmIWHiR2X"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
            "WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Starting lm-eval on model 'meta-llama/Llama-3.2-1B'\n",
            "Tasks: ['wikitext', 'boolq', 'lambada_openai'] (full dataset)\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:lm_eval.api.task:[Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity\n",
            "WARNING:lm_eval.api.task:[Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False\n",
            "WARNING:lm_eval.api.task:[Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity\n",
            "WARNING:lm_eval.api.task:[Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False\n",
            "WARNING:lm_eval.api.task:[Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte\n",
            "WARNING:lm_eval.api.task:[Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False\n",
            "WARNING:lm_eval.api.task:[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
            "WARNING:lm_eval.api.task:[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of lambada_openai from None to 0\n",
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of boolq from None to 0\n",
            "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of wikitext from None to 0\n",
            "100%|██████████| 5153/5153 [00:09<00:00, 539.62it/s]\n",
            "100%|██████████| 3270/3270 [00:01<00:00, 1785.44it/s]\n",
            "100%|██████████| 62/62 [00:00<00:00, 542.92it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 11693/11693 [03:06<00:00, 62.74it/s]\n",
            "100%|██████████| 62/62 [00:00<00:00, 87.82it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 11.98it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  4.58it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  5.70it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  4.89it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  6.79it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  3.86it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 11.19it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  5.30it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 18.24it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  6.46it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 10.11it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.84it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.97it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  8.23it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.82it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  4.65it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  9.44it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 44.00it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 42.45it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  7.39it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  6.73it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  7.50it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.80s/it]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  4.65it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 10.02it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.99it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  5.20it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 10.23it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  8.75it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  7.92it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  8.08it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  4.16it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  4.18it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 10.90it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 10.39it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 11.11it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 12.10it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00, 18.50it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  3.64it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.86it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.51it/s]\n",
            "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  3.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bootstrapping for stddev: perplexity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:08<00:00, 11.66it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VOW_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "408f5877-66f0-4f1c-caaf-9d8f26f6b9af",
        "id": "yWBhFwMuiR2X"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boolq': {'accuracy': '0.6193', 'acc_norm': '0.0000'},\n",
              " 'lambada_openai': {'perplexity': '532.36',\n",
              "  'word_perplexity': '0.00',\n",
              "  'bits_per_byte': '0.0000'},\n",
              " 'wikitext': {'word_perplexity,none': '50.5592',\n",
              "  'byte_perplexity,none': '2.0827',\n",
              "  'bits_per_byte,none': '1.0584'}}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del pruned_model_vow\n",
        "clear_gpu_cache()"
      ],
      "metadata": {
        "id": "I_AFjsBziR2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📊 Method Comparison Results\n",
        "\n",
        "| Model | WikiText PPL ↓ | Lambada PPL ↓ | BoolQ Acc ↑ | Status |\n",
        "|-------|----------------|---------------|-------------|--------|\n",
        "| **Base** | **11.57** | **5.72** | **0.6391** | Baseline |\n",
        "| **MAW** | **17.49** (+51%) | **20.54** (+259%) | **0.6251** (-2.2%) | ✅ **SELECTED** |\n",
        "| VOW | 50.56 (+337%) | 532.36 (+9,207%) | 0.6193 (-3.1%) | ❌ Rejected |\n",
        "| PON | 72.52 (+527%) | 2032.80 (+35,440%) | 0.6220 (-2.7%) | ❌ Rejected |\n",
        "\n",
        "**Conclusion**: MAW is the clear winner with acceptable degradation (~50% PPL increase) while VOW/PON show catastrophic failures (10,000%+ increases). MAW will be used for all experiments."
      ],
      "metadata": {
        "id": "T6Uy7OIloo6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results are conclusive. While all pruning methods introduce some performance degradation compared to the base model, the MAW (Maximum Absolute Weight) method preserves the model's capabilities far more effectively than the alternatives.\n",
        "\n",
        "MAW shows only a minor increase in perplexity and a negligible drop in accuracy, demonstrating its ability to remove parameters without causing significant damage to the model's core fluency and comprehension.\n",
        "\n",
        "VOW and PON both result in a catastrophic loss of performance, especially visible in the wikitext and lambada_openai perplexity scores. A Lambada perplexity score above 2000 for the PON method indicates that the model has become almost unusable for text generation tasks.\n",
        "\n",
        "This data provides a strong empirical justification for selecting MAW as the sole neuron selection method for all subsequent, large-scale pruning experiments in this research project."
      ],
      "metadata": {
        "id": "qbEdhlyJpe-d"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m0x5VOo9op5W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}