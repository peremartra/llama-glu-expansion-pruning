{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyN/+x6YXmcDGNk/DWpyu1vX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/llama-glu-expansion-pruning/blob/main/notebooks/00_Neuron_Selection_Method_Comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GLU Pruning Research\n",
        "## 00 - Neuron Selection Method Comparison: MAW vs VOW vs PON\n",
        "\n",
        "### Exploring GLU Expansion Ratios in Llama-3.2 Models\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "\n",
        "[![Paper](https://img.shields.io/badge/OSF-Paper-blue?logo=osf&logoColor=white)](https://doi.org/10.31219/osf.io/qgxea)\n",
        "[![GitHub](https://img.shields.io/badge/â­_Star-OptiPFair-orange?logo=github&logoColor=white)](https://github.com/peremartra/optipfair)\n",
        "[![PyPI](https://img.shields.io/pypi/v/optipfair?logo=python&logoColor=white&label=v)](https://pypi.org/project/optipfair/)\n",
        "\n",
        "**Repository:** [github.com/peremartra/llama-glu-expansion-pruning](https://github.com/peremartra/llama-glu-expansion-pruning)\n",
        "\n",
        "---\n",
        "\n",
        "**Colab Environment:** GPU L4\n",
        "\n",
        "**Models:**\n",
        "* Llama-3.2-1B (base)\n",
        "\n",
        "**Benchmarks (in order):**\n",
        "* BoolQ (0-shot) - ~15 min\n",
        "* Lambada (0-shot) - ~15 min\n",
        "* IFEval (0-shot) - ~20 min\n",
        "* GSM8K (5-shot CoT) - ~30 min *(if needed)*\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“‹ Notebook Objective\n",
        "\n",
        "This preliminary experiment compares three neuron importance metrics for GLU-based pruning:\n",
        "- **MAW** (Maximum Absolute Weight) - Default method in OptiPFair\n",
        "- **VOW** (Variance of Weights)\n",
        "- **PON** (Product of Norms)\n",
        "\n",
        "**Goal:** Empirically determine which method best preserves model capabilities under pruning before conducting the main experiments. We will create three versions of Llama-3.2-1B pruned at 10% (one per method) and evaluate them sequentially on progressively demanding benchmarks.\n",
        "\n",
        "**Evaluation Strategy:** We'll start with the fastest benchmarks (BoolQ, Lambada) to quickly identify clear winners or losers. If methods show similar performance, we'll proceed to more demanding benchmarks (IFEval, GSM8K) for differentiation.\n",
        "\n",
        "**Why this matters:** The choice of neuron selection method fundamentally affects which neurons are removed during pruning. This \"tournament\" ensures our main experiments use the most effective approach, with results documented in the paper's methodology section.\n",
        "\n",
        "**Output:** The winning method will be used exclusively for all subsequent pruning experiments (01 and 02 notebooks).\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“¦ Required Libraries\n",
        "\n",
        "```python\n",
        "# Install OptiPFair for GLU pruning\n",
        "!pip install optipfair\n",
        "\n",
        "# Install LM Evaluation Harness for benchmarking\n",
        "!pip install lm-eval\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Note:** This notebook is part of a larger research project on width pruning in GLU architectures. See the full paper and codebase at the links above.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "KDsNzh0Edz4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup & Install"
      ],
      "metadata": {
        "id": "-YaGDutmijI0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYIJswmUdQqk",
        "outputId": "5331444c-61ee-466a-8dc7-15a62b982eb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries.\n",
        "!pip install -q optipfair\n",
        "!pip install -q lm-eval\n",
        "!pip install -q langdetect"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from optipfair import prune_model\n",
        "from datasets import load_dataset\n",
        "import copy\n",
        "import json\n",
        "import gc\n",
        "from datetime import datetime\n",
        "from lm_eval import evaluator\n",
        "from lm_eval.models.huggingface import HFLM"
      ],
      "metadata": {
        "id": "MqC86Q0-iuK9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_gpu_cache():\n",
        "    \"\"\"Limpia completamente la cache de GPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "    gc.collect()\n"
      ],
      "metadata": {
        "id": "tEXrIRSTdsDg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"meta-llama/Llama-3.2-1B\"\n",
        "PRUNING_PERCENTAGE = 10\n",
        "PERPLEXITY_SAMPLES=500\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "RF_Vg3t7lhM8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Base Model"
      ],
      "metadata": {
        "id": "p4KtFoaBkbt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "  MODEL_ID,\n",
        "  dtype=torch.float16,\n",
        "  device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "bU6rkLzvkdzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "if tokenizer.pad_token is None:\n",
        "  tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "xNRsCAFujdGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluations"
      ],
      "metadata": {
        "id": "mZQkno1cmK3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluations Code"
      ],
      "metadata": {
        "id": "5VACHwy93Sdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluation(model_obj, tokenizer, tasks, limit=None):\n",
        "    \"\"\"\n",
        "    Runs lm-eval on a model and tokenizer object already in memory.\n",
        "\n",
        "    Args:\n",
        "        model_obj: The PyTorch model object to evaluate.\n",
        "        tokenizer_obj: The tokenizer object.\n",
        "        tasks (list): A list of task names for lm-eval.\n",
        "        limit (int): The number of samples per task.\n",
        "    \"\"\"\n",
        "    model_name = getattr(model_obj.config, '_name_or_path', 'unknown')\n",
        "    limit_str = f\"(limit={limit})\" if limit else \"(full dataset)\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Starting lm-eval on model '{model_name}'\")\n",
        "    print(f\"Tasks: {tasks} {limit_str}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Wrap the local model object and tokenizer for lm-eval\n",
        "    model_wrapper = HFLM(\n",
        "        pretrained=model_obj,\n",
        "        tokenizer=tokenizer,\n",
        "        device=str(DEVICE)\n",
        "    )\n",
        "\n",
        "    # Run evaluation\n",
        "    results = evaluator.simple_evaluate(\n",
        "        model=model_wrapper,\n",
        "        tasks=tasks,\n",
        "        num_fewshot=0,\n",
        "        limit=limit,\n",
        "        device=str(DEVICE)\n",
        "    )\n",
        "\n",
        "    # Format results for clean display\n",
        "    formatted_results = {}\n",
        "    for task_name, res in results[\"results\"].items():\n",
        "        # Extract relevant metrics based on task type\n",
        "        if 'perplexity,none' in res:\n",
        "            # For perplexity tasks (wikitext)\n",
        "            formatted_results[task_name] = {\n",
        "                'perplexity': f\"{res.get('perplexity,none', 0):.2f}\",\n",
        "                'word_perplexity': f\"{res.get('word_perplexity,none', 0):.2f}\",\n",
        "                'bits_per_byte': f\"{res.get('bits_per_byte,none', 0):.4f}\"\n",
        "            }\n",
        "        elif 'acc,none' in res:\n",
        "            # For accuracy tasks (boolq, etc.)\n",
        "            formatted_results[task_name] = {\n",
        "                'accuracy': f\"{res.get('acc,none', 0):.4f}\",\n",
        "                'acc_norm': f\"{res.get('acc_norm,none', 0):.4f}\"\n",
        "            }\n",
        "        else:\n",
        "            # Fallback: store all available metrics\n",
        "            formatted_results[task_name] = {k: f\"{v:.4f}\" for k, v in res.items() if isinstance(v, (int, float))}\n",
        "\n",
        "    return formatted_results"
      ],
      "metadata": {
        "id": "372ftmpVt08W"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TASKS = [\"wikitext\", \"boolq\", \"lambada_openai\"]\n"
      ],
      "metadata": {
        "id": "HNN1IbF2l3e6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_results = model_evaluation(\n",
        "  model_obj=base_model,\n",
        "  tokenizer=tokenizer,\n",
        "  tasks=TASKS,\n",
        "  limit=None  # Full dataset\n",
        ")"
      ],
      "metadata": {
        "id": "BNwOoxgSmvcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5470802-9a74-41ad-bf64-9186513f0497",
        "id": "FAaXGou_mvcw"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boolq': {'accuracy': '0.6391', 'acc_norm': '0.0000'},\n",
              " 'lambada_openai': {'perplexity': '5.72',\n",
              "  'word_perplexity': '0.00',\n",
              "  'bits_per_byte': '0.0000'},\n",
              " 'wikitext': {'word_perplexity,none': '11.5688',\n",
              "  'byte_perplexity,none': '1.5807',\n",
              "  'bits_per_byte,none': '0.6605'}}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAW selecion model\n"
      ],
      "metadata": {
        "id": "zd05349RmS-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Pruned Model with MAW\n",
        "pruned_model_maw, stats = prune_model(\n",
        "    model=copy.deepcopy(base_model),\n",
        "    pruning_type=\"MLP_GLU\",\n",
        "    neuron_selection_method=\"MAW\",\n",
        "    pruning_percentage=PRUNING_PERCENTAGE,\n",
        "    show_progress=True,\n",
        "    return_stats=True\n",
        ")\n",
        "stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GdvV3jglOSY",
        "outputId": "7f66fa4a-bfe3-4b10-f548-b1b48551bd31"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning layers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:06<00:00,  2.59it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'original_parameters': 1235814400,\n",
              " 'pruned_parameters': 1155303424,\n",
              " 'reduction': 80510976,\n",
              " 'percentage_reduction': 6.5148112855781575,\n",
              " 'expansion_rate': 360.009765625}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAW_results = model_evaluation(\n",
        "  model_obj=pruned_model_maw,\n",
        "  tokenizer=tokenizer,\n",
        "  tasks=TASKS,\n",
        "  limit=None  # Full dataset\n",
        ")"
      ],
      "metadata": {
        "id": "bZpAut-2uw7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAW_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBc3oZm2pxR1",
        "outputId": "dca41a2f-6637-4b7d-dcca-0b216dfd9935"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boolq': {'accuracy': '0.6251', 'acc_norm': '0.0000'},\n",
              " 'lambada_openai': {'perplexity': '20.54',\n",
              "  'word_perplexity': '0.00',\n",
              "  'bits_per_byte': '0.0000'},\n",
              " 'wikitext': {'word_perplexity,none': '17.4945',\n",
              "  'byte_perplexity,none': '1.7078',\n",
              "  'bits_per_byte,none': '0.7721'}}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Delete MAW model\n",
        "del pruned_model_maw\n",
        "clear_gpu_cache()"
      ],
      "metadata": {
        "id": "LdF8nznicqbb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PON Selection Model\n"
      ],
      "metadata": {
        "id": "bExwACzx0sl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Pruned Model with PON\n",
        "pruned_model_pon, stats = prune_model(\n",
        "    model=copy.deepcopy(base_model),\n",
        "    pruning_type=\"MLP_GLU\",\n",
        "    neuron_selection_method=\"PON\",\n",
        "    pruning_percentage=PRUNING_PERCENTAGE,\n",
        "    show_progress=True,\n",
        "    return_stats=True\n",
        ")\n",
        "stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbee5c45-e22b-4a02-cb23-5647d9fecaee",
        "id": "Nu6tHMBIeiP-"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning layers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:06<00:00,  2.56it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'original_parameters': 1235814400,\n",
              " 'pruned_parameters': 1155303424,\n",
              " 'reduction': 80510976,\n",
              " 'percentage_reduction': 6.5148112855781575,\n",
              " 'expansion_rate': 360.009765625}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PON_results = model_evaluation(\n",
        "  model_obj=pruned_model_pon,\n",
        "  tokenizer=tokenizer,\n",
        "  tasks=TASKS,\n",
        "  limit=None  # Full dataset\n",
        ")"
      ],
      "metadata": {
        "id": "oqzNbvbefOQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PON_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7xvX-IpfjUO",
        "outputId": "4ede4804-723b-4efe-ab73-7cc627c77066"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boolq': {'accuracy': '0.6220', 'acc_norm': '0.0000'},\n",
              " 'lambada_openai': {'perplexity': '2032.80',\n",
              "  'word_perplexity': '0.00',\n",
              "  'bits_per_byte': '0.0000'},\n",
              " 'wikitext': {'word_perplexity,none': '72.5170',\n",
              "  'byte_perplexity,none': '2.2280',\n",
              "  'bits_per_byte,none': '1.1557'}}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del pruned_model_pon\n",
        "clear_gpu_cache()"
      ],
      "metadata": {
        "id": "aOOOzqhbiNvh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VOW Selection Model\n"
      ],
      "metadata": {
        "id": "VrqLDCm6h_0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Pruned Model with PON\n",
        "pruned_model_vow, stats = prune_model(\n",
        "    model=copy.deepcopy(base_model),\n",
        "    pruning_type=\"MLP_GLU\",\n",
        "    neuron_selection_method=\"VOW\",\n",
        "    pruning_percentage=PRUNING_PERCENTAGE,\n",
        "    show_progress=True,\n",
        "    return_stats=True\n",
        ")\n",
        "stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10164aab-cba5-4749-bfad-881656153b39",
        "id": "YV2ENt2NiR2W"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pruning layers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:05<00:00,  2.69it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'original_parameters': 1235814400,\n",
              " 'pruned_parameters': 1155303424,\n",
              " 'reduction': 80510976,\n",
              " 'percentage_reduction': 6.5148112855781575,\n",
              " 'expansion_rate': 360.009765625}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VOW_results = model_evaluation(\n",
        "  model_obj=pruned_model_vow,\n",
        "  tokenizer=tokenizer,\n",
        "  tasks=TASKS,\n",
        "  limit=None  # Full dataset\n",
        ")"
      ],
      "metadata": {
        "id": "RljqmIWHiR2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOW_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "253dd1e7-4438-49c9-b2c1-26f49ab2451f",
        "id": "yWBhFwMuiR2X"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boolq': {'accuracy': '0.6193', 'acc_norm': '0.0000'},\n",
              " 'lambada_openai': {'perplexity': '532.36',\n",
              "  'word_perplexity': '0.00',\n",
              "  'bits_per_byte': '0.0000'},\n",
              " 'wikitext': {'word_perplexity,none': '50.5592',\n",
              "  'byte_perplexity,none': '2.0827',\n",
              "  'bits_per_byte,none': '1.0584'}}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del pruned_model_vow\n",
        "clear_gpu_cache()"
      ],
      "metadata": {
        "id": "I_AFjsBziR2X"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“Š Method Comparison Results\n",
        "\n",
        "| Model | WikiText PPL â†“ | Lambada PPL â†“ | BoolQ Acc â†‘ | Status |\n",
        "|-------|----------------|---------------|-------------|--------|\n",
        "| **Base** | **11.57** | **5.72** | **0.6391** | Baseline |\n",
        "| **MAW** | **17.49** (+51%) | **20.54** (+259%) | **0.6251** (-2.2%) | âœ… **SELECTED** |\n",
        "| VOW | 50.56 (+337%) | 532.36 (+9,207%) | 0.6193 (-3.1%) | âŒ Rejected |\n",
        "| PON | 72.52 (+527%) | 2032.80 (+35,440%) | 0.6220 (-2.7%) | âŒ Rejected |\n",
        "\n",
        "**Conclusion**: MAW is the clear winner with acceptable degradation (~50% PPL increase) while VOW/PON show catastrophic failures (10,000%+ increases). MAW will be used for all experiments."
      ],
      "metadata": {
        "id": "T6Uy7OIloo6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results are conclusive. While all pruning methods introduce some performance degradation compared to the base model, the MAW (Maximum Absolute Weight) method preserves the model's capabilities far more effectively than the alternatives.\n",
        "\n",
        "MAW shows only a minor increase in perplexity and a negligible drop in accuracy, demonstrating its ability to remove parameters without causing significant damage to the model's core fluency and comprehension.\n",
        "\n",
        "VOW and PON both result in a catastrophic loss of performance, especially visible in the wikitext and lambada_openai perplexity scores. A Lambada perplexity score above 2000 for the PON method indicates that the model has become almost unusable for text generation tasks.\n",
        "\n",
        "This data provides a strong empirical justification for selecting MAW as the sole neuron selection method for all subsequent, large-scale pruning experiments in this research project."
      ],
      "metadata": {
        "id": "qbEdhlyJpe-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These experiments were powered by **OptiPFair**. If this research helps your work, consider:\n",
        "- â­ Starring [the repo](https://github.com/peremartra/optipfair)\n",
        "- ğŸ“– Reading the [documentation](https://peremartra.github.io/optipfair/)\n",
        "- ğŸ› Reporting issues or suggesting features"
      ],
      "metadata": {
        "id": "7op-1K9uIQmb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m0x5VOo9op5W"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}