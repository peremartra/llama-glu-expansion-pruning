{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/llama-glu-expansion-pruning/blob/main/notebooks/04_3B_Graphics_Carbon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IajFtfOkj35u"
      },
      "source": [
        "# GLU Pruning Research - Llama-3.2-3B Inference Analysis\n",
        "## 05 - Visualization and Analysis of Inference & Carbon Results\n",
        "\n",
        "### Exploring the \"Deployment Dilemma\": Batch Throughput vs. Interactive Latency\n",
        "by [Pere Martra](https://github.com/peremartra)\n",
        "\n",
        "[![Paper](https://img.shields.io/badge/OSF-Paper-blue?logo=osf&logoColor=white)](https://doi.org/10.31219/osf.io/qgxea)\n",
        "[![GitHub](https://img.shields.io/badge/⭐_Star-OptiPFair-orange?logo=github&logoColor=white)](https://github.com/peremartra/optipfair)\n",
        "[![PyPI](https://img.shields.io/pypi/v/optipfair?logo=python&logoColor=white&label=v)](https://pypi.org/project/optipfair/)\n",
        "\n",
        "**Repository:** [github.com/peremartra/llama-glu-expansion-pruning](https://github.com/peremartra/llama-glu-expansion-pruning)\n",
        "\n",
        "---\n",
        "\n",
        "**Colab Environment:** CPU (no GPU required for visualization)\n",
        "\n",
        "**Estimated Runtime:** ~1 minute\n",
        "## Objective\n",
        "This notebook loads the complete carbon and inference performance results for the Llama-3.2-3B model from `llama_3b_carbon_complete_results_latest.json`.\n",
        "\n",
        "The primary goal is to visualize the impact of pruning on deployment metrics. We will use the **`expansion_rate`** as the primary independent variable (X-axis) to explore our \"Deployment Dilemma\" hypothesis: that pruning creates a trade-off, dramatically improving **batch throughput** and **energy efficiency** at the cost of **interactive latency (Time To First Token)**."
      ],
      "id": "IajFtfOkj35u"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5B1IbDaj35v"
      },
      "outputs": [],
      "source": [
        "# === 1. Setup & Imports ===\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install pandas matplotlib seaborn\n",
        "\n",
        "# Import libraries\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Configure plots for better readability\n",
        "sns.set_theme(style=\"whitegrid\", palette=\"muted\")\n",
        "plt.rcParams['figure.figsize'] = (12, 7)\n",
        "plt.rcParams['axes.titlesize'] = 18\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['legend.fontsize'] = 12\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12"
      ],
      "id": "c5B1IbDaj35v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kzcui943j35v"
      },
      "outputs": [],
      "source": [
        "# === 2. Load Data ===\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Download llama_3b_carbon_complete_results_latest.json from GitHub repository\n",
        "print(\"Downloading results file...\")\n",
        "file_name = 'llama_3b_carbon_complete_results_latest.json'\n",
        "!wget -q https://raw.githubusercontent.com/peremartra/llama-glu-expansion-pruning/main/results/llama_3b_carbon_complete_results_latest.json\n",
        "\n",
        "# Verify download\n",
        "if os.path.exists(file_name):\n",
        "    print(f\"✅ {file_name} downloaded successfully\")\n",
        "else:\n",
        "    print(f\"❌ Failed to download {file_name}\")\n",
        "\n",
        "# Load the JSON data\n",
        "try:\n",
        "    with open(file_name, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    print(\"File loaded successfully into 'data' variable.\")\n",
        "    # Set data entry points\n",
        "    models_data = data.get('models_evaluated', {})\n",
        "    summary_data = data.get('summary_statistics', {})\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Could not read or parse JSON file. {e}\")\n",
        "    data = None\n",
        "    models_data = {}\n",
        "    summary_data = {}"
      ],
      "id": "Kzcui943j35v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUSCSCCTj35w"
      },
      "outputs": [],
      "source": [
        "# === 3. Data Preprocessing (Summary Data) ===\n",
        "# This DataFrame is for the main graphs, using the pre-calculated averages.\n",
        "\n",
        "# This mapping is from our first JSON file and is crucial for the X-axis\n",
        "expansion_rate_map = {\n",
        "    0: 4.0,\n",
        "    10: 3.6,\n",
        "    20: 3.2,\n",
        "    30: 2.8,\n",
        "    40: 2.4,\n",
        "    50: 2.0,\n",
        "    60: 1.6\n",
        "}\n",
        "\n",
        "summary_list = []\n",
        "\n",
        "# Add baseline data\n",
        "if 'baseline' in summary_data:\n",
        "    baseline = summary_data['baseline']\n",
        "    pruning_pct = baseline.get('pruning_pct', 0)\n",
        "    baseline_row = {\n",
        "        'model': baseline.get('model'),\n",
        "        'pruning_pct': pruning_pct,\n",
        "        'expansion_rate': expansion_rate_map.get(pruning_pct),\n",
        "        'avg_throughput_tok_s': baseline.get('avg_throughput_tok_s'),\n",
        "        'avg_joules_per_token': baseline.get('avg_joules_per_token'),\n",
        "        'avg_ttft_ms': baseline.get('avg_ttft_ms'),\n",
        "        'model_size_gb': baseline.get('model_size_gb')\n",
        "    }\n",
        "    summary_list.append(baseline_row)\n",
        "\n",
        "# Add pruned models data\n",
        "for model in summary_data.get('pruned_models', []):\n",
        "    pruning_pct = model.get('pruning_pct', 0)\n",
        "    row = {\n",
        "        'model': model.get('model'),\n",
        "        'pruning_pct': pruning_pct,\n",
        "        'expansion_rate': expansion_rate_map.get(pruning_pct),\n",
        "        'avg_throughput_tok_s': model.get('avg_throughput_tok_s'),\n",
        "        'avg_joules_per_token': model.get('avg_joules_per_token'),\n",
        "        'avg_ttft_ms': model.get('avg_ttft_ms'),\n",
        "        'model_size_gb': model.get('model_size_gb')\n",
        "    }\n",
        "    summary_list.append(row)\n",
        "\n",
        "# Create DataFrame and sort by expansion_rate (descending)\n",
        "df_summary = pd.DataFrame(summary_list)\n",
        "df_summary = df_summary.sort_values(by='expansion_rate', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"Summary DataFrame created successfully:\")\n",
        "df_summary"
      ],
      "id": "SUSCSCCTj35w"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTycyRlAj35w"
      },
      "outputs": [],
      "source": [
        "# === 4. Data Preprocessing (Detailed Benchmarks) ===\n",
        "# This DataFrame is for the detailed breakdown graphs in Section 2.\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for model_key, v in models_data.items():\n",
        "    results = v.get('results', {})\n",
        "    pruning_pct = v.get('pruning_pct', 0)\n",
        "\n",
        "    # Get metrics for a specific benchmark, handling potential missing data\n",
        "    def get_metrics(benchmark_name):\n",
        "        data = results.get(benchmark_name, {})\n",
        "        return (\n",
        "            data.get('throughput_mean'),\n",
        "            data.get('ttft_mean'),\n",
        "            data.get('joules_per_token_mean')\n",
        "        )\n",
        "\n",
        "    h_thr, h_ttf, h_j = get_metrics('hellaswag_latency_b1')\n",
        "    m_thr, m_ttf, m_j = get_metrics('mmlu_latency_b1')\n",
        "    i_thr, i_ttf, i_j = get_metrics('ifeval_latency_b1')\n",
        "\n",
        "    h_thr_b8, _, h_j_b8 = get_metrics('hellaswag_throughput_b8') # TTFT is null for b8\n",
        "    m_thr_b8, _, m_j_b8 = get_metrics('mmlu_throughput_b8')\n",
        "    i_thr_b8, _, i_j_b8 = get_metrics('ifeval_throughput_b8')\n",
        "\n",
        "    row = {\n",
        "        'model': v.get('name'),\n",
        "        'pruning_pct': pruning_pct,\n",
        "        'expansion_rate': expansion_rate_map.get(pruning_pct),\n",
        "\n",
        "        # bsz=1 (Latency)\n",
        "        'hellaswag_latency_b1_throughput': h_thr,\n",
        "        'hellaswag_latency_b1_ttft': h_ttf,\n",
        "        'hellaswag_latency_b1_joules': h_j,\n",
        "        'mmlu_latency_b1_throughput': m_thr,\n",
        "        'mmlu_latency_b1_ttft': m_ttf,\n",
        "        'mmlu_latency_b1_joules': m_j,\n",
        "        'ifeval_latency_b1_throughput': i_thr,\n",
        "        'ifeval_latency_b1_ttft': i_ttf,\n",
        "        'ifeval_latency_b1_joules': i_j,\n",
        "\n",
        "        # bsz=8 (Throughput)\n",
        "        'hellaswag_throughput_b8_throughput': h_thr_b8,\n",
        "        'hellaswag_throughput_b8_joules': h_j_b8,\n",
        "        'mmlu_throughput_b8_throughput': m_thr_b8,\n",
        "        'mmlu_throughput_b8_joules': m_j_b8,\n",
        "        'ifeval_throughput_b8_throughput': i_thr_b8,\n",
        "        'ifeval_throughput_b8_joules': i_j_b8,\n",
        "    }\n",
        "    all_results.append(row)\n",
        "\n",
        "# Create DataFrame and sort\n",
        "df_detailed = pd.DataFrame(all_results)\n",
        "df_detailed = df_detailed.sort_values(by='expansion_rate', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"Detailed DataFrame created successfully:\")\n",
        "df_detailed.head()"
      ],
      "id": "iTycyRlAj35w"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrqiLId2j35w"
      },
      "source": [
        "## Section 1: Key Hypothesis Graphs (The \"Deployment Dilemma\")\n",
        "\n",
        "This section visualizes the core trade-off. We use the `df_summary` DataFrame to plot the high-level averages.\n",
        "\n",
        "1.  **Energy:** How does pruning affect `Joules/Token`?\n",
        "2.  **Latency:** How does pruning affect `Time To First Token` (TTFT)?\n",
        "3.  **The Dilemma:** How do `Throughput` and `TTFT` behave together?\n",
        "4.  **The \"Why\":** How does `Model Size` correlate with `Throughput`?"
      ],
      "id": "VrqiLId2j35w"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNJAOr4cj35w"
      },
      "outputs": [],
      "source": [
        "# === Graph 1: The Great Win - Energy Efficiency (Joules/Token) ===\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "ax = sns.lineplot(\n",
        "    data=df_summary,\n",
        "    x='expansion_rate',\n",
        "    y='avg_joules_per_token',\n",
        "    marker='o',\n",
        "    linewidth=3,\n",
        "    color='green'\n",
        ")\n",
        "\n",
        "ax.invert_xaxis() # Ensure 4.0 (baseline) is on the left\n",
        "ax.set_title('Graph 1: Energy Efficiency Gains (Average Joules per Token)', fontsize=18, pad=20)\n",
        "ax.set_xlabel('Expansion Rate')\n",
        "ax.set_ylabel('Average Joules per Token (Lower is better)')\n",
        "\n",
        "# Annotate percentage reduction\n",
        "baseline_joules = df_summary.iloc[0]['avg_joules_per_token']\n",
        "final_joules = df_summary.iloc[-1]['avg_joules_per_token']\n",
        "reduction_pct = (1 - final_joules / baseline_joules) * 100\n",
        "ax.text(1.6, final_joules, f'  {reduction_pct:.1f}% reduction',\n",
        "        horizontalalignment='left', verticalalignment='center', fontsize=12, color='green')\n",
        "\n",
        "plt.show()"
      ],
      "id": "uNJAOr4cj35w"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jkJOnl5lnFsC"
      },
      "id": "jkJOnl5lnFsC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gZEf1g9j35w"
      },
      "outputs": [],
      "source": [
        "# === Graph 2: The Great Loss - Interactive Latency (TTFT) ===\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "ax = sns.lineplot(\n",
        "    data=df_summary,\n",
        "    x='expansion_rate',\n",
        "    y='avg_ttft_ms',\n",
        "    marker='o',\n",
        "    linewidth=3,\n",
        "    color='red'\n",
        ")\n",
        "\n",
        "ax.invert_xaxis()\n",
        "ax.set_title('Graph 2: The Latency Cost (Average Time To First Token)', fontsize=18, pad=20)\n",
        "ax.set_xlabel('Expansion Rate')\n",
        "ax.set_ylabel('Average TTFT (ms) (Lower is better)')\n",
        "\n",
        "# Annotate percentage increase\n",
        "baseline_ttft = df_summary.iloc[0]['avg_ttft_ms']\n",
        "final_ttft = df_summary.iloc[-1]['avg_ttft_ms']\n",
        "increase_pct = (final_ttft / baseline_ttft - 1) * 100\n",
        "ax.text(1.6, final_ttft, f'  {increase_pct:.1f}% worse',\n",
        "        horizontalalignment='left', verticalalignment='center', fontsize=12, color='red')\n",
        "\n",
        "plt.show()"
      ],
      "id": "5gZEf1g9j35w"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MCBRRLeSnTNI"
      },
      "id": "MCBRRLeSnTNI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VA1EHSPJj35w"
      },
      "outputs": [],
      "source": [
        "# === Graph 3: The Deployment Dilemma (Throughput vs. Latency) ===\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
        "plt.title('Graph 3: The Deployment Dilemma (Throughput vs. Latency)', fontsize=18, pad=20)\n",
        "\n",
        "# Plot Avg Throughput (Higher is better)\n",
        "color1 = 'g'\n",
        "ax1.set_xlabel('Expansion Rate')\n",
        "ax1.set_ylabel('Avg Throughput (tok/s) (Higher is better)', color=color1, fontsize=14)\n",
        "sns.lineplot(data=df_summary, x='expansion_rate', y='avg_throughput_tok_s', ax=ax1, color=color1, marker='o', linewidth=3, label='Avg Throughput')\n",
        "ax1.tick_params(axis='y', labelcolor=color1)\n",
        "ax1.invert_xaxis()\n",
        "\n",
        "# Create second Y-axis for TTFT (Lower is better)\n",
        "ax2 = ax1.twinx()\n",
        "color2 = 'r'\n",
        "ax2.set_ylabel('Avg TTFT (ms) (Lower is better)', color=color2, fontsize=14)\n",
        "sns.lineplot(data=df_summary, x='expansion_rate', y='avg_ttft_ms', ax=ax2, color=color2, marker='s', linewidth=3, label='Avg TTFT')\n",
        "ax2.tick_params(axis='y', labelcolor=color2)\n",
        "\n",
        "fig.tight_layout(pad=2.0)\n",
        "plt.show()"
      ],
      "id": "VA1EHSPJj35w"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "G8f7q4XvqAIx"
      },
      "id": "G8f7q4XvqAIx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Wa97b22j35w"
      },
      "outputs": [],
      "source": [
        "# === Graph 4: The \"Why\" - Model Size vs. Throughput ===\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
        "plt.title('Graph 4: Model Size Reduction vs. Avg Throughput', fontsize=18, pad=20)\n",
        "\n",
        "# Plot Model Size (Lower is better)\n",
        "color1 = 'blue'\n",
        "ax1.set_xlabel('Expansion Rate')\n",
        "ax1.set_ylabel('Model Size (GB) (Lower is better)', color=color1, fontsize=14)\n",
        "sns.lineplot(data=df_summary, x='expansion_rate', y='model_size_gb', ax=ax1, color=color1, marker='o', linewidth=3, label='Model Size')\n",
        "ax1.tick_params(axis='y', labelcolor=color1)\n",
        "ax1.invert_xaxis()\n",
        "\n",
        "# Create second Y-axis for Throughput (Higher is better)\n",
        "ax2 = ax1.twinx()\n",
        "color2 = 'green'\n",
        "ax2.set_ylabel('Avg Throughput (tok/s) (Higher is better)', color=color2, fontsize=14)\n",
        "sns.lineplot(data=df_summary, x='expansion_rate', y='avg_throughput_tok_s', ax=ax2, color=color2, marker='s', linewidth=3, label='Avg Throughput')\n",
        "ax2.tick_params(axis='y', labelcolor=color2)\n",
        "\n",
        "fig.tight_layout(pad=2.0)\n",
        "plt.show()"
      ],
      "id": "5Wa97b22j35w"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8iv8Yt8uoCYj"
      },
      "id": "8iv8Yt8uoCYj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w562JgP5j35x"
      },
      "source": [
        "## Section 2: Exploratory Graphs (Detailed Benchmark Breakdown)\n",
        "\n",
        "This section uses the `df_detailed` DataFrame to explore performance on a per-task basis. This helps us confirm if the trends seen in the averages hold true for short, medium, and long tasks."
      ],
      "id": "w562JgP5j35x"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5FqCgMEj35x"
      },
      "outputs": [],
      "source": [
        "# === Graph 5: Batch Throughput (bsz=8) Breakdown ===\n",
        "# Purpose: Check if batch throughput gains are consistent across tasks.\n",
        "\n",
        "df_melted_b8_thr = df_detailed.melt(\n",
        "    id_vars=['expansion_rate'],\n",
        "    value_vars=[\n",
        "        'hellaswag_throughput_b8_throughput',\n",
        "        'mmlu_throughput_b8_throughput',\n",
        "        'ifeval_throughput_b8_throughput'\n",
        "    ],\n",
        "    var_name='Benchmark',\n",
        "    value_name='Throughput (tok/s)'\n",
        ")\n",
        "# Clean up names\n",
        "df_melted_b8_thr['Task'] = df_melted_b8_thr['Benchmark'].str.split('_', expand=True)[0]\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "ax = sns.lineplot(data=df_melted_b8_thr, x='expansion_rate', y='Throughput (tok/s)', hue='Task', marker='o', linewidth=2.5)\n",
        "ax.invert_xaxis()\n",
        "ax.set_title('Graph 5: Batch Throughput (bsz=8) Breakdown by Task', fontsize=18, pad=20)\n",
        "ax.set_xlabel('Expansion Rate')\n",
        "ax.set_ylabel('Throughput (tok/s) (Higher is better)')\n",
        "plt.show()"
      ],
      "id": "e5FqCgMEj35x"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d4s1E4MYu_oS"
      },
      "id": "d4s1E4MYu_oS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZ7qZxoOj35x"
      },
      "outputs": [],
      "source": [
        "# === Graph 6: Time To First Token (bsz=1) Breakdown ===\n",
        "# Purpose: Check if the TTFT cost is consistent across tasks.\n",
        "\n",
        "df_melted_b1_ttft = df_detailed.melt(\n",
        "    id_vars=['expansion_rate'],\n",
        "    value_vars=[\n",
        "        'hellaswag_latency_b1_ttft',\n",
        "        'mmlu_latency_b1_ttft',\n",
        "        'ifeval_latency_b1_ttft'\n",
        "    ],\n",
        "    var_name='Benchmark',\n",
        "    value_name='TTFT (ms)'\n",
        ")\n",
        "# Clean up names\n",
        "df_melted_b1_ttft['Task'] = df_melted_b1_ttft['Benchmark'].str.split('_', expand=True)[0]\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "ax = sns.lineplot(data=df_melted_b1_ttft, x='expansion_rate', y='TTFT (ms)', hue='Task', marker='o', linewidth=2.5)\n",
        "ax.invert_xaxis()\n",
        "ax.set_title('Graph 6: Time To First Token (bsz=1) Breakdown by Task', fontsize=18, pad=20)\n",
        "ax.set_xlabel('Expansion Rate')\n",
        "ax.set_ylabel('TTFT (ms) (Lower is better)')\n",
        "plt.show()"
      ],
      "id": "oZ7qZxoOj35x"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WV1Ykt3Mw8Xm"
      },
      "id": "WV1Ykt3Mw8Xm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlHgTwtkj35x"
      },
      "outputs": [],
      "source": [
        "# === Graph 7: Joules per Token Breakdown (bsz=1 vs bsz=8) ===\n",
        "# Purpose: Confirm efficiency gains across all tasks and batch sizes.\n",
        "\n",
        "joules_cols = [col for col in df_detailed.columns if 'joules' in col]\n",
        "df_melted_joules = df_detailed.melt(\n",
        "    id_vars=['expansion_rate'],\n",
        "    value_vars=joules_cols,\n",
        "    var_name='Benchmark',\n",
        "    value_name='Joules/Token'\n",
        ")\n",
        "\n",
        "# Create new columns for plotting\n",
        "df_melted_joules['Batch Size'] = df_melted_joules['Benchmark'].apply(lambda x: 'bsz=1' if 'latency' in x else 'bsz=8')\n",
        "df_melted_joules['Task'] = df_melted_joules['Benchmark'].str.split('_', expand=True)[0]\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "ax = sns.lineplot(\n",
        "    data=df_melted_joules,\n",
        "    x='expansion_rate',\n",
        "    y='Joules/Token',\n",
        "    hue='Task',\n",
        "    style='Batch Size',\n",
        "    markers=True,\n",
        "    linewidth=2.5,\n",
        "    markersize=8\n",
        ")\n",
        "\n",
        "ax.set_yscale('log') # Use log scale to see both bsz=1 and bsz=8 clearly\n",
        "ax.invert_xaxis()\n",
        "ax.set_title('Graph 7: Joules per Token Breakdown (Log Scale)', fontsize=18, pad=20)\n",
        "ax.set_xlabel('Expansion Rate')\n",
        "ax.set_ylabel('Joules/Token (Lower is better) [Log Scale]')\n",
        "plt.legend(title='Task & Batch Size', bbox_to_anchor=(1.05, 1), loc=2)\n",
        "plt.show()"
      ],
      "id": "YlHgTwtkj35x"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "igsJwAnX4se9"
      },
      "id": "igsJwAnX4se9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQDFh-fSj35x"
      },
      "outputs": [],
      "source": [
        "# === Graph 8: Single-Batch Throughput (bsz=1) \"Null Result\" ===\n",
        "# Purpose: Show that single-token generation speed is not the bottleneck or the benefit.\n",
        "\n",
        "df_melted_b1_thr = df_detailed.melt(\n",
        "    id_vars=['expansion_rate'],\n",
        "    value_vars=[\n",
        "        'hellaswag_latency_b1_throughput',\n",
        "        'mmlu_latency_b1_throughput',\n",
        "        'ifeval_latency_b1_throughput'\n",
        "    ],\n",
        "    var_name='Benchmark',\n",
        "    value_name='Throughput (tok/s)'\n",
        ")\n",
        "# Clean up names\n",
        "df_melted_b1_thr['Task'] = df_melted_b1_thr['Benchmark'].str.split('_', expand=True)[0]\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "ax = sns.lineplot(data=df_melted_b1_thr, x='expansion_rate', y='Throughput (tok/s)', hue='Task', marker='o', linewidth=2.5)\n",
        "ax.invert_xaxis()\n",
        "ax.set_title('Graph 8: Single-Batch Throughput (bsz=1) - \"The Null Result\"', fontsize=18, pad=20)\n",
        "ax.set_xlabel('Expansion Rate')\n",
        "ax.set_ylabel('Throughput (tok/s)')\n",
        "\n",
        "# Set Y-axis limit to highlight the flatness\n",
        "ax.set_ylim(48, 53)\n",
        "ax.text(3.8, 48.5, 'Y-axis zoomed (48-53) to show minor variance', fontsize=12, color='gray')\n",
        "\n",
        "plt.show()"
      ],
      "id": "kQDFh-fSj35x"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bMwybvUG6hD2"
      },
      "id": "bMwybvUG6hD2"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hE_sVfNl7CwZ"
      },
      "id": "hE_sVfNl7CwZ"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DJ_r9hP27rzC"
      },
      "id": "DJ_r9hP27rzC",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}