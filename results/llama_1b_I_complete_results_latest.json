{
  "experiment_metadata": {
    "timestamp": "2025-11-02T11:43:09.115480",
    "notebook": "03_Evaluate_1B_Instruct.ipynb",
    "model_family": "Llama-3.2-1B-Instruct",
    "pruning_method": "MAW (Maximum Absolute Weight)",
    "hardware": {
      "device": "NVIDIA L4",
      "gpu_memory_gb": 23.795204096
    }
  },
  "benchmarks": [
    {
      "name": "leaderboard_musr",
      "num_fewshot": 0
    },
    {
      "name": "truthfulqa_mc1",
      "num_fewshot": 0
    },
    {
      "name": "truthfulqa_mc2",
      "num_fewshot": 0
    },
    {
      "name": "lambada_openai",
      "num_fewshot": 0
    },
    {
      "name": "mmlu",
      "num_fewshot": 5
    },
    {
      "name": "gsm8k",
      "num_fewshot": 5
    },
    {
      "name": "ifeval",
      "num_fewshot": 0
    }
  ],
  "models_evaluated": {
    "pruned_10pct": {
      "name": "Llama-3.2-1B-I-pruned-10%",
      "pruning_pct": 10,
      "expansion_rate": null,
      "is_star": false,
      "hf_repo": "peremartra/Llama-3.2-1B-I-pruned-10pct",
      "results": {
        "gsm8k": {
          "exact_match,strict-match": 0.1926,
          "exact_match_stderr,strict-match": 0.0109,
          "exact_match,flexible-extract": 0.1713,
          "exact_match_stderr,flexible-extract": 0.0104
        },
        "ifeval": {
          "prompt_level_strict_acc,none": 0.2218,
          "prompt_level_strict_acc_stderr,none": 0.0179,
          "inst_level_strict_acc,none": 0.3525,
          "prompt_level_loose_acc,none": 0.2514,
          "prompt_level_loose_acc_stderr,none": 0.0187,
          "inst_level_loose_acc,none": 0.3825
        },
        "lambada_openai": {
          "perplexity": 14.11,
          "word_perplexity": 0.0,
          "bits_per_byte": 0.0
        },
        "leaderboard_musr": {
          "acc_norm,none": 0.3836,
          "acc_norm_stderr,none": 0.0174
        },
        "mmlu": {
          "accuracy": 0.4027,
          "acc_norm": "N/A"
        },
        "truthfulqa_mc1": {
          "accuracy": 0.2632,
          "acc_norm": "N/A"
        },
        "truthfulqa_mc2": {
          "accuracy": 0.4423,
          "acc_norm": "N/A"
        }
      }
    },
    "pruned_40pct": {
      "name": "Llama-3.2-1B-I-pruned-40%",
      "pruning_pct": 40,
      "expansion_rate": null,
      "is_star": true,
      "hf_repo": "peremartra/Llama-3.2-1B-I-pruned-40pct",
      "results": {
        "gsm8k": {
          "exact_match,strict-match": 0.0159,
          "exact_match_stderr,strict-match": 0.0034,
          "exact_match,flexible-extract": 0.0167,
          "exact_match_stderr,flexible-extract": 0.0035
        },
        "ifeval": {
          "prompt_level_strict_acc,none": 0.146,
          "prompt_level_strict_acc_stderr,none": 0.0152,
          "inst_level_strict_acc,none": 0.259,
          "prompt_level_loose_acc,none": 0.1534,
          "prompt_level_loose_acc_stderr,none": 0.0155,
          "inst_level_loose_acc,none": 0.271
        },
        "lambada_openai": {
          "perplexity": 93.47,
          "word_perplexity": 0.0,
          "bits_per_byte": 0.0
        },
        "leaderboard_musr": {
          "acc_norm,none": 0.3796,
          "acc_norm_stderr,none": 0.0173
        },
        "mmlu": {
          "accuracy": 0.2614,
          "acc_norm": "N/A"
        },
        "truthfulqa_mc1": {
          "accuracy": 0.2436,
          "acc_norm": "N/A"
        },
        "truthfulqa_mc2": {
          "accuracy": 0.437,
          "acc_norm": "N/A"
        }
      }
    },
    "pruned_60pct": {
      "name": "Llama-3.2-1B-I-pruned-60%",
      "pruning_pct": 60,
      "expansion_rate": null,
      "is_star": false,
      "hf_repo": "peremartra/Llama-3.2-1B-I-pruned-60pct",
      "results": {
        "gsm8k": {
          "exact_match,strict-match": 0.0076,
          "exact_match_stderr,strict-match": 0.0024,
          "exact_match,flexible-extract": 0.019,
          "exact_match_stderr,flexible-extract": 0.0038
        },
        "ifeval": {
          "prompt_level_strict_acc,none": 0.1368,
          "prompt_level_strict_acc_stderr,none": 0.0148,
          "inst_level_strict_acc,none": 0.2506,
          "prompt_level_loose_acc,none": 0.1423,
          "prompt_level_loose_acc_stderr,none": 0.015,
          "inst_level_loose_acc,none": 0.2638
        },
        "lambada_openai": {
          "perplexity": 1317.59,
          "word_perplexity": 0.0,
          "bits_per_byte": 0.0
        },
        "leaderboard_musr": {
          "acc_norm,none": 0.3466,
          "acc_norm_stderr,none": 0.0168
        },
        "mmlu": {
          "accuracy": 0.246,
          "acc_norm": "N/A"
        },
        "truthfulqa_mc1": {
          "accuracy": 0.2289,
          "acc_norm": "N/A"
        },
        "truthfulqa_mc2": {
          "accuracy": 0.444,
          "acc_norm": "N/A"
        }
      }
    },
    "baseline": {
      "name": "Llama-3.2-1B-Instruct",
      "pruning_pct": 0,
      "expansion_rate": 300,
      "is_star": false,
      "hf_repo": "meta-llama/Llama-3.2-1B-Instruct",
      "results": {
        "gsm8k": {
          "exact_match,strict-match": 0.3389,
          "exact_match_stderr,strict-match": 0.013,
          "exact_match,flexible-extract": 0.3389,
          "exact_match_stderr,flexible-extract": 0.013
        },
        "ifeval": {
          "prompt_level_strict_acc,none": 0.3641,
          "prompt_level_strict_acc_stderr,none": 0.0207,
          "inst_level_strict_acc,none": 0.5108,
          "prompt_level_loose_acc,none": 0.4159,
          "prompt_level_loose_acc_stderr,none": 0.0212,
          "inst_level_loose_acc,none": 0.56
        },
        "lambada_openai": {
          "perplexity": 6.58,
          "word_perplexity": 0.0,
          "bits_per_byte": 0.0
        },
        "leaderboard_musr": {
          "acc_norm,none": 0.3426,
          "acc_norm_stderr,none": 0.0168
        },
        "mmlu": {
          "accuracy": 0.4557,
          "acc_norm": "N/A"
        },
        "truthfulqa_mc1": {
          "accuracy": 0.2729,
          "acc_norm": "N/A"
        },
        "truthfulqa_mc2": {
          "accuracy": 0.4383,
          "acc_norm": "N/A"
        }
      }
    }
  },
  "summary_statistics": {
    "baseline": {
      "avg_accuracy": 0.3889666666666667,
      "avg_perplexity": 6.58
    },
    "pruned_models": [
      {
        "pruning_pct": 10,
        "is_star": false,
        "avg_accuracy": 0.3694,
        "avg_perplexity": 14.11,
        "accuracy_degradation_pct": -5.0304224869311875,
        "perplexity_degradation_pct": 114.43768996960486
      },
      {
        "pruning_pct": 40,
        "is_star": true,
        "avg_accuracy": 0.314,
        "avg_perplexity": 93.47,
        "accuracy_degradation_pct": -19.27328819950296,
        "perplexity_degradation_pct": 1320.516717325228
      },
      {
        "pruning_pct": 60,
        "is_star": false,
        "avg_accuracy": 0.3063,
        "avg_perplexity": 1317.59,
        "accuracy_degradation_pct": -21.25289227868712,
        "perplexity_degradation_pct": 19924.164133738603
      }
    ]
  },
  "upload_decisions": [
    {
      "model": "Llama-3.2-1B-I-pruned-10%",
      "pruning": 10,
      "star": "",
      "outperform_count": 1,
      "outperforming_tasks": [
        "truthfulqa_mc2"
      ],
      "upload": false,
      "reason": "Only 1 tasks won (truthfulqa_mc2)"
    },
    {
      "model": "Llama-3.2-1B-I-pruned-40%",
      "pruning": 40,
      "star": "‚≠ê",
      "outperform_count": 0,
      "outperforming_tasks": [],
      "upload": true,
      "reason": "Star model. Won/Tied: None"
    },
    {
      "model": "Llama-3.2-1B-I-pruned-60%",
      "pruning": 60,
      "star": "",
      "outperform_count": 1,
      "outperforming_tasks": [
        "truthfulqa_mc2"
      ],
      "upload": false,
      "reason": "Only 1 tasks won (truthfulqa_mc2)"
    }
  ],
  "citation": {
    "paper": "Exploring GLU Expansion Ratios: Structured Pruning in Llama-3.2 Models",
    "author": "Pere Martra",
    "doi": "https://doi.org/10.31219/osf.io/qgxea",
    "github": "https://github.com/peremartra/llama-glu-expansion-pruning",
    "note": "Results are freely available for research purposes. Please cite the paper if you use this data."
  }
}