{
  "metadata": {
    "model_name": "Llama-3.2-1B-baseline-run2",
    "started_at": "2025-11-07T12:08:01.512690",
    "last_updated": "2025-11-07T12:11:01.570268",
    "mode": "carbon_profiling"
  },
  "results": {
    "hellaswag_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 96.50264539216694,
      "std_ttft_ms": 124.53280266449488,
      "throughput_tokens_per_sec": 49.1941791000682,
      "total_loop_time_sec": 9.504374265670776,
      "total_new_tokens": 451,
      "avg_new_tokens_per_prompt": 4.747368421052632,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 2.3098196983337402,
      "memory_reserved_gb": 2.365234375,
      "max_memory_allocated_gb": 2.5331315994262695,
      "model_size_gb": 2.3018836975097656,
      "energy_kwh": 3.693248618447246e-05,
      "energy_raw_kwh": 0.00012695677392054056,
      "energy_idle_kwh": 9.00242877360681e-05,
      "joules_per_token": 0.294804767769625,
      "joules_per_token_calculation": {
        "total_joules": 132.95695026410087,
        "total_tokens": 451,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:08:13.415339",
        "project_name": "N/A",
        "duration_sec": 10.271127462387085,
        "energy_kwh": 3.693248618447246e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.3098196983337402,
        "gpu_memory_reserved_gb": 2.365234375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00012695677392054056,
        "energy_idle_contribution_kwh": 9.00242877360681e-05,
        "energy_net_kwh": 3.693248618447246e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.00036353735983094747,
        "codecarbon_cpu_energy_kwh": 0.00012086122245139099,
        "codecarbon_gpu_energy_kwh": 0.00018582181532400023
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Latency, TTFT, bsz=1)"
    },
    "mmlu_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 787.6487204903051,
      "std_ttft_ms": 365.05504109765235,
      "throughput_tokens_per_sec": 50.690510567477176,
      "total_loop_time_sec": 78.91005253791809,
      "total_new_tokens": 3793,
      "avg_new_tokens_per_prompt": 39.92631578947368,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 2.3098196983337402,
      "memory_reserved_gb": 2.392578125,
      "max_memory_allocated_gb": 2.5331315994262695,
      "model_size_gb": 2.3018836975097656,
      "energy_kwh": 0.0002974393635191729,
      "energy_raw_kwh": 0.0009966499168089595,
      "energy_idle_kwh": 0.0006992105532897866,
      "joules_per_token": 0.2823046951407916,
      "joules_per_token_calculation": {
        "total_joules": 1070.7817086690225,
        "total_tokens": 3793,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:09:34.833010",
        "project_name": "N/A",
        "duration_sec": 79.7749240398407,
        "energy_kwh": 0.0002974393635191729,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.3098196983337402,
        "gpu_memory_reserved_gb": 2.392578125,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0009966499168089595,
        "energy_idle_contribution_kwh": 0.0006992105532897866,
        "energy_net_kwh": 0.0002974393635191729,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0028538806417626075,
        "codecarbon_cpu_energy_kwh": 0.0009395693542319415,
        "codecarbon_gpu_energy_kwh": 0.0014722825667139996
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Latency, TTFT, bsz=1)"
    },
    "ifeval_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 1111.840763092041,
      "std_ttft_ms": 1372.4281958672364,
      "throughput_tokens_per_sec": 51.158404951637245,
      "total_loop_time_sec": 35.22475504875183,
      "total_new_tokens": 1422,
      "avg_new_tokens_per_prompt": 56.88,
      "num_measured_prompts": 25,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 2.3098196983337402,
      "memory_reserved_gb": 2.33984375,
      "max_memory_allocated_gb": 2.5331315994262695,
      "model_size_gb": 2.3018836975097656,
      "energy_kwh": 0.0001356707911498266,
      "energy_raw_kwh": 0.00045365926550319796,
      "energy_idle_kwh": 0.00031798847435337137,
      "joules_per_token": 0.34347035734133313,
      "joules_per_token_calculation": {
        "total_joules": 488.41484813937575,
        "total_tokens": 1422,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:10:12.755607",
        "project_name": "N/A",
        "duration_sec": 36.280210971832275,
        "energy_kwh": 0.0001356707911498266,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.3098196983337402,
        "gpu_memory_reserved_gb": 2.33984375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 77,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00045365926550319796,
        "energy_idle_contribution_kwh": 0.00031798847435337137,
        "energy_net_kwh": 0.0001356707911498266,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0012990412921731966,
        "codecarbon_cpu_energy_kwh": 0.0004273091124909737,
        "codecarbon_gpu_energy_kwh": 0.0006707213699100002
      },
      "batch_size": 1,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Latency, TTFT, bsz=1)"
    },
    "hellaswag_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 485.2900288321755,
      "throughput_tokens_per_sec": 248.7734458871872,
      "total_loop_time_sec": 6.593410968780518,
      "total_new_tokens": 1328,
      "avg_new_tokens_per_prompt": 15.090909090909092,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 2.3098196983337402,
      "memory_reserved_gb": 2.77734375,
      "max_memory_allocated_gb": 2.5331315994262695,
      "model_size_gb": 2.3018836975097656,
      "energy_kwh": 2.4253393797688807e-05,
      "energy_raw_kwh": 9.016059045686544e-05,
      "energy_idle_kwh": 6.590719665917663e-05,
      "joules_per_token": 0.0657471518612046,
      "joules_per_token_calculation": {
        "total_joules": 87.3122176716797,
        "total_tokens": 1328,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:10:21.914902",
        "project_name": "N/A",
        "duration_sec": 7.519539833068848,
        "energy_kwh": 2.4253393797688807e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.3098196983337402,
        "gpu_memory_reserved_gb": 2.77734375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 75,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 9.016059045686544e-05,
        "energy_idle_contribution_kwh": 6.590719665917663e-05,
        "energy_net_kwh": 2.4253393797688807e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.00025817246298336484,
        "codecarbon_cpu_energy_kwh": 8.856109758403118e-05,
        "codecarbon_gpu_energy_kwh": 0.00012795760236599882
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Throughput, bsz=8)"
    },
    "mmlu_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 1420.6634651530874,
      "throughput_tokens_per_sec": 268.7604708529884,
      "total_loop_time_sec": 18.593250036239624,
      "total_new_tokens": 4200,
      "avg_new_tokens_per_prompt": 47.72727272727273,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 2.3098196983337402,
      "memory_reserved_gb": 3.06640625,
      "max_memory_allocated_gb": 2.6879000663757324,
      "model_size_gb": 2.3018836975097656,
      "energy_kwh": 6.550416647142676e-05,
      "energy_raw_kwh": 0.00023584906179079855,
      "energy_idle_kwh": 0.0001703448953193718,
      "joules_per_token": 0.05614642840408008,
      "joules_per_token_calculation": {
        "total_joules": 235.81499929713632,
        "total_tokens": 4200,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:10:43.013749",
        "project_name": "N/A",
        "duration_sec": 19.435134410858154,
        "energy_kwh": 6.550416647142676e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.3098196983337402,
        "gpu_memory_reserved_gb": 3.06640625,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00023584906179079855,
        "energy_idle_contribution_kwh": 0.0001703448953193718,
        "energy_net_kwh": 6.550416647142676e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0006753475422721093,
        "codecarbon_cpu_energy_kwh": 0.00022882789050277677,
        "codecarbon_gpu_energy_kwh": 0.0003388855488859997
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Throughput, bsz=8)"
    },
    "ifeval_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 3999.852101008097,
      "throughput_tokens_per_sec": 275.0101684316685,
      "total_loop_time_sec": 16.016680240631104,
      "total_new_tokens": 3300,
      "avg_new_tokens_per_prompt": 137.5,
      "num_measured_prompts": 24,
      "num_warmup_batches": 1,
      "memory_allocated_gb": 2.3098196983337402,
      "memory_reserved_gb": 2.4453125,
      "max_memory_allocated_gb": 2.6879000663757324,
      "model_size_gb": 2.3018836975097656,
      "energy_kwh": 5.202262399463489e-05,
      "energy_raw_kwh": 0.00020010030667671236,
      "energy_idle_kwh": 0.00014807768268207747,
      "joules_per_token": 0.05675195344869261,
      "joules_per_token_calculation": {
        "total_joules": 187.28144638068562,
        "total_tokens": 3300,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:11:01.568681",
        "project_name": "N/A",
        "duration_sec": 16.894604682922363,
        "energy_kwh": 5.202262399463489e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.3098196983337402,
        "gpu_memory_reserved_gb": 2.4453125,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00020010030667671236,
        "energy_idle_contribution_kwh": 0.00014807768268207747,
        "energy_net_kwh": 5.202262399463489e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0005729819287637519,
        "codecarbon_cpu_energy_kwh": 0.0001989270702215292,
        "codecarbon_gpu_energy_kwh": 0.00028046966882000045
      },
      "batch_size": 8,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Throughput, bsz=8)"
    }
  },
  "completed_workloads": [
    "hellaswag_latency_b1",
    "mmlu_latency_b1",
    "ifeval_latency_b1",
    "hellaswag_throughput_b8",
    "mmlu_throughput_b8",
    "ifeval_throughput_b8"
  ],
  "failed_workloads": []
}