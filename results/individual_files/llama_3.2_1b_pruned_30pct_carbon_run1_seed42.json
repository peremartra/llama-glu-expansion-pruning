{
  "metadata": {
    "model_name": "Llama-3.2-1B-pruned-30%-run1",
    "started_at": "2025-11-07T12:37:16.133799",
    "last_updated": "2025-11-07T12:41:44.979042",
    "mode": "carbon_profiling"
  },
  "results": {
    "hellaswag_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 385.47943516781453,
      "std_ttft_ms": 50.99040643718942,
      "throughput_tokens_per_sec": 50.900387521417635,
      "total_loop_time_sec": 38.74768114089966,
      "total_new_tokens": 1864,
      "avg_new_tokens_per_prompt": 19.621052631578948,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.8609061241149902,
      "memory_reserved_gb": 2.505859375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.8519935607910156,
      "energy_kwh": 0.00013132171313638116,
      "energy_raw_kwh": 0.0004776943463235092,
      "energy_idle_kwh": 0.000346372633187128,
      "joules_per_token": 0.2536256262290623,
      "joules_per_token_calculation": {
        "total_joules": 472.7581672909722,
        "total_tokens": 1864,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:37:57.362827",
        "project_name": "N/A",
        "duration_sec": 39.51864051818848,
        "energy_kwh": 0.00013132171313638116,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.8609061241149902,
        "gpu_memory_reserved_gb": 2.505859375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0004776943463235092,
        "energy_idle_contribution_kwh": 0.000346372633187128,
        "energy_net_kwh": 0.00013132171313638116,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0013678651095632643,
        "codecarbon_cpu_energy_kwh": 0.0004653596886312595,
        "codecarbon_gpu_energy_kwh": 0.0006835822135319997
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Latency, TTFT, bsz=1)"
    },
    "mmlu_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 891.5272060193514,
      "std_ttft_ms": 281.29085186354223,
      "throughput_tokens_per_sec": 50.87662432576126,
      "total_loop_time_sec": 89.69742059707642,
      "total_new_tokens": 4309,
      "avg_new_tokens_per_prompt": 45.357894736842105,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.8609061241149902,
      "memory_reserved_gb": 2.50390625,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.8519935607910156,
      "energy_kwh": 0.00029713674379270997,
      "energy_raw_kwh": 0.0010923311293654131,
      "energy_idle_kwh": 0.0007951943855727031,
      "joules_per_token": 0.24824606118676162,
      "joules_per_token_calculation": {
        "total_joules": 1069.6922776537558,
        "total_tokens": 4309,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:39:29.749470",
        "project_name": "N/A",
        "duration_sec": 90.72599291801453,
        "energy_kwh": 0.00029713674379270997,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.8609061241149902,
        "gpu_memory_reserved_gb": 2.50390625,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0010923311293654131,
        "energy_idle_contribution_kwh": 0.0007951943855727031,
        "energy_net_kwh": 0.00029713674379270997,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0031278612599214086,
        "codecarbon_cpu_energy_kwh": 0.001068586957501414,
        "codecarbon_gpu_energy_kwh": 0.0015565609674699984
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Latency, TTFT, bsz=1)"
    },
    "ifeval_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 2843.421106338501,
      "std_ttft_ms": 309.65003517309384,
      "throughput_tokens_per_sec": 51.613881486933245,
      "total_loop_time_sec": 82.87023138999939,
      "total_new_tokens": 3669,
      "avg_new_tokens_per_prompt": 146.76,
      "num_measured_prompts": 25,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.8609061241149902,
      "memory_reserved_gb": 2.5,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.8519935607910156,
      "energy_kwh": 0.00027461369055708346,
      "energy_raw_kwh": 0.0010087121858481545,
      "energy_idle_kwh": 0.000734098495291071,
      "joules_per_token": 0.26944924666271475,
      "joules_per_token_calculation": {
        "total_joules": 988.6092860055005,
        "total_tokens": 3669,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:40:55.168703",
        "project_name": "N/A",
        "duration_sec": 83.75538873672485,
        "energy_kwh": 0.00027461369055708346,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.8609061241149902,
        "gpu_memory_reserved_gb": 2.5,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0010087121858481545,
        "energy_idle_contribution_kwh": 0.000734098495291071,
        "energy_net_kwh": 0.00027461369055708346,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.002888420629702314,
        "codecarbon_cpu_energy_kwh": 0.000986529855574968,
        "codecarbon_gpu_energy_kwh": 0.0014377783724440069
      },
      "batch_size": 1,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Latency, TTFT, bsz=1)"
    },
    "hellaswag_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 615.5552647330544,
      "throughput_tokens_per_sec": 248.11301515470817,
      "total_loop_time_sec": 8.0671226978302,
      "total_new_tokens": 1680,
      "avg_new_tokens_per_prompt": 19.09090909090909,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 1.8609061241149902,
      "memory_reserved_gb": 2.525390625,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.8519935607910156,
      "energy_kwh": 3.240184845783138e-05,
      "energy_raw_kwh": 0.00011001678514813071,
      "energy_idle_kwh": 7.761493669029933e-05,
      "joules_per_token": 0.06943253240963868,
      "joules_per_token_calculation": {
        "total_joules": 116.64665444819298,
        "total_tokens": 1680,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:41:05.679764",
        "project_name": "N/A",
        "duration_sec": 8.855309247970581,
        "energy_kwh": 3.240184845783138e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.8609061241149902,
        "gpu_memory_reserved_gb": 2.525390625,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00011001678514813071,
        "energy_idle_contribution_kwh": 7.761493669029933e-05,
        "energy_net_kwh": 3.240184845783138e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.00031503015061545397,
        "codecarbon_cpu_energy_kwh": 0.00010431841839722721,
        "codecarbon_gpu_energy_kwh": 0.00016164790709600335
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Throughput, bsz=8)"
    },
    "mmlu_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 1377.8109333731911,
      "throughput_tokens_per_sec": 277.1194309537122,
      "total_loop_time_sec": 18.022979974746704,
      "total_new_tokens": 4200,
      "avg_new_tokens_per_prompt": 47.72727272727273,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 1.8609061241149902,
      "memory_reserved_gb": 2.525390625,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.8519935607910156,
      "energy_kwh": 6.850609219204642e-05,
      "energy_raw_kwh": 0.00023413166924661494,
      "energy_idle_kwh": 0.00016562557705456852,
      "joules_per_token": 0.058719507593182646,
      "joules_per_token_calculation": {
        "total_joules": 246.62193189136713,
        "total_tokens": 4200,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:41:26.230464",
        "project_name": "N/A",
        "duration_sec": 18.89669394493103,
        "energy_kwh": 6.850609219204642e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.8609061241149902,
        "gpu_memory_reserved_gb": 2.525390625,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00023413166924661494,
        "energy_idle_contribution_kwh": 0.00016562557705456852,
        "energy_net_kwh": 6.850609219204642e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0006704298342048216,
        "codecarbon_cpu_energy_kwh": 0.0002225946877979311,
        "codecarbon_gpu_energy_kwh": 0.00034312694116799686
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Throughput, bsz=8)"
    },
    "ifeval_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 4011.392037073771,
      "throughput_tokens_per_sec": 274.2190216846588,
      "total_loop_time_sec": 16.07320547103882,
      "total_new_tokens": 3300,
      "avg_new_tokens_per_prompt": 137.5,
      "num_measured_prompts": 24,
      "num_warmup_batches": 1,
      "memory_allocated_gb": 1.8609061241149902,
      "memory_reserved_gb": 2.529296875,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.8519935607910156,
      "energy_kwh": 5.792449306683725e-05,
      "energy_raw_kwh": 0.00020760718860298756,
      "energy_idle_kwh": 0.00014968269553615032,
      "joules_per_token": 0.06319035607291336,
      "joules_per_token_calculation": {
        "total_joules": 208.5281750406141,
        "total_tokens": 3300,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:41:44.977329",
        "project_name": "N/A",
        "duration_sec": 17.077725172042847,
        "energy_kwh": 5.792449306683725e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.8609061241149902,
        "gpu_memory_reserved_gb": 2.529296875,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 77,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00020760718860298756,
        "energy_idle_contribution_kwh": 0.00014968269553615032,
        "energy_net_kwh": 5.792449306683725e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0005944776863493123,
        "codecarbon_cpu_energy_kwh": 0.00020106119591597687,
        "codecarbon_gpu_energy_kwh": 0.0002988405168500019
      },
      "batch_size": 8,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Throughput, bsz=8)"
    }
  },
  "completed_workloads": [
    "hellaswag_latency_b1",
    "mmlu_latency_b1",
    "ifeval_latency_b1",
    "hellaswag_throughput_b8",
    "mmlu_throughput_b8",
    "ifeval_throughput_b8"
  ],
  "failed_workloads": []
}