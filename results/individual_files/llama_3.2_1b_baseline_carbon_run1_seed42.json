{
  "metadata": {
    "model_name": "Llama-3.2-1B-baseline-run1",
    "started_at": "2025-11-07T12:04:36.176749",
    "last_updated": "2025-11-07T12:07:59.375956",
    "mode": "carbon_profiling"
  },
  "results": {
    "hellaswag_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 100.12730548256322,
      "std_ttft_ms": 126.48919670514188,
      "throughput_tokens_per_sec": 49.936428189119006,
      "total_loop_time_sec": 10.228797674179077,
      "total_new_tokens": 475,
      "avg_new_tokens_per_prompt": 5.0,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 2.3107962608337402,
      "memory_reserved_gb": 2.357421875,
      "max_memory_allocated_gb": 2.338654041290283,
      "model_size_gb": 2.3018836975097656,
      "energy_kwh": 3.331158857837227e-05,
      "energy_raw_kwh": 0.0001606456530117894,
      "energy_idle_kwh": 0.00012733406443341712,
      "joules_per_token": 0.2524667765939793,
      "joules_per_token_calculation": {
        "total_joules": 119.92171888214018,
        "total_tokens": 475,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:04:53.787548",
        "project_name": "N/A",
        "duration_sec": 14.5279061794281,
        "energy_kwh": 3.331158857837227e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.3107962608337402,
        "gpu_memory_reserved_gb": 2.357421875,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 51,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0001606456530117894,
        "energy_idle_contribution_kwh": 0.00012733406443341712,
        "energy_net_kwh": 3.331158857837227e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0004600045729011365,
        "codecarbon_cpu_energy_kwh": 0.0001711613709729153,
        "codecarbon_gpu_energy_kwh": 0.00020832016665599988
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Latency, TTFT, bsz=1)"
    },
    "mmlu_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 830.1633910128945,
      "std_ttft_ms": 292.604672409687,
      "throughput_tokens_per_sec": 51.822391949960995,
      "total_loop_time_sec": 81.88305902481079,
      "total_new_tokens": 4087,
      "avg_new_tokens_per_prompt": 43.02105263157895,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 2.3107962608337402,
      "memory_reserved_gb": 2.35546875,
      "max_memory_allocated_gb": 2.338714122772217,
      "model_size_gb": 2.3018836975097656,
      "energy_kwh": 0.0002717294262816114,
      "energy_raw_kwh": 0.0010324806350947293,
      "energy_idle_kwh": 0.0007607512088131179,
      "joules_per_token": 0.23935060793095206,
      "joules_per_token_calculation": {
        "total_joules": 978.2259346138011,
        "total_tokens": 4087,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:06:22.218603",
        "project_name": "N/A",
        "duration_sec": 86.79627275466919,
        "energy_kwh": 0.0002717294262816114,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.3107962608337402,
        "gpu_memory_reserved_gb": 2.35546875,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 70,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0010324806350947293,
        "energy_idle_contribution_kwh": 0.0007607512088131179,
        "energy_net_kwh": 0.0002717294262816114,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.002956480954642389,
        "codecarbon_cpu_energy_kwh": 0.001022350906365277,
        "codecarbon_gpu_energy_kwh": 0.001453146162516
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Latency, TTFT, bsz=1)"
    },
    "ifeval_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 1701.5351963043213,
      "std_ttft_ms": 1351.1886226820757,
      "throughput_tokens_per_sec": 51.74150977965075,
      "total_loop_time_sec": 45.756102561950684,
      "total_new_tokens": 2201,
      "avg_new_tokens_per_prompt": 88.04,
      "num_measured_prompts": 25,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 2.3107962608337402,
      "memory_reserved_gb": 2.33203125,
      "max_memory_allocated_gb": 2.338714122772217,
      "model_size_gb": 2.3018836975097656,
      "energy_kwh": 0.00017144385870914304,
      "energy_raw_kwh": 0.0005858578545648232,
      "energy_idle_kwh": 0.0004144139958556801,
      "joules_per_token": 0.2804170337814243,
      "joules_per_token_calculation": {
        "total_joules": 617.1978913529149,
        "total_tokens": 2201,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:07:11.277114",
        "project_name": "N/A",
        "duration_sec": 47.281673431396484,
        "energy_kwh": 0.00017144385870914304,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.3107962608337402,
        "gpu_memory_reserved_gb": 2.33203125,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 78,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0005858578545648232,
        "energy_idle_contribution_kwh": 0.0004144139958556801,
        "energy_net_kwh": 0.00017144385870914304,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0016775884508377571,
        "codecarbon_cpu_energy_kwh": 0.0005568510324993112,
        "codecarbon_gpu_energy_kwh": 0.0008587640203440001
      },
      "batch_size": 1,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Latency, TTFT, bsz=1)"
    },
    "hellaswag_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 484.4366203654896,
      "throughput_tokens_per_sec": 240.20404625035218,
      "total_loop_time_sec": 6.18011736869812,
      "total_new_tokens": 1280,
      "avg_new_tokens_per_prompt": 14.545454545454545,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 2.3107962608337402,
      "memory_reserved_gb": 2.625,
      "max_memory_allocated_gb": 2.532787799835205,
      "model_size_gb": 2.3018836975097656,
      "energy_kwh": 2.3659813864930986e-05,
      "energy_raw_kwh": 8.493377073136322e-05,
      "energy_idle_kwh": 6.127395686643223e-05,
      "joules_per_token": 0.0665432264951184,
      "joules_per_token_calculation": {
        "total_joules": 85.17532991375155,
        "total_tokens": 1280,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:07:20.031573",
        "project_name": "N/A",
        "duration_sec": 6.990920305252075,
        "energy_kwh": 2.3659813864930986e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.3107962608337402,
        "gpu_memory_reserved_gb": 2.625,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 78,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 8.493377073136322e-05,
        "energy_idle_contribution_kwh": 6.127395686643223e-05,
        "energy_net_kwh": 2.3659813864930986e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0002432056031251374,
        "codecarbon_cpu_energy_kwh": 8.233018533402693e-05,
        "codecarbon_gpu_energy_kwh": 0.00012214870882999978
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Throughput, bsz=8)"
    },
    "mmlu_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 1358.8495471260765,
      "throughput_tokens_per_sec": 280.98635542523095,
      "total_loop_time_sec": 17.692697286605835,
      "total_new_tokens": 4200,
      "avg_new_tokens_per_prompt": 47.72727272727273,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 2.3107962608337402,
      "memory_reserved_gb": 2.8125,
      "max_memory_allocated_gb": 2.5331315994262695,
      "model_size_gb": 2.3018836975097656,
      "energy_kwh": 6.341295005017131e-05,
      "energy_raw_kwh": 0.00022591009539591465,
      "energy_idle_kwh": 0.00016249714534574333,
      "joules_per_token": 0.054353957185861126,
      "joules_per_token_calculation": {
        "total_joules": 228.28662018061672,
        "total_tokens": 4200,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:07:40.225252",
        "project_name": "N/A",
        "duration_sec": 18.539762258529663,
        "energy_kwh": 6.341295005017131e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.3107962608337402,
        "gpu_memory_reserved_gb": 2.8125,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 78,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00022591009539591465,
        "energy_idle_contribution_kwh": 0.00016249714534574333,
        "energy_net_kwh": 6.341295005017131e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0006468875752213869,
        "codecarbon_cpu_energy_kwh": 0.0002184095251013879,
        "codecarbon_gpu_energy_kwh": 0.0003257335939199999
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Throughput, bsz=8)"
    },
    "ifeval_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 4018.3101495107016,
      "throughput_tokens_per_sec": 273.746913272472,
      "total_loop_time_sec": 16.008768320083618,
      "total_new_tokens": 3300,
      "avg_new_tokens_per_prompt": 137.5,
      "num_measured_prompts": 24,
      "num_warmup_batches": 1,
      "memory_allocated_gb": 2.3107962608337402,
      "memory_reserved_gb": 2.45703125,
      "max_memory_allocated_gb": 2.5331315994262695,
      "model_size_gb": 2.3018836975097656,
      "energy_kwh": 5.3178385584346866e-05,
      "energy_raw_kwh": 0.00020658745645903034,
      "energy_idle_kwh": 0.00015340907087468348,
      "joules_per_token": 0.05801278427383294,
      "joules_per_token_calculation": {
        "total_joules": 191.4421881036487,
        "total_tokens": 3300,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:07:59.374445",
        "project_name": "N/A",
        "duration_sec": 17.502877950668335,
        "energy_kwh": 5.3178385584346866e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.3107962608337402,
        "gpu_memory_reserved_gb": 2.45703125,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00020658745645903034,
        "energy_idle_contribution_kwh": 0.00015340907087468348,
        "energy_net_kwh": 5.3178385584346866e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0005915577103614144,
        "codecarbon_cpu_energy_kwh": 0.00020611485745763725,
        "codecarbon_gpu_energy_kwh": 0.00028847717522600003
      },
      "batch_size": 8,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Throughput, bsz=8)"
    }
  },
  "completed_workloads": [
    "hellaswag_latency_b1",
    "mmlu_latency_b1",
    "ifeval_latency_b1",
    "hellaswag_throughput_b8",
    "mmlu_throughput_b8",
    "ifeval_throughput_b8"
  ],
  "failed_workloads": []
}