{
  "metadata": {
    "model_name": "Llama-3.2-1B-pruned-60%-run2",
    "started_at": "2025-11-07T13:23:25.620629",
    "last_updated": "2025-11-07T13:28:06.107861",
    "mode": "carbon_profiling"
  },
  "results": {
    "hellaswag_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 387.9871343311511,
      "std_ttft_ms": 36.1686119049777,
      "throughput_tokens_per_sec": 50.57140017130613,
      "total_loop_time_sec": 38.98248600959778,
      "total_new_tokens": 1864,
      "avg_new_tokens_per_prompt": 19.621052631578948,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.4120001792907715,
      "memory_reserved_gb": 2.4375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.4019203186035156,
      "energy_kwh": 0.0001114299123625075,
      "energy_raw_kwh": 0.0004600576084303087,
      "energy_idle_kwh": 0.00034862769606780117,
      "joules_per_token": 0.215207985249478,
      "joules_per_token_calculation": {
        "total_joules": 401.14768450502703,
        "total_tokens": 1864,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:24:07.099728",
        "project_name": "N/A",
        "duration_sec": 39.77592706680298,
        "energy_kwh": 0.0001114299123625075,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.4120001792907715,
        "gpu_memory_reserved_gb": 2.4375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0004600576084303087,
        "energy_idle_contribution_kwh": 0.00034862769606780117,
        "energy_net_kwh": 0.0001114299123625075,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.001317362777692911,
        "codecarbon_cpu_energy_kwh": 0.00046851946982427614,
        "codecarbon_gpu_energy_kwh": 0.0006284196694019972
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Latency, TTFT, bsz=1)"
    },
    "mmlu_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 973.4675708569979,
      "std_ttft_ms": 13.504136167124262,
      "throughput_tokens_per_sec": 51.36277930242937,
      "total_loop_time_sec": 97.51117134094238,
      "total_new_tokens": 4750,
      "avg_new_tokens_per_prompt": 50.0,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.4120001792907715,
      "memory_reserved_gb": 2.4453125,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.4019203186035156,
      "energy_kwh": 0.00027161965764353345,
      "energy_raw_kwh": 0.0011337874284931802,
      "energy_idle_kwh": 0.0008621677708496468,
      "joules_per_token": 0.20585910895088852,
      "joules_per_token_calculation": {
        "total_joules": 977.8307675167205,
        "total_tokens": 4750,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:25:47.131104",
        "project_name": "N/A",
        "duration_sec": 98.3671772480011,
        "energy_kwh": 0.00027161965764353345,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.4120001792907715,
        "gpu_memory_reserved_gb": 2.4453125,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 75,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0011337874284931802,
        "energy_idle_contribution_kwh": 0.0008621677708496468,
        "energy_net_kwh": 0.00027161965764353345,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0032465702745558146,
        "codecarbon_cpu_energy_kwh": 0.0011586311559458513,
        "codecarbon_gpu_energy_kwh": 0.001542854012060016
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Latency, TTFT, bsz=1)"
    },
    "ifeval_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 2915.290184020996,
      "std_ttft_ms": 24.63744730612233,
      "throughput_tokens_per_sec": 51.45285392931563,
      "total_loop_time_sec": 87.41519093513489,
      "total_new_tokens": 3750,
      "avg_new_tokens_per_prompt": 150.0,
      "num_measured_prompts": 25,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.4120001792907715,
      "memory_reserved_gb": 2.431640625,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.4019203186035156,
      "energy_kwh": 0.00024432062283852004,
      "energy_raw_kwh": 0.0010183629876003232,
      "energy_idle_kwh": 0.0007740423647618032,
      "joules_per_token": 0.23454779792497923,
      "joules_per_token_calculation": {
        "total_joules": 879.5542422186721,
        "total_tokens": 3750,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:27:17.102143",
        "project_name": "N/A",
        "duration_sec": 88.31269860267639,
        "energy_kwh": 0.00024432062283852004,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.4120001792907715,
        "gpu_memory_reserved_gb": 2.431640625,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 75,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0010183629876003232,
        "energy_idle_contribution_kwh": 0.0007740423647618032,
        "energy_net_kwh": 0.00024432062283852004,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0029160554449302995,
        "codecarbon_cpu_energy_kwh": 0.0010401503465235964,
        "codecarbon_gpu_energy_kwh": 0.0013865580536900207
      },
      "batch_size": 1,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Latency, TTFT, bsz=1)"
    },
    "hellaswag_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 578.0035582455722,
      "throughput_tokens_per_sec": 264.23240921016026,
      "total_loop_time_sec": 7.552082061767578,
      "total_new_tokens": 1680,
      "avg_new_tokens_per_prompt": 19.09090909090909,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 1.4120001792907715,
      "memory_reserved_gb": 2.458984375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.4019203186035156,
      "energy_kwh": 2.5328405381474088e-05,
      "energy_raw_kwh": 9.856574021169508e-05,
      "energy_idle_kwh": 7.3237334830221e-05,
      "joules_per_token": 0.05427515438887305,
      "joules_per_token_calculation": {
        "total_joules": 91.18225937330672,
        "total_tokens": 1680,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:27:27.153363",
        "project_name": "N/A",
        "duration_sec": 8.35585618019104,
        "energy_kwh": 2.5328405381474088e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.4120001792907715,
        "gpu_memory_reserved_gb": 2.458984375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 9.856574021169508e-05,
        "energy_idle_contribution_kwh": 7.3237334830221e-05,
        "energy_net_kwh": 2.5328405381474088e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.000282240386706497,
        "codecarbon_cpu_energy_kwh": 9.843890726805223e-05,
        "codecarbon_gpu_energy_kwh": 0.00013749955444400386
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Throughput, bsz=8)"
    },
    "mmlu_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 1381.3693523406982,
      "throughput_tokens_per_sec": 276.4055689893509,
      "total_loop_time_sec": 18.128015518188477,
      "total_new_tokens": 4200,
      "avg_new_tokens_per_prompt": 47.72727272727273,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 1.4120001792907715,
      "memory_reserved_gb": 2.458984375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.4019203186035156,
      "energy_kwh": 6.104416282018654e-05,
      "energy_raw_kwh": 0.000227306482761375,
      "energy_idle_kwh": 0.00016626231994118845,
      "joules_per_token": 0.05232356813158846,
      "joules_per_token_calculation": {
        "total_joules": 219.75898615267155,
        "total_tokens": 4200,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:27:47.772418",
        "project_name": "N/A",
        "duration_sec": 18.96934175491333,
        "energy_kwh": 6.104416282018654e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.4120001792907715,
        "gpu_memory_reserved_gb": 2.458984375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 77,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.000227306482761375,
        "energy_idle_contribution_kwh": 0.00016626231994118845,
        "energy_net_kwh": 6.104416282018654e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.000650886093460819,
        "codecarbon_cpu_energy_kwh": 0.00022327756032569637,
        "codecarbon_gpu_energy_kwh": 0.00032257886917401446
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Throughput, bsz=8)"
    },
    "ifeval_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 3944.78972752889,
      "throughput_tokens_per_sec": 278.84882997022663,
      "total_loop_time_sec": 15.784177303314209,
      "total_new_tokens": 3300,
      "avg_new_tokens_per_prompt": 137.5,
      "num_measured_prompts": 24,
      "num_warmup_batches": 1,
      "memory_allocated_gb": 1.4120001792907715,
      "memory_reserved_gb": 2.458984375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.4019203186035156,
      "energy_kwh": 4.711944071924737e-05,
      "energy_raw_kwh": 0.0001930587236818991,
      "energy_idle_kwh": 0.00014593928296265172,
      "joules_per_token": 0.05140302623917896,
      "joules_per_token_calculation": {
        "total_joules": 169.62998658929055,
        "total_tokens": 3300,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:28:06.106190",
        "project_name": "N/A",
        "duration_sec": 16.650628566741943,
        "energy_kwh": 4.711944071924737e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.4120001792907715,
        "gpu_memory_reserved_gb": 2.458984375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0001930587236818991,
        "energy_idle_contribution_kwh": 0.00014593928296265172,
        "energy_net_kwh": 4.711944071924737e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0005528185423455755,
        "codecarbon_cpu_energy_kwh": 0.00019596152904513946,
        "codecarbon_gpu_energy_kwh": 0.00026468132285599466
      },
      "batch_size": 8,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Throughput, bsz=8)"
    }
  },
  "completed_workloads": [
    "hellaswag_latency_b1",
    "mmlu_latency_b1",
    "ifeval_latency_b1",
    "hellaswag_throughput_b8",
    "mmlu_throughput_b8",
    "ifeval_throughput_b8"
  ],
  "failed_workloads": []
}