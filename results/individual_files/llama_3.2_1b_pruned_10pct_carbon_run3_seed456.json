{
  "metadata": {
    "model_name": "Llama-3.2-1B-pruned-10%-run3",
    "started_at": "2025-11-07T12:21:36.885976",
    "last_updated": "2025-11-07T12:25:13.846218",
    "mode": "carbon_profiling"
  },
  "results": {
    "hellaswag_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 214.02354491384406,
      "std_ttft_ms": 167.70901466167874,
      "throughput_tokens_per_sec": 50.26500584864499,
      "total_loop_time_sec": 21.137845039367676,
      "total_new_tokens": 1022,
      "avg_new_tokens_per_prompt": 10.757894736842106,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 2.1616110801696777,
      "memory_reserved_gb": 2.490234375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 2.1519203186035156,
      "energy_kwh": 7.674614432027016e-05,
      "energy_raw_kwh": 0.0002697743344464981,
      "energy_idle_kwh": 0.00019302819012622792,
      "joules_per_token": 0.27033866883852503,
      "joules_per_token_calculation": {
        "total_joules": 276.2861195529726,
        "total_tokens": 1022,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:22:00.608074",
        "project_name": "N/A",
        "duration_sec": 22.023136138916016,
        "energy_kwh": 7.674614432027016e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.1616110801696777,
        "gpu_memory_reserved_gb": 2.490234375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 77,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0002697743344464981,
        "energy_idle_contribution_kwh": 0.00019302819012622792,
        "energy_net_kwh": 7.674614432027016e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0007724916620535167,
        "codecarbon_cpu_energy_kwh": 0.0002592149236993016,
        "codecarbon_gpu_energy_kwh": 0.000391325035281994
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Latency, TTFT, bsz=1)"
    },
    "mmlu_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 787.9773340727154,
      "std_ttft_ms": 339.9008544923537,
      "throughput_tokens_per_sec": 51.56440063746668,
      "total_loop_time_sec": 79.88580751419067,
      "total_new_tokens": 3860,
      "avg_new_tokens_per_prompt": 40.63157894736842,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 2.1616110801696777,
      "memory_reserved_gb": 2.4921875,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 2.1519203186035156,
      "energy_kwh": 0.0002965615955618304,
      "energy_raw_kwh": 0.0010058038266003797,
      "energy_idle_kwh": 0.0007092422310385493,
      "joules_per_token": 0.27658594404730297,
      "joules_per_token_calculation": {
        "total_joules": 1067.6217440225894,
        "total_tokens": 3860,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:23:23.190372",
        "project_name": "N/A",
        "duration_sec": 80.9194667339325,
        "energy_kwh": 0.0002965615955618304,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.1616110801696777,
        "gpu_memory_reserved_gb": 2.4921875,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 75,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0010058038266003797,
        "energy_idle_contribution_kwh": 0.0007092422310385493,
        "energy_net_kwh": 0.0002965615955618304,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0028800926200205487,
        "codecarbon_cpu_energy_kwh": 0.0009529932600861007,
        "codecarbon_gpu_energy_kwh": 0.0014787595163400016
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Latency, TTFT, bsz=1)"
    },
    "ifeval_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 1861.6533184051514,
      "std_ttft_ms": 1378.1785136289507,
      "throughput_tokens_per_sec": 51.7819290234899,
      "total_loop_time_sec": 57.44512963294983,
      "total_new_tokens": 2410,
      "avg_new_tokens_per_prompt": 96.4,
      "num_measured_prompts": 25,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 2.1616110801696777,
      "memory_reserved_gb": 2.484375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 2.1519203186035156,
      "energy_kwh": 0.0002136692105358019,
      "energy_raw_kwh": 0.0007270289284336298,
      "energy_idle_kwh": 0.0005133597178978279,
      "joules_per_token": 0.31917392445182025,
      "joules_per_token_calculation": {
        "total_joules": 769.2091579288868,
        "total_tokens": 2410,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:24:23.407292",
        "project_name": "N/A",
        "duration_sec": 58.57067275047302,
        "energy_kwh": 0.0002136692105358019,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.1616110801696777,
        "gpu_memory_reserved_gb": 2.484375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0007270289284336298,
        "energy_idle_contribution_kwh": 0.0005133597178978279,
        "energy_net_kwh": 0.0002136692105358019,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0020818280821227036,
        "codecarbon_cpu_energy_kwh": 0.0006899063480680426,
        "codecarbon_gpu_energy_kwh": 0.0010673539094379972
      },
      "batch_size": 1,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Latency, TTFT, bsz=1)"
    },
    "hellaswag_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 614.9197058244185,
      "throughput_tokens_per_sec": 236.54233890510238,
      "total_loop_time_sec": 8.155831098556519,
      "total_new_tokens": 1600,
      "avg_new_tokens_per_prompt": 18.181818181818183,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 2.1616110801696777,
      "memory_reserved_gb": 2.58203125,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 2.1519203186035156,
      "energy_kwh": 3.396394297983723e-05,
      "energy_raw_kwh": 0.00011391832419534599,
      "energy_idle_kwh": 7.995438121550876e-05,
      "joules_per_token": 0.07641887170463377,
      "joules_per_token_calculation": {
        "total_joules": 122.27019472741402,
        "total_tokens": 1600,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:24:34.189157",
        "project_name": "N/A",
        "duration_sec": 9.122223138809204,
        "energy_kwh": 3.396394297983723e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.1616110801696777,
        "gpu_memory_reserved_gb": 2.58203125,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 75,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00011391832419534599,
        "energy_idle_contribution_kwh": 7.995438121550876e-05,
        "energy_net_kwh": 3.396394297983723e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0003262021043497991,
        "codecarbon_cpu_energy_kwh": 0.00010740688382291303,
        "codecarbon_gpu_energy_kwh": 0.00016826985683799853
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Throughput, bsz=8)"
    },
    "mmlu_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 1421.0359183224764,
      "throughput_tokens_per_sec": 268.6900287987904,
      "total_loop_time_sec": 18.47112202644348,
      "total_new_tokens": 4200,
      "avg_new_tokens_per_prompt": 47.72727272727273,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 2.1616110801696777,
      "memory_reserved_gb": 2.654296875,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 2.1519203186035156,
      "energy_kwh": 7.487314377976759e-05,
      "energy_raw_kwh": 0.0002443124660727717,
      "energy_idle_kwh": 0.0001694393222930041,
      "joules_per_token": 0.06417698038265793,
      "joules_per_token_calculation": {
        "total_joules": 269.5433176071633,
        "total_tokens": 4200,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:24:55.176715",
        "project_name": "N/A",
        "duration_sec": 19.331815004348755,
        "energy_kwh": 7.487314377976759e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.1616110801696777,
        "gpu_memory_reserved_gb": 2.654296875,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 75,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0002443124660727717,
        "energy_idle_contribution_kwh": 0.0001694393222930041,
        "energy_net_kwh": 7.487314377976759e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.000699582276290919,
        "codecarbon_cpu_energy_kwh": 0.00022751529203958446,
        "codecarbon_gpu_energy_kwh": 0.00036504362536799936
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Throughput, bsz=8)"
    },
    "ifeval_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 3954.709847768148,
      "throughput_tokens_per_sec": 278.14935667677065,
      "total_loop_time_sec": 15.816143989562988,
      "total_new_tokens": 3300,
      "avg_new_tokens_per_prompt": 137.5,
      "num_measured_prompts": 24,
      "num_warmup_batches": 1,
      "memory_allocated_gb": 2.1616110801696777,
      "memory_reserved_gb": 2.51171875,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 2.1519203186035156,
      "energy_kwh": 6.284599290557906e-05,
      "energy_raw_kwh": 0.00021186957334395527,
      "energy_idle_kwh": 0.0001490235804383762,
      "joules_per_token": 0.06855926498790443,
      "joules_per_token_calculation": {
        "total_joules": 226.2455744600846,
        "total_tokens": 3300,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:25:13.844658",
        "project_name": "N/A",
        "duration_sec": 17.002524852752686,
        "energy_kwh": 6.284599290557906e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.1616110801696777,
        "gpu_memory_reserved_gb": 2.51171875,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 77,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00021186957334395527,
        "energy_idle_contribution_kwh": 0.0001490235804383762,
        "energy_net_kwh": 6.284599290557906e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.000606682912171173,
        "codecarbon_cpu_energy_kwh": 0.0002002587527902814,
        "codecarbon_gpu_energy_kwh": 0.00031221247199200025
      },
      "batch_size": 8,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Throughput, bsz=8)"
    }
  },
  "completed_workloads": [
    "hellaswag_latency_b1",
    "mmlu_latency_b1",
    "ifeval_latency_b1",
    "hellaswag_throughput_b8",
    "mmlu_throughput_b8",
    "ifeval_throughput_b8"
  ],
  "failed_workloads": []
}