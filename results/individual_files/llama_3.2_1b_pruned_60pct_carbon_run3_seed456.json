{
  "metadata": {
    "model_name": "Llama-3.2-1B-pruned-60%-run3",
    "started_at": "2025-11-07T13:28:10.908639",
    "last_updated": "2025-11-07T13:32:45.601472",
    "mode": "carbon_profiling"
  },
  "results": {
    "hellaswag_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 392.29316962392704,
      "std_ttft_ms": 22.544295926316114,
      "throughput_tokens_per_sec": 50.66028610586893,
      "total_loop_time_sec": 39.39725875854492,
      "total_new_tokens": 1888,
      "avg_new_tokens_per_prompt": 19.873684210526317,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.4120001792907715,
      "memory_reserved_gb": 2.4375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.4019203186035156,
      "energy_kwh": 0.00011255601500418825,
      "energy_raw_kwh": 0.0004650708879842488,
      "energy_idle_kwh": 0.00035251487298006053,
      "joules_per_token": 0.2146195201351047,
      "joules_per_token_calculation": {
        "total_joules": 405.2016540150777,
        "total_tokens": 1888,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:28:52.842928",
        "project_name": "N/A",
        "duration_sec": 40.21942615509033,
        "energy_kwh": 0.00011255601500418825,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.4120001792907715,
        "gpu_memory_reserved_gb": 2.4375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 77,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0004650708879842488,
        "energy_idle_contribution_kwh": 0.00035251487298006053,
        "energy_net_kwh": 0.00011255601500418825,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0013317181709252134,
        "codecarbon_cpu_energy_kwh": 0.00047372857078341394,
        "codecarbon_gpu_energy_kwh": 0.0006351360636639991
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Latency, TTFT, bsz=1)"
    },
    "mmlu_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 965.0972265946237,
      "std_ttft_ms": 59.51124194854689,
      "throughput_tokens_per_sec": 51.3501576537617,
      "total_loop_time_sec": 96.64488244056702,
      "total_new_tokens": 4708,
      "avg_new_tokens_per_prompt": 49.55789473684211,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.4120001792907715,
      "memory_reserved_gb": 2.443359375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.4019203186035156,
      "energy_kwh": 0.00027037582814640985,
      "energy_raw_kwh": 0.0011252669756723359,
      "energy_idle_kwh": 0.000854891147525926,
      "joules_per_token": 0.20674447351892003,
      "joules_per_token_calculation": {
        "total_joules": 973.3529813270754,
        "total_tokens": 4708,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:30:32.063699",
        "project_name": "N/A",
        "duration_sec": 97.53696656227112,
        "energy_kwh": 0.00027037582814640985,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.4120001792907715,
        "gpu_memory_reserved_gb": 2.443359375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0011252669756723359,
        "energy_idle_contribution_kwh": 0.000854891147525926,
        "energy_net_kwh": 0.00027037582814640985,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.003222172183556806,
        "codecarbon_cpu_energy_kwh": 0.0011488034806548262,
        "codecarbon_gpu_energy_kwh": 0.0015329065041019968
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Latency, TTFT, bsz=1)"
    },
    "ifeval_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 2690.955629348755,
      "std_ttft_ms": 696.1172869425768,
      "throughput_tokens_per_sec": 51.83288735004371,
      "total_loop_time_sec": 81.97662425041199,
      "total_new_tokens": 3487,
      "avg_new_tokens_per_prompt": 139.48,
      "num_measured_prompts": 25,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.4120001792907715,
      "memory_reserved_gb": 2.431640625,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.4019203186035156,
      "energy_kwh": 0.00023000106310666504,
      "energy_raw_kwh": 0.0009569855149731378,
      "energy_idle_kwh": 0.0007269844518664728,
      "joules_per_token": 0.2374544958944635,
      "joules_per_token_calculation": {
        "total_joules": 828.0038271839942,
        "total_tokens": 3487,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:31:56.692933",
        "project_name": "N/A",
        "duration_sec": 82.94372725486755,
        "energy_kwh": 0.00023000106310666504,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.4120001792907715,
        "gpu_memory_reserved_gb": 2.431640625,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 75,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0009569855149731378,
        "energy_idle_contribution_kwh": 0.0007269844518664728,
        "energy_net_kwh": 0.00023000106310666504,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0027403026775675403,
        "codecarbon_cpu_energy_kwh": 0.0009769630575028197,
        "codecarbon_gpu_energy_kwh": 0.0013037207651980104
      },
      "batch_size": 1,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Latency, TTFT, bsz=1)"
    },
    "hellaswag_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 579.9622318961403,
      "throughput_tokens_per_sec": 263.340032036126,
      "total_loop_time_sec": 7.60105037689209,
      "total_new_tokens": 1680,
      "avg_new_tokens_per_prompt": 19.09090909090909,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 1.4120001792907715,
      "memory_reserved_gb": 2.458984375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.4019203186035156,
      "energy_kwh": 2.6083799122212034e-05,
      "energy_raw_kwh": 9.960707237657448e-05,
      "energy_idle_kwh": 7.352327325436245e-05,
      "joules_per_token": 0.05589385526188293,
      "joules_per_token_calculation": {
        "total_joules": 93.90167683996333,
        "total_tokens": 1680,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:32:06.762956",
        "project_name": "N/A",
        "duration_sec": 8.388479709625244,
        "energy_kwh": 2.6083799122212034e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.4120001792907715,
        "gpu_memory_reserved_gb": 2.458984375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 9.960707237657448e-05,
        "energy_idle_contribution_kwh": 7.352327325436245e-05,
        "energy_net_kwh": 2.6083799122212034e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0002852222137822562,
        "codecarbon_cpu_energy_kwh": 9.877050246803379e-05,
        "codecarbon_gpu_energy_kwh": 0.00013999705644200666
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Throughput, bsz=8)"
    },
    "mmlu_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 1367.5453446128151,
      "throughput_tokens_per_sec": 279.19965017780356,
      "total_loop_time_sec": 17.799757480621338,
      "total_new_tokens": 4200,
      "avg_new_tokens_per_prompt": 47.72727272727273,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 1.4120001792907715,
      "memory_reserved_gb": 2.455078125,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.4019203186035156,
      "energy_kwh": 5.866258154500485e-05,
      "energy_raw_kwh": 0.00022226889205387255,
      "energy_idle_kwh": 0.0001636063105088677,
      "joules_per_token": 0.0502822127528613,
      "joules_per_token_calculation": {
        "total_joules": 211.18529356201745,
        "total_tokens": 4200,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:32:27.082082",
        "project_name": "N/A",
        "duration_sec": 18.66631007194519,
        "energy_kwh": 5.866258154500485e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.4120001792907715,
        "gpu_memory_reserved_gb": 2.455078125,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00022226889205387255,
        "energy_idle_contribution_kwh": 0.0001636063105088677,
        "energy_net_kwh": 5.866258154500485e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0006364610858841413,
        "codecarbon_cpu_energy_kwh": 0.0002197040350805588,
        "codecarbon_gpu_energy_kwh": 0.00031342830629800533
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Throughput, bsz=8)"
    },
    "ifeval_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 3969.5886770884194,
      "throughput_tokens_per_sec": 277.10679606402414,
      "total_loop_time_sec": 15.937870979309082,
      "total_new_tokens": 3300,
      "avg_new_tokens_per_prompt": 137.5,
      "num_measured_prompts": 24,
      "num_warmup_batches": 1,
      "memory_allocated_gb": 1.4120001792907715,
      "memory_reserved_gb": 2.458984375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.4019203186035156,
      "energy_kwh": 4.733869608343098e-05,
      "energy_raw_kwh": 0.0001950695656091484,
      "energy_idle_kwh": 0.00014773086952571743,
      "joules_per_token": 0.05164221390919743,
      "joules_per_token_calculation": {
        "total_joules": 170.41930590035153,
        "total_tokens": 3300,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:32:45.599780",
        "project_name": "N/A",
        "duration_sec": 16.85503578186035,
        "energy_kwh": 4.733869608343098e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.4120001792907715,
        "gpu_memory_reserved_gb": 2.458984375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0001950695656091484,
        "energy_idle_contribution_kwh": 0.00014773086952571743,
        "energy_net_kwh": 4.733869608343098e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0005585765349496339,
        "codecarbon_cpu_energy_kwh": 0.00019842737726251587,
        "codecarbon_gpu_energy_kwh": 0.0002668035467759955
      },
      "batch_size": 8,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Throughput, bsz=8)"
    }
  },
  "completed_workloads": [
    "hellaswag_latency_b1",
    "mmlu_latency_b1",
    "ifeval_latency_b1",
    "hellaswag_throughput_b8",
    "mmlu_throughput_b8",
    "ifeval_throughput_b8"
  ],
  "failed_workloads": []
}