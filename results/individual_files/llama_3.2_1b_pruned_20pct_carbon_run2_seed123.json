{
  "metadata": {
    "model_name": "Llama-3.2-1B-pruned-20%-run2",
    "started_at": "2025-11-07T12:29:23.790764",
    "last_updated": "2025-11-07T12:33:19.351476",
    "mode": "carbon_profiling"
  },
  "results": {
    "hellaswag_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 296.73154228612,
      "std_ttft_ms": 151.7500051433214,
      "throughput_tokens_per_sec": 50.97643369132732,
      "total_loop_time_sec": 29.22531533241272,
      "total_new_tokens": 1437,
      "avg_new_tokens_per_prompt": 15.126315789473685,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 2.012814998626709,
      "memory_reserved_gb": 2.474609375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 2.0019569396972656,
      "energy_kwh": 0.00010443222565614704,
      "energy_raw_kwh": 0.0003684169636595047,
      "energy_idle_kwh": 0.0002639847380033577,
      "joules_per_token": 0.2616256175101805,
      "joules_per_token_calculation": {
        "total_joules": 375.95601236212934,
        "total_tokens": 1437,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:29:55.615570",
        "project_name": "N/A",
        "duration_sec": 30.118770837783813,
        "energy_kwh": 0.00010443222565614704,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.012814998626709,
        "gpu_memory_reserved_gb": 2.474609375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 77,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0003684169636595047,
        "energy_idle_contribution_kwh": 0.0002639847380033577,
        "energy_net_kwh": 0.00010443222565614704,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0010549522183789608,
        "codecarbon_cpu_energy_kwh": 0.0003546321333013949,
        "codecarbon_gpu_energy_kwh": 0.0005334793156720023
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Latency, TTFT, bsz=1)"
    },
    "mmlu_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 755.827115711413,
      "std_ttft_ms": 390.30913204447984,
      "throughput_tokens_per_sec": 49.80253351684496,
      "total_loop_time_sec": 75.81593608856201,
      "total_new_tokens": 3576,
      "avg_new_tokens_per_prompt": 37.642105263157895,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 2.012814998626709,
      "memory_reserved_gb": 2.482421875,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 2.0019569396972656,
      "energy_kwh": 0.0002565197905268587,
      "energy_raw_kwh": 0.0009291505855193147,
      "energy_idle_kwh": 0.000672630794992456,
      "joules_per_token": 0.2582413998592537,
      "joules_per_token_calculation": {
        "total_joules": 923.4712458966912,
        "total_tokens": 3576,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:31:14.029006",
        "project_name": "N/A",
        "duration_sec": 76.74236369132996,
        "energy_kwh": 0.0002565197905268587,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.012814998626709,
        "gpu_memory_reserved_gb": 2.482421875,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 77,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0009291505855193147,
        "energy_idle_contribution_kwh": 0.000672630794992456,
        "energy_net_kwh": 0.0002565197905268587,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0026605980942496243,
        "codecarbon_cpu_energy_kwh": 0.000903921317612512,
        "codecarbon_gpu_energy_kwh": 0.0013314293984759973
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Latency, TTFT, bsz=1)"
    },
    "ifeval_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 2314.8561000823975,
      "std_ttft_ms": 1096.1881890911586,
      "throughput_tokens_per_sec": 51.52804098524924,
      "total_loop_time_sec": 72.48051881790161,
      "total_new_tokens": 2982,
      "avg_new_tokens_per_prompt": 119.28,
      "num_measured_prompts": 25,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 2.012814998626709,
      "memory_reserved_gb": 2.46875,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 2.0019569396972656,
      "energy_kwh": 0.00025129448774217087,
      "energy_raw_kwh": 0.0008956464400504824,
      "energy_idle_kwh": 0.0006443519523083116,
      "joules_per_token": 0.3033736270529226,
      "joules_per_token_calculation": {
        "total_joules": 904.6601558718152,
        "total_tokens": 2982,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:32:29.232454",
        "project_name": "N/A",
        "duration_sec": 73.51594996452332,
        "energy_kwh": 0.00025129448774217087,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.012814998626709,
        "gpu_memory_reserved_gb": 2.46875,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0008956464400504824,
        "energy_idle_contribution_kwh": 0.0006443519523083116,
        "energy_net_kwh": 0.00025129448774217087,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.002564659861014787,
        "codecarbon_cpu_energy_kwh": 0.0008658074445694507,
        "codecarbon_gpu_energy_kwh": 0.0012915438110119998
      },
      "batch_size": 1,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Latency, TTFT, bsz=1)"
    },
    "hellaswag_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 603.4069061279297,
      "throughput_tokens_per_sec": 253.10826106934326,
      "total_loop_time_sec": 7.890251874923706,
      "total_new_tokens": 1680,
      "avg_new_tokens_per_prompt": 19.09090909090909,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 2.012814998626709,
      "memory_reserved_gb": 2.49609375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 2.0019569396972656,
      "energy_kwh": 2.7849597984021157e-05,
      "energy_raw_kwh": 0.00010381617957930342,
      "energy_idle_kwh": 7.596658159528227e-05,
      "joules_per_token": 0.05967770996575963,
      "joules_per_token_calculation": {
        "total_joules": 100.25855274247617,
        "total_tokens": 1680,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:32:39.567361",
        "project_name": "N/A",
        "duration_sec": 8.667243719100952,
        "energy_kwh": 2.7849597984021157e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.012814998626709,
        "gpu_memory_reserved_gb": 2.49609375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 75,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00010381617957930342,
        "energy_idle_contribution_kwh": 7.596658159528227e-05,
        "energy_net_kwh": 2.7849597984021157e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0002972748807843588,
        "codecarbon_cpu_energy_kwh": 0.00010211162385346628,
        "codecarbon_gpu_energy_kwh": 0.00014712900659200404
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Throughput, bsz=8)"
    },
    "mmlu_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 1410.2805961262095,
      "throughput_tokens_per_sec": 270.7391584816302,
      "total_loop_time_sec": 18.5113525390625,
      "total_new_tokens": 4200,
      "avg_new_tokens_per_prompt": 47.72727272727273,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 2.012814998626709,
      "memory_reserved_gb": 2.546875,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 2.0019569396972656,
      "energy_kwh": 6.470837243552005e-05,
      "energy_raw_kwh": 0.00023435922350435834,
      "energy_idle_kwh": 0.0001696508510688383,
      "joules_per_token": 0.05546431923044576,
      "joules_per_token_calculation": {
        "total_joules": 232.95014076787217,
        "total_tokens": 4200,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:33:00.573934",
        "project_name": "N/A",
        "duration_sec": 19.35594892501831,
        "energy_kwh": 6.470837243552005e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.012814998626709,
        "gpu_memory_reserved_gb": 2.546875,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 77,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00023435922350435834,
        "energy_idle_contribution_kwh": 0.0001696508510688383,
        "energy_net_kwh": 6.470837243552005e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0006710814297953815,
        "codecarbon_cpu_energy_kwh": 0.00022797030243472174,
        "codecarbon_gpu_energy_kwh": 0.00033586776869399745
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Throughput, bsz=8)"
    },
    "ifeval_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 4004.187901814779,
      "throughput_tokens_per_sec": 274.71238287830045,
      "total_loop_time_sec": 16.024658918380737,
      "total_new_tokens": 3300,
      "avg_new_tokens_per_prompt": 137.5,
      "num_measured_prompts": 24,
      "num_warmup_batches": 1,
      "memory_allocated_gb": 2.012814998626709,
      "memory_reserved_gb": 2.49609375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 2.0019569396972656,
      "energy_kwh": 5.0978416530280616e-05,
      "energy_raw_kwh": 0.00020094198299458428,
      "energy_idle_kwh": 0.00014996356646430366,
      "joules_per_token": 0.055612818033033404,
      "joules_per_token_calculation": {
        "total_joules": 183.52229950901022,
        "total_tokens": 3300,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:33:19.349821",
        "project_name": "N/A",
        "duration_sec": 17.10977053642273,
        "energy_kwh": 5.0978416530280616e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.012814998626709,
        "gpu_memory_reserved_gb": 2.49609375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00020094198299458428,
        "energy_idle_contribution_kwh": 0.00014996356646430366,
        "energy_net_kwh": 5.0978416530280616e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0005753920466092391,
        "codecarbon_cpu_energy_kwh": 0.0002014894549812447,
        "codecarbon_gpu_energy_kwh": 0.000279118556627999
      },
      "batch_size": 8,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Throughput, bsz=8)"
    }
  },
  "completed_workloads": [
    "hellaswag_latency_b1",
    "mmlu_latency_b1",
    "ifeval_latency_b1",
    "hellaswag_throughput_b8",
    "mmlu_throughput_b8",
    "ifeval_throughput_b8"
  ],
  "failed_workloads": []
}