{
  "metadata": {
    "model_name": "Llama-3.2-1B-pruned-40%-run2",
    "started_at": "2025-11-07T12:55:27.833664",
    "last_updated": "2025-11-07T13:00:04.843042",
    "mode": "carbon_profiling"
  },
  "results": {
    "hellaswag_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 398.73984989367034,
      "std_ttft_ms": 10.351045936106905,
      "throughput_tokens_per_sec": 50.05242075043205,
      "total_loop_time_sec": 40.081380128860474,
      "total_new_tokens": 1896,
      "avg_new_tokens_per_prompt": 19.957894736842107,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.7099661827087402,
      "memory_reserved_gb": 1.748046875,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.7020301818847656,
      "energy_kwh": 0.00013025719121860404,
      "energy_raw_kwh": 0.0004892976887963234,
      "energy_idle_kwh": 0.0003590404975777194,
      "joules_per_token": 0.24732378079481782,
      "joules_per_token_calculation": {
        "total_joules": 468.9258883869746,
        "total_tokens": 1896,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:56:10.465455",
        "project_name": "N/A",
        "duration_sec": 40.963953256607056,
        "energy_kwh": 0.00013025719121860404,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.7099661827087402,
        "gpu_memory_reserved_gb": 1.748046875,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0004892976887963234,
        "energy_idle_contribution_kwh": 0.0003590404975777194,
        "energy_net_kwh": 0.00013025719121860404,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0014010909734342328,
        "codecarbon_cpu_energy_kwh": 0.00048251265976668165,
        "codecarbon_gpu_energy_kwh": 0.0006915780532619972
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Latency, TTFT, bsz=1)"
    },
    "mmlu_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 909.6286848971719,
      "std_ttft_ms": 255.93447019550322,
      "throughput_tokens_per_sec": 50.51222458562497,
      "total_loop_time_sec": 91.48302125930786,
      "total_new_tokens": 4365,
      "avg_new_tokens_per_prompt": 45.94736842105263,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.7099661827087402,
      "memory_reserved_gb": 1.775390625,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.7020301818847656,
      "energy_kwh": 0.00028264350880401006,
      "energy_raw_kwh": 0.0010927101257912058,
      "energy_idle_kwh": 0.0008100666169871957,
      "joules_per_token": 0.23310804849815264,
      "joules_per_token_calculation": {
        "total_joules": 1017.5166316944362,
        "total_tokens": 4365,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:57:44.546437",
        "project_name": "N/A",
        "duration_sec": 92.4228081703186,
        "energy_kwh": 0.00028264350880401006,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.7099661827087402,
        "gpu_memory_reserved_gb": 1.775390625,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0010927101257912058,
        "energy_idle_contribution_kwh": 0.0008100666169871957,
        "energy_net_kwh": 0.00028264350880401006,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.003128946506149422,
        "codecarbon_cpu_energy_kwh": 0.0010885837284569683,
        "codecarbon_gpu_energy_kwh": 0.0015282278892480017
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Latency, TTFT, bsz=1)"
    },
    "ifeval_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 2917.8234481811523,
      "std_ttft_ms": 28.177795667110217,
      "throughput_tokens_per_sec": 51.40818238797267,
      "total_loop_time_sec": 87.70073246955872,
      "total_new_tokens": 3750,
      "avg_new_tokens_per_prompt": 150.0,
      "num_measured_prompts": 25,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.7099661827087402,
      "memory_reserved_gb": 1.7421875,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.7020301818847656,
      "energy_kwh": 0.00027444401960442257,
      "energy_raw_kwh": 0.0010509372803681489,
      "energy_idle_kwh": 0.0007764932607637263,
      "joules_per_token": 0.26346625882024566,
      "joules_per_token_calculation": {
        "total_joules": 987.9984705759213,
        "total_tokens": 3750,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:59:14.810167",
        "project_name": "N/A",
        "duration_sec": 88.59232831001282,
        "energy_kwh": 0.00027444401960442257,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.7099661827087402,
        "gpu_memory_reserved_gb": 1.7421875,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 77,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0010509372803681489,
        "energy_idle_contribution_kwh": 0.0007764932607637263,
        "energy_net_kwh": 0.00027444401960442257,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.003009331069581784,
        "codecarbon_cpu_energy_kwh": 0.0010434621625771171,
        "codecarbon_gpu_energy_kwh": 0.0014749653466380047
      },
      "batch_size": 1,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Latency, TTFT, bsz=1)"
    },
    "hellaswag_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 592.9199132052335,
      "throughput_tokens_per_sec": 257.58499474516327,
      "total_loop_time_sec": 7.754446029663086,
      "total_new_tokens": 1680,
      "avg_new_tokens_per_prompt": 19.09090909090909,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 1.7099661827087402,
      "memory_reserved_gb": 2.021484375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.7020301818847656,
      "energy_kwh": 2.538567257159011e-05,
      "energy_raw_kwh": 0.00010011213027959142,
      "energy_idle_kwh": 7.47264577080013e-05,
      "joules_per_token": 0.05439786979626453,
      "joules_per_token_calculation": {
        "total_joules": 91.3884212577244,
        "total_tokens": 1680,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:59:25.010766",
        "project_name": "N/A",
        "duration_sec": 8.525754451751709,
        "energy_kwh": 2.538567257159011e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.7099661827087402,
        "gpu_memory_reserved_gb": 2.021484375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00010011213027959142,
        "energy_idle_contribution_kwh": 7.47264577080013e-05,
        "energy_net_kwh": 2.538567257159011e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.00028666843371172166,
        "codecarbon_cpu_energy_kwh": 0.00010044010575971892,
        "codecarbon_gpu_energy_kwh": 0.0001389817778520014
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Throughput, bsz=8)"
    },
    "mmlu_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 1403.572992845015,
      "throughput_tokens_per_sec": 272.03300702177506,
      "total_loop_time_sec": 18.412543296813965,
      "total_new_tokens": 4200,
      "avg_new_tokens_per_prompt": 47.72727272727273,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 1.7099661827087402,
      "memory_reserved_gb": 2.236328125,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.7020301818847656,
      "energy_kwh": 5.8823799527219715e-05,
      "energy_raw_kwh": 0.00022796457003287235,
      "energy_idle_kwh": 0.00016914077050565263,
      "joules_per_token": 0.05042039959475975,
      "joules_per_token_calculation": {
        "total_joules": 211.76567829799097,
        "total_tokens": 4200,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:59:45.975883",
        "project_name": "N/A",
        "duration_sec": 19.297752380371094,
        "energy_kwh": 5.8823799527219715e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.7099661827087402,
        "gpu_memory_reserved_gb": 2.236328125,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 74,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00022796457003287235,
        "energy_idle_contribution_kwh": 0.00016914077050565263,
        "energy_net_kwh": 5.8823799527219715e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0006527705089341378,
        "codecarbon_cpu_energy_kwh": 0.00022720383424859402,
        "codecarbon_gpu_energy_kwh": 0.00031869747717999664
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Throughput, bsz=8)"
    },
    "ifeval_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 4069.5323944091797,
      "throughput_tokens_per_sec": 270.3013254081,
      "total_loop_time_sec": 16.289603233337402,
      "total_new_tokens": 3300,
      "avg_new_tokens_per_prompt": 137.5,
      "num_measured_prompts": 24,
      "num_warmup_batches": 1,
      "memory_allocated_gb": 1.7099661827087402,
      "memory_reserved_gb": 1.84765625,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.7020301818847656,
      "energy_kwh": 4.497130043605081e-05,
      "energy_raw_kwh": 0.00019550989917912893,
      "energy_idle_kwh": 0.00015053859874307812,
      "joules_per_token": 0.04905960047569179,
      "joules_per_token_calculation": {
        "total_joules": 161.8966815697829,
        "total_tokens": 3300,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:00:04.841344",
        "project_name": "N/A",
        "duration_sec": 17.17537760734558,
        "energy_kwh": 4.497130043605081e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.7099661827087402,
        "gpu_memory_reserved_gb": 1.84765625,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 72,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00019550989917912893,
        "energy_idle_contribution_kwh": 0.00015053859874307812,
        "energy_net_kwh": 4.497130043605081e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0005598374184655922,
        "codecarbon_cpu_energy_kwh": 0.00020224710943471204,
        "codecarbon_gpu_energy_kwh": 0.0002624485432920001
      },
      "batch_size": 8,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Throughput, bsz=8)"
    }
  },
  "completed_workloads": [
    "hellaswag_latency_b1",
    "mmlu_latency_b1",
    "ifeval_latency_b1",
    "hellaswag_throughput_b8",
    "mmlu_throughput_b8",
    "ifeval_throughput_b8"
  ],
  "failed_workloads": []
}