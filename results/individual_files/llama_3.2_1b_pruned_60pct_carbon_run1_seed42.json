{
  "metadata": {
    "model_name": "Llama-3.2-1B-pruned-60%-run1",
    "started_at": "2025-11-07T13:18:44.316393",
    "last_updated": "2025-11-07T13:23:20.813286",
    "mode": "carbon_profiling"
  },
  "results": {
    "hellaswag_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 387.3390373430754,
      "std_ttft_ms": 18.613069495603494,
      "throughput_tokens_per_sec": 51.33541577092286,
      "total_loop_time_sec": 38.89458608627319,
      "total_new_tokens": 1889,
      "avg_new_tokens_per_prompt": 19.88421052631579,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.4120001792907715,
      "memory_reserved_gb": 2.4375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.4019203186035156,
      "energy_kwh": 0.00011293819662332302,
      "energy_raw_kwh": 0.00046066648006812684,
      "energy_idle_kwh": 0.0003477282834448038,
      "joules_per_token": 0.21523425507885804,
      "joules_per_token_calculation": {
        "total_joules": 406.57750784396285,
        "total_tokens": 1889,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:19:25.794466",
        "project_name": "N/A",
        "duration_sec": 39.67331051826477,
        "energy_kwh": 0.00011293819662332302,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.4120001792907715,
        "gpu_memory_reserved_gb": 2.4375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 75,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00046066648006812684,
        "energy_idle_contribution_kwh": 0.0003477282834448038,
        "energy_net_kwh": 0.00011293819662332302,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0013191062655026036,
        "codecarbon_cpu_energy_kwh": 0.0004672229355937286,
        "codecarbon_gpu_energy_kwh": 0.0006320741167699928
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Latency, TTFT, bsz=1)"
    },
    "mmlu_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 972.8530532435367,
      "std_ttft_ms": 17.341613808236534,
      "throughput_tokens_per_sec": 51.395223392985926,
      "total_loop_time_sec": 97.41596150398254,
      "total_new_tokens": 4750,
      "avg_new_tokens_per_prompt": 50.0,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.4120001792907715,
      "memory_reserved_gb": 2.435546875,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.4019203186035156,
      "energy_kwh": 0.0002712114944274091,
      "energy_raw_kwh": 0.001132507085481983,
      "energy_idle_kwh": 0.000861295591054574,
      "joules_per_token": 0.20554976419761534,
      "joules_per_token_calculation": {
        "total_joules": 976.3613799386728,
        "total_tokens": 4750,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:21:05.730109",
        "project_name": "N/A",
        "duration_sec": 98.26766777038574,
        "energy_kwh": 0.0002712114944274091,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.4120001792907715,
        "gpu_memory_reserved_gb": 2.435546875,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 75,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.001132507085481983,
        "energy_idle_contribution_kwh": 0.000861295591054574,
        "energy_net_kwh": 0.0002712114944274091,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0032429040462515265,
        "codecarbon_cpu_energy_kwh": 0.0011573468206437572,
        "codecarbon_gpu_energy_kwh": 0.0015410734550799915
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Latency, TTFT, bsz=1)"
    },
    "ifeval_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 2885.792942047119,
      "std_ttft_ms": 21.487523731785718,
      "throughput_tokens_per_sec": 51.97878122662301,
      "total_loop_time_sec": 83.78992772102356,
      "total_new_tokens": 3750,
      "avg_new_tokens_per_prompt": 150.0,
      "num_measured_prompts": 25,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.4120001792907715,
      "memory_reserved_gb": 2.431640625,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.4019203186035156,
      "energy_kwh": 0.00023556090930223442,
      "energy_raw_kwh": 0.0009778059487582172,
      "energy_idle_kwh": 0.0007422450394559827,
      "joules_per_token": 0.22613847293014505,
      "joules_per_token_calculation": {
        "total_joules": 848.019273488044,
        "total_tokens": 3750,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:22:32.086023",
        "project_name": "N/A",
        "duration_sec": 84.68485116958618,
        "energy_kwh": 0.00023556090930223442,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.4120001792907715,
        "gpu_memory_reserved_gb": 2.431640625,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 75,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0009778059487582172,
        "energy_idle_contribution_kwh": 0.0007422450394559827,
        "energy_net_kwh": 0.00023556090930223442,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.002799921438308106,
        "codecarbon_cpu_energy_kwh": 0.0009974625180097037,
        "codecarbon_gpu_energy_kwh": 0.0013331935665539946
      },
      "batch_size": 1,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Latency, TTFT, bsz=1)"
    },
    "hellaswag_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 577.4894627657803,
      "throughput_tokens_per_sec": 264.46763547132673,
      "total_loop_time_sec": 7.576159477233887,
      "total_new_tokens": 1680,
      "avg_new_tokens_per_prompt": 19.09090909090909,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 1.4120001792907715,
      "memory_reserved_gb": 2.458984375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.4019203186035156,
      "energy_kwh": 2.6263283012038097e-05,
      "energy_raw_kwh": 0.00010006501360368115,
      "energy_idle_kwh": 7.380173059164306e-05,
      "joules_per_token": 0.0562784635972245,
      "joules_per_token_calculation": {
        "total_joules": 94.54781884333715,
        "total_tokens": 1680,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:22:42.178940",
        "project_name": "N/A",
        "duration_sec": 8.420249700546265,
        "energy_kwh": 2.6263283012038097e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.4120001792907715,
        "gpu_memory_reserved_gb": 2.458984375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 75,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00010006501360368115,
        "energy_idle_contribution_kwh": 7.380173059164306e-05,
        "energy_net_kwh": 2.6263283012038097e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.00028653351635807853,
        "codecarbon_cpu_energy_kwh": 9.919948709096062e-05,
        "codecarbon_gpu_energy_kwh": 0.00014067427920599895
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Throughput, bsz=8)"
    },
    "mmlu_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 1345.937338742343,
      "throughput_tokens_per_sec": 283.6819893673182,
      "total_loop_time_sec": 17.54533576965332,
      "total_new_tokens": 4200,
      "avg_new_tokens_per_prompt": 47.72727272727273,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 1.4120001792907715,
      "memory_reserved_gb": 2.45703125,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.4019203186035156,
      "energy_kwh": 5.617060893325397e-05,
      "energy_raw_kwh": 0.00021739323410966912,
      "energy_idle_kwh": 0.00016122262517641515,
      "joules_per_token": 0.0481462362285034,
      "joules_per_token_calculation": {
        "total_joules": 202.2141921597143,
        "total_tokens": 4200,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:23:02.215880",
        "project_name": "N/A",
        "duration_sec": 18.394348621368408,
        "energy_kwh": 5.617060893325397e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.4120001792907715,
        "gpu_memory_reserved_gb": 2.45703125,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 77,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00021739323410966912,
        "energy_idle_contribution_kwh": 0.00016122262517641515,
        "energy_net_kwh": 5.617060893325397e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0006224997684865848,
        "codecarbon_cpu_energy_kwh": 0.0002165413282583441,
        "codecarbon_gpu_energy_kwh": 0.00030409107660600565
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Throughput, bsz=8)"
    },
    "ifeval_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 4020.5074151357016,
      "throughput_tokens_per_sec": 273.5973066133177,
      "total_loop_time_sec": 16.005842208862305,
      "total_new_tokens": 3300,
      "avg_new_tokens_per_prompt": 137.5,
      "num_measured_prompts": 24,
      "num_warmup_batches": 1,
      "memory_allocated_gb": 1.4120001792907715,
      "memory_reserved_gb": 2.458984375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.4019203186035156,
      "energy_kwh": 4.77529084648509e-05,
      "energy_raw_kwh": 0.00019589636828969874,
      "energy_idle_kwh": 0.00014814345982484784,
      "joules_per_token": 0.05209408196165553,
      "joules_per_token_calculation": {
        "total_joules": 171.91047047346325,
        "total_tokens": 3300,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:23:20.811527",
        "project_name": "N/A",
        "duration_sec": 16.902109384536743,
        "energy_kwh": 4.77529084648509e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.4120001792907715,
        "gpu_memory_reserved_gb": 2.458984375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00019589636828969874,
        "energy_idle_contribution_kwh": 0.00014814345982484784,
        "energy_net_kwh": 4.77529084648509e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0005609440625285604,
        "codecarbon_cpu_energy_kwh": 0.0001989794928027695,
        "codecarbon_gpu_energy_kwh": 0.00026835882579799575
      },
      "batch_size": 8,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Throughput, bsz=8)"
    }
  },
  "completed_workloads": [
    "hellaswag_latency_b1",
    "mmlu_latency_b1",
    "ifeval_latency_b1",
    "hellaswag_throughput_b8",
    "mmlu_throughput_b8",
    "ifeval_throughput_b8"
  ],
  "failed_workloads": []
}