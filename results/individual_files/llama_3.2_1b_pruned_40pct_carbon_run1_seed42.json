{
  "metadata": {
    "model_name": "Llama-3.2-1B-pruned-40%-run1",
    "started_at": "2025-11-07T12:50:58.832574",
    "last_updated": "2025-11-07T12:55:25.781255",
    "mode": "carbon_profiling"
  },
  "results": {
    "hellaswag_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 394.6392912613718,
      "std_ttft_ms": 8.655580581239247,
      "throughput_tokens_per_sec": 50.679190954541546,
      "total_loop_time_sec": 39.634947061538696,
      "total_new_tokens": 1900,
      "avg_new_tokens_per_prompt": 20.0,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.7099661827087402,
      "memory_reserved_gb": 1.748046875,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.7020301818847656,
      "energy_kwh": 0.0001122078209518039,
      "energy_raw_kwh": 0.00046768206352608047,
      "energy_idle_kwh": 0.0003554742425742766,
      "joules_per_token": 0.2126042923297337,
      "joules_per_token_calculation": {
        "total_joules": 403.94815542649405,
        "total_tokens": 1900,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:51:41.080211",
        "project_name": "N/A",
        "duration_sec": 40.557069063186646,
        "energy_kwh": 0.0001122078209518039,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.7099661827087402,
        "gpu_memory_reserved_gb": 1.748046875,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 66,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00046768206352608047,
        "energy_idle_contribution_kwh": 0.0003554742425742766,
        "energy_net_kwh": 0.0001122078209518039,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0013391952029355475,
        "codecarbon_cpu_energy_kwh": 0.00047772588514444556,
        "codecarbon_gpu_energy_kwh": 0.00063671856493
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Latency, TTFT, bsz=1)"
    },
    "mmlu_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 947.0908089687949,
      "std_ttft_ms": 167.47659121257442,
      "throughput_tokens_per_sec": 51.159435948600574,
      "total_loop_time_sec": 94.90934300422668,
      "total_new_tokens": 4603,
      "avg_new_tokens_per_prompt": 48.45263157894737,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.7099661827087402,
      "memory_reserved_gb": 1.748046875,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.7020301818847656,
      "energy_kwh": 0.00026884963379868296,
      "energy_raw_kwh": 0.0011089448110229373,
      "energy_idle_kwh": 0.0008400951772242544,
      "joules_per_token": 0.21026693062682134,
      "joules_per_token_calculation": {
        "total_joules": 967.8586816752586,
        "total_tokens": 4603,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:53:18.604836",
        "project_name": "N/A",
        "duration_sec": 95.84885215759277,
        "energy_kwh": 0.00026884963379868296,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.7099661827087402,
        "gpu_memory_reserved_gb": 1.748046875,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 74,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0011089448110229373,
        "energy_idle_contribution_kwh": 0.0008400951772242544,
        "energy_net_kwh": 0.00026884963379868296,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0031754340973552606,
        "codecarbon_cpu_energy_kwh": 0.00112902237648058,
        "codecarbon_gpu_energy_kwh": 0.001515261212207998
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Latency, TTFT, bsz=1)"
    },
    "ifeval_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 2520.5520725250244,
      "std_ttft_ms": 868.1855527527584,
      "throughput_tokens_per_sec": 51.861654208575054,
      "total_loop_time_sec": 74.82233142852783,
      "total_new_tokens": 3268,
      "avg_new_tokens_per_prompt": 130.72,
      "num_measured_prompts": 25,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.7099661827087402,
      "memory_reserved_gb": 1.7421875,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.7020301818847656,
      "energy_kwh": 0.00023778758030999954,
      "energy_raw_kwh": 0.0009012334604606405,
      "energy_idle_kwh": 0.000663445880150641,
      "joules_per_token": 0.2619447029118722,
      "joules_per_token_calculation": {
        "total_joules": 856.0352891159984,
        "total_tokens": 3268,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:54:35.966424",
        "project_name": "N/A",
        "duration_sec": 75.6944305896759,
        "energy_kwh": 0.00023778758030999954,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.7099661827087402,
        "gpu_memory_reserved_gb": 1.7421875,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 74,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0009012334604606405,
        "energy_idle_contribution_kwh": 0.000663445880150641,
        "energy_net_kwh": 0.00023778758030999954,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0025806581459940645,
        "codecarbon_cpu_energy_kwh": 0.0008915763417742925,
        "codecarbon_gpu_energy_kwh": 0.0012696229601419987
      },
      "batch_size": 1,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Latency, TTFT, bsz=1)"
    },
    "hellaswag_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 590.1635776866566,
      "throughput_tokens_per_sec": 258.78803521887664,
      "total_loop_time_sec": 7.708029270172119,
      "total_new_tokens": 1680,
      "avg_new_tokens_per_prompt": 19.09090909090909,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 1.7099661827087402,
      "memory_reserved_gb": 1.98828125,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.7020301818847656,
      "energy_kwh": 2.4828811862919814e-05,
      "energy_raw_kwh": 9.910389046332409e-05,
      "energy_idle_kwh": 7.427507860040428e-05,
      "joules_per_token": 0.05320459684911389,
      "joules_per_token_calculation": {
        "total_joules": 89.38372270651134,
        "total_tokens": 1680,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:54:46.092587",
        "project_name": "N/A",
        "duration_sec": 8.474255323410034,
        "energy_kwh": 2.4828811862919814e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.7099661827087402,
        "gpu_memory_reserved_gb": 1.98828125,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 73,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 9.910389046332409e-05,
        "energy_idle_contribution_kwh": 7.427507860040428e-05,
        "energy_net_kwh": 2.4828811862919814e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0002837813656998039,
        "codecarbon_cpu_energy_kwh": 9.982872006180609e-05,
        "codecarbon_gpu_energy_kwh": 0.00013699205403799875
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Throughput, bsz=8)"
    },
    "mmlu_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 1377.80347737399,
      "throughput_tokens_per_sec": 277.1209305886672,
      "total_loop_time_sec": 17.963413953781128,
      "total_new_tokens": 4200,
      "avg_new_tokens_per_prompt": 47.72727272727273,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 1.7099661827087402,
      "memory_reserved_gb": 2.0703125,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.7020301818847656,
      "energy_kwh": 5.4097131729059675e-05,
      "energy_raw_kwh": 0.00021908655936870333,
      "energy_idle_kwh": 0.00016498942763964366,
      "joules_per_token": 0.04636897005347972,
      "joules_per_token_calculation": {
        "total_joules": 194.74967422461484,
        "total_tokens": 4200,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:55:06.588287",
        "project_name": "N/A",
        "duration_sec": 18.824113845825195,
        "energy_kwh": 5.4097131729059675e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.7099661827087402,
        "gpu_memory_reserved_gb": 2.0703125,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00021908655936870333,
        "energy_idle_contribution_kwh": 0.00016498942763964366,
        "energy_net_kwh": 5.4097131729059675e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.000627348560520239,
        "codecarbon_cpu_energy_kwh": 0.00022169415892224026,
        "codecarbon_gpu_energy_kwh": 0.00030137968554799777
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Throughput, bsz=8)"
    },
    "ifeval_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 4039.906104405721,
      "throughput_tokens_per_sec": 272.28355599660955,
      "total_loop_time_sec": 16.203492403030396,
      "total_new_tokens": 3300,
      "avg_new_tokens_per_prompt": 137.5,
      "num_measured_prompts": 24,
      "num_warmup_batches": 1,
      "memory_allocated_gb": 1.7099661827087402,
      "memory_reserved_gb": 1.849609375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.7020301818847656,
      "energy_kwh": 4.7223932108022025e-05,
      "energy_raw_kwh": 0.00020071950566164504,
      "energy_idle_kwh": 0.00015349557355362302,
      "joules_per_token": 0.051517016845114935,
      "joules_per_token_calculation": {
        "total_joules": 170.0061555888793,
        "total_tokens": 3300,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:55:25.779595",
        "project_name": "N/A",
        "duration_sec": 17.512747287750244,
        "energy_kwh": 4.7223932108022025e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.7099661827087402,
        "gpu_memory_reserved_gb": 1.849609375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 77,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00020071950566164504,
        "energy_idle_contribution_kwh": 0.00015349557355362302,
        "energy_net_kwh": 4.7223932108022025e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0005747549886584001,
        "codecarbon_cpu_energy_kwh": 0.0002061819102270707,
        "codecarbon_gpu_energy_kwh": 0.000271587995048006
      },
      "batch_size": 8,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Throughput, bsz=8)"
    }
  },
  "completed_workloads": [
    "hellaswag_latency_b1",
    "mmlu_latency_b1",
    "ifeval_latency_b1",
    "hellaswag_throughput_b8",
    "mmlu_throughput_b8",
    "ifeval_throughput_b8"
  ],
  "failed_workloads": []
}