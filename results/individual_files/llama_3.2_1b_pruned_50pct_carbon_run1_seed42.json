{
  "metadata": {
    "model_name": "Llama-3.2-1B-pruned-50%-run1",
    "started_at": "2025-11-07T13:04:37.825509",
    "last_updated": "2025-11-07T13:09:13.837386",
    "mode": "carbon_profiling"
  },
  "results": {
    "hellaswag_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 391.75812068738435,
      "std_ttft_ms": 7.406555038560651,
      "throughput_tokens_per_sec": 51.051909185463,
      "total_loop_time_sec": 39.31636571884155,
      "total_new_tokens": 1900,
      "avg_new_tokens_per_prompt": 20.0,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.5607962608337402,
      "memory_reserved_gb": 2.39453125,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.5518836975097656,
      "energy_kwh": 0.00012228699857529443,
      "energy_raw_kwh": 0.0004739092148971121,
      "energy_idle_kwh": 0.00035162221632181767,
      "joules_per_token": 0.2317016815110842,
      "joules_per_token_calculation": {
        "total_joules": 440.23319487106,
        "total_tokens": 1900,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:05:19.650546",
        "project_name": "N/A",
        "duration_sec": 40.11758041381836,
        "energy_kwh": 0.00012228699857529443,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.5607962608337402,
        "gpu_memory_reserved_gb": 2.39453125,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0004739092148971121,
        "energy_idle_contribution_kwh": 0.00035162221632181767,
        "energy_net_kwh": 0.00012228699857529443,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0013570264859682226,
        "codecarbon_cpu_energy_kwh": 0.000472533164393764,
        "codecarbon_gpu_energy_kwh": 0.0006621908075300081
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Latency, TTFT, bsz=1)"
    },
    "mmlu_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 959.4279389632376,
      "std_ttft_ms": 97.4739482974678,
      "throughput_tokens_per_sec": 51.576787079797434,
      "total_loop_time_sec": 96.0873053073883,
      "total_new_tokens": 4701,
      "avg_new_tokens_per_prompt": 49.48421052631579,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.5607962608337402,
      "memory_reserved_gb": 2.392578125,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.5518836975097656,
      "energy_kwh": 0.0002848290608434168,
      "energy_raw_kwh": 0.0011347644919391159,
      "energy_idle_kwh": 0.0008499354310956991,
      "joules_per_token": 0.21812053159674544,
      "joules_per_token_calculation": {
        "total_joules": 1025.3846190363004,
        "total_tokens": 4701,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:06:58.280297",
        "project_name": "N/A",
        "duration_sec": 96.97155475616455,
        "energy_kwh": 0.0002848290608434168,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.5607962608337402,
        "gpu_memory_reserved_gb": 2.392578125,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0011347644919391159,
        "energy_idle_contribution_kwh": 0.0008499354310956991,
        "energy_net_kwh": 0.0002848290608434168,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0032493680698569543,
        "codecarbon_cpu_energy_kwh": 0.0011422562238576287,
        "codecarbon_gpu_energy_kwh": 0.001569736533565999
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Latency, TTFT, bsz=1)"
    },
    "ifeval_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 2884.547882080078,
      "std_ttft_ms": 25.5588547566266,
      "throughput_tokens_per_sec": 52.00121687417905,
      "total_loop_time_sec": 83.81091141700745,
      "total_new_tokens": 3750,
      "avg_new_tokens_per_prompt": 150.0,
      "num_measured_prompts": 25,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.5607962608337402,
      "memory_reserved_gb": 2.388671875,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.5518836975097656,
      "energy_kwh": 0.0002507746951299918,
      "energy_raw_kwh": 0.0009931476089290678,
      "energy_idle_kwh": 0.0007423729137990759,
      "joules_per_token": 0.24074370732479214,
      "joules_per_token_calculation": {
        "total_joules": 902.7889024679705,
        "total_tokens": 3750,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:08:24.648621",
        "project_name": "N/A",
        "duration_sec": 84.69944071769714,
        "energy_kwh": 0.0002507746951299918,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.5607962608337402,
        "gpu_memory_reserved_gb": 2.388671875,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 75,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0009931476089290678,
        "energy_idle_contribution_kwh": 0.0007423729137990759,
        "energy_net_kwh": 0.0002507746951299918,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.002843851875902758,
        "codecarbon_cpu_energy_kwh": 0.0009976910850756587,
        "codecarbon_gpu_energy_kwh": 0.001376791934766003
      },
      "batch_size": 1,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Latency, TTFT, bsz=1)"
    },
    "hellaswag_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 577.6517174460671,
      "throughput_tokens_per_sec": 264.3933500319459,
      "total_loop_time_sec": 7.558628797531128,
      "total_new_tokens": 1680,
      "avg_new_tokens_per_prompt": 19.09090909090909,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 1.5607962608337402,
      "memory_reserved_gb": 2.416015625,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.5518836975097656,
      "energy_kwh": 2.336429882880678e-05,
      "energy_raw_kwh": 9.780367296305778e-05,
      "energy_idle_kwh": 7.4439374134251e-05,
      "joules_per_token": 0.05006635463315739,
      "joules_per_token_calculation": {
        "total_joules": 84.1114757837044,
        "total_tokens": 1680,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:08:34.824915",
        "project_name": "N/A",
        "duration_sec": 8.493000268936157,
        "energy_kwh": 2.336429882880678e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.5607962608337402,
        "gpu_memory_reserved_gb": 2.416015625,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 75,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 9.780367296305778e-05,
        "energy_idle_contribution_kwh": 7.4439374134251e-05,
        "energy_net_kwh": 2.336429882880678e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.00028005822732241693,
        "codecarbon_cpu_energy_kwh": 0.00010001527633821183,
        "codecarbon_gpu_energy_kwh": 0.00013299427306198708
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Throughput, bsz=8)"
    },
    "mmlu_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 1358.7856075980446,
      "throughput_tokens_per_sec": 280.99957762514885,
      "total_loop_time_sec": 17.684813022613525,
      "total_new_tokens": 4200,
      "avg_new_tokens_per_prompt": 47.72727272727273,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 1.5607962608337402,
      "memory_reserved_gb": 2.4140625,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.5518836975097656,
      "energy_kwh": 5.077791165250574e-05,
      "energy_raw_kwh": 0.0002157104830320072,
      "energy_idle_kwh": 0.00016493257137950145,
      "joules_per_token": 0.043523924273576355,
      "joules_per_token_calculation": {
        "total_joules": 182.80048194902068,
        "total_tokens": 4200,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:08:55.290660",
        "project_name": "N/A",
        "duration_sec": 18.817626953125,
        "energy_kwh": 5.077791165250574e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.5607962608337402,
        "gpu_memory_reserved_gb": 2.4140625,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0002157104830320072,
        "energy_idle_contribution_kwh": 0.00016493257137950145,
        "energy_net_kwh": 5.077791165250574e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0006176812553412462,
        "codecarbon_cpu_energy_kwh": 0.00022156904803611677,
        "codecarbon_gpu_energy_kwh": 0.0002918885668440063
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Throughput, bsz=8)"
    },
    "ifeval_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 3985.4251543680825,
      "throughput_tokens_per_sec": 276.0056850633324,
      "total_loop_time_sec": 15.900215864181519,
      "total_new_tokens": 3300,
      "avg_new_tokens_per_prompt": 137.5,
      "num_measured_prompts": 24,
      "num_warmup_batches": 1,
      "memory_allocated_gb": 1.5607962608337402,
      "memory_reserved_gb": 2.41796875,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.5518836975097656,
      "energy_kwh": 4.370753410948543e-05,
      "energy_raw_kwh": 0.00019164227996967272,
      "energy_idle_kwh": 0.0001479347458601873,
      "joules_per_token": 0.04768094630125684,
      "joules_per_token_calculation": {
        "total_joules": 157.34712279414757,
        "total_tokens": 3300,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T13:09:13.835769",
        "project_name": "N/A",
        "duration_sec": 16.878296613693237,
        "energy_kwh": 4.370753410948543e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.5607962608337402,
        "gpu_memory_reserved_gb": 2.41796875,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 77,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00019164227996967272,
        "energy_idle_contribution_kwh": 0.0001479347458601873,
        "energy_net_kwh": 4.370753410948543e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0005487625932883459,
        "codecarbon_cpu_energy_kwh": 0.00019885918976456326,
        "codecarbon_gpu_energy_kwh": 0.00025636326064600035
      },
      "batch_size": 8,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Throughput, bsz=8)"
    }
  },
  "completed_workloads": [
    "hellaswag_latency_b1",
    "mmlu_latency_b1",
    "ifeval_latency_b1",
    "hellaswag_throughput_b8",
    "mmlu_throughput_b8",
    "ifeval_throughput_b8"
  ],
  "failed_workloads": []
}