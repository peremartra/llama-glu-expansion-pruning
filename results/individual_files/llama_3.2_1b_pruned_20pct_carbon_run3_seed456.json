{
  "metadata": {
    "model_name": "Llama-3.2-1B-pruned-20%-run3",
    "started_at": "2025-11-07T12:33:26.720660",
    "last_updated": "2025-11-07T12:37:09.438228",
    "mode": "carbon_profiling"
  },
  "results": {
    "hellaswag_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 285.2077609614322,
      "std_ttft_ms": 157.57297001920594,
      "throughput_tokens_per_sec": 50.37878704350748,
      "total_loop_time_sec": 28.170587062835693,
      "total_new_tokens": 1365,
      "avg_new_tokens_per_prompt": 14.368421052631579,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 2.012814998626709,
      "memory_reserved_gb": 2.474609375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 2.0019569396972656,
      "energy_kwh": 9.933961688955289e-05,
      "energy_raw_kwh": 0.00035339678545347913,
      "energy_idle_kwh": 0.00025405716856392624,
      "joules_per_token": 0.2619945939944252,
      "joules_per_token_calculation": {
        "total_joules": 357.6226208023904,
        "total_tokens": 1365,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:33:57.405135",
        "project_name": "N/A",
        "duration_sec": 28.986106157302856,
        "energy_kwh": 9.933961688955289e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.012814998626709,
        "gpu_memory_reserved_gb": 2.474609375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 79,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00035339678545347913,
        "energy_idle_contribution_kwh": 0.00025405716856392624,
        "energy_net_kwh": 9.933961688955289e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0010119423358765397,
        "codecarbon_cpu_energy_kwh": 0.0003413475700132129,
        "codecarbon_gpu_energy_kwh": 0.0005100101302299986
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Latency, TTFT, bsz=1)"
    },
    "mmlu_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 764.3128294693796,
      "std_ttft_ms": 370.8070777848209,
      "throughput_tokens_per_sec": 51.05377160641032,
      "total_loop_time_sec": 77.3604519367218,
      "total_new_tokens": 3707,
      "avg_new_tokens_per_prompt": 39.02105263157895,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 2.012814998626709,
      "memory_reserved_gb": 2.48046875,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 2.0019569396972656,
      "energy_kwh": 0.00026706167256632443,
      "energy_raw_kwh": 0.0009526594291455699,
      "energy_idle_kwh": 0.0006855977565792455,
      "joules_per_token": 0.2593531214563712,
      "joules_per_token_calculation": {
        "total_joules": 961.4220212387679,
        "total_tokens": 3707,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:35:17.271347",
        "project_name": "N/A",
        "duration_sec": 78.22180128097534,
        "energy_kwh": 0.00026706167256632443,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.012814998626709,
        "gpu_memory_reserved_gb": 2.48046875,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 77,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0009526594291455699,
        "energy_idle_contribution_kwh": 0.0006855977565792455,
        "energy_net_kwh": 0.00026706167256632443,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.002727915045371243,
        "codecarbon_cpu_energy_kwh": 0.000921422258430572,
        "codecarbon_gpu_energy_kwh": 0.001372996376173996
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Latency, TTFT, bsz=1)"
    },
    "ifeval_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 1812.3510837554932,
      "std_ttft_ms": 1330.926170970978,
      "throughput_tokens_per_sec": 51.88840111769816,
      "total_loop_time_sec": 59.92562198638916,
      "total_new_tokens": 2351,
      "avg_new_tokens_per_prompt": 94.04,
      "num_measured_prompts": 25,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 2.012814998626709,
      "memory_reserved_gb": 2.466796875,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 2.0019569396972656,
      "energy_kwh": 0.00020951851308727352,
      "energy_raw_kwh": 0.0007426881489925487,
      "energy_idle_kwh": 0.0005331696359052752,
      "joules_per_token": 0.32082800813023593,
      "joules_per_token_calculation": {
        "total_joules": 754.2666471141847,
        "total_tokens": 2351,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:36:19.775384",
        "project_name": "N/A",
        "duration_sec": 60.83084273338318,
        "energy_kwh": 0.00020951851308727352,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.012814998626709,
        "gpu_memory_reserved_gb": 2.466796875,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 75,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0007426881489925487,
        "energy_idle_contribution_kwh": 0.0005331696359052752,
        "energy_net_kwh": 0.00020951851308727352,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0021266678454784013,
        "codecarbon_cpu_energy_kwh": 0.0007164447659152858,
        "codecarbon_gpu_energy_kwh": 0.0010731700252019985
      },
      "batch_size": 1,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Latency, TTFT, bsz=1)"
    },
    "hellaswag_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 601.4619523828679,
      "throughput_tokens_per_sec": 253.9267398747316,
      "total_loop_time_sec": 7.902717113494873,
      "total_new_tokens": 1680,
      "avg_new_tokens_per_prompt": 19.09090909090909,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 2.012814998626709,
      "memory_reserved_gb": 2.49609375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 2.0019569396972656,
      "energy_kwh": 2.8144833243374682e-05,
      "energy_raw_kwh": 0.00010415363073719391,
      "energy_idle_kwh": 7.600879749381923e-05,
      "joules_per_token": 0.06031035695008861,
      "joules_per_token_calculation": {
        "total_joules": 101.32139967614886,
        "total_tokens": 1680,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:36:30.110919",
        "project_name": "N/A",
        "duration_sec": 8.672060251235962,
        "energy_kwh": 2.8144833243374682e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.012814998626709,
        "gpu_memory_reserved_gb": 2.49609375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 75,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00010415363073719391,
        "energy_idle_contribution_kwh": 7.600879749381923e-05,
        "energy_net_kwh": 2.8144833243374682e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0002982411632380086,
        "codecarbon_cpu_energy_kwh": 0.00010200505381667365,
        "codecarbon_gpu_energy_kwh": 0.00014826289638800172
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Throughput, bsz=8)"
    },
    "mmlu_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 1386.698159304532,
      "throughput_tokens_per_sec": 275.34339701559446,
      "total_loop_time_sec": 18.03326940536499,
      "total_new_tokens": 4200,
      "avg_new_tokens_per_prompt": 47.72727272727273,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 2.012814998626709,
      "memory_reserved_gb": 2.4921875,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 2.0019569396972656,
      "energy_kwh": 6.178667481921305e-05,
      "energy_raw_kwh": 0.00022806128213211704,
      "energy_idle_kwh": 0.000166274607312904,
      "joules_per_token": 0.0529600069878969,
      "joules_per_token_calculation": {
        "total_joules": 222.43202934916698,
        "total_tokens": 4200,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:36:50.739600",
        "project_name": "N/A",
        "duration_sec": 18.970743656158447,
        "energy_kwh": 6.178667481921305e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.012814998626709,
        "gpu_memory_reserved_gb": 2.4921875,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 77,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00022806128213211704,
        "energy_idle_contribution_kwh": 0.000166274607312904,
        "energy_net_kwh": 6.178667481921305e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0006530474414690266,
        "codecarbon_cpu_energy_kwh": 0.0002233464549034731,
        "codecarbon_gpu_energy_kwh": 0.0003246377597099989
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Throughput, bsz=8)"
    },
    "ifeval_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 4059.5812797546387,
      "throughput_tokens_per_sec": 270.9639059293534,
      "total_loop_time_sec": 16.166686058044434,
      "total_new_tokens": 3300,
      "avg_new_tokens_per_prompt": 137.5,
      "num_measured_prompts": 24,
      "num_warmup_batches": 1,
      "memory_allocated_gb": 2.012814998626709,
      "memory_reserved_gb": 2.494140625,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 2.0019569396972656,
      "energy_kwh": 5.138326184673256e-05,
      "energy_raw_kwh": 0.00020066695215237878,
      "energy_idle_kwh": 0.00014928369030564622,
      "joules_per_token": 0.056054467469162796,
      "joules_per_token_calculation": {
        "total_joules": 184.97974264823722,
        "total_tokens": 3300,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:37:09.436613",
        "project_name": "N/A",
        "duration_sec": 17.032201528549194,
        "energy_kwh": 5.138326184673256e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.012814998626709,
        "gpu_memory_reserved_gb": 2.494140625,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 77,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00020066695215237878,
        "energy_idle_contribution_kwh": 0.00014928369030564622,
        "energy_net_kwh": 5.138326184673256e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0005746045030764297,
        "codecarbon_cpu_energy_kwh": 0.00020048013016665513,
        "codecarbon_gpu_energy_kwh": 0.0002798180016320012
      },
      "batch_size": 8,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Throughput, bsz=8)"
    }
  },
  "completed_workloads": [
    "hellaswag_latency_b1",
    "mmlu_latency_b1",
    "ifeval_latency_b1",
    "hellaswag_throughput_b8",
    "mmlu_throughput_b8",
    "ifeval_throughput_b8"
  ],
  "failed_workloads": []
}