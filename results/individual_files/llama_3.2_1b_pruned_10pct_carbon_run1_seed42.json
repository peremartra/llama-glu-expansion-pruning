{
  "metadata": {
    "model_name": "Llama-3.2-1B-pruned-10%-run1",
    "started_at": "2025-11-07T12:14:25.076087",
    "last_updated": "2025-11-07T12:17:58.906847",
    "mode": "carbon_profiling"
  },
  "results": {
    "hellaswag_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 194.17754474439118,
      "std_ttft_ms": 165.10525135134398,
      "throughput_tokens_per_sec": 49.81875851432991,
      "total_loop_time_sec": 19.557950735092163,
      "total_new_tokens": 919,
      "avg_new_tokens_per_prompt": 9.673684210526316,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 2.1616110801696777,
      "memory_reserved_gb": 2.490234375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 2.1519203186035156,
      "energy_kwh": 7.370480468278765e-05,
      "energy_raw_kwh": 0.00025396496535236285,
      "energy_idle_kwh": 0.0001802601606695752,
      "joules_per_token": 0.28872393564530524,
      "joules_per_token_calculation": {
        "total_joules": 265.33729685803553,
        "total_tokens": 919,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:14:47.352848",
        "project_name": "N/A",
        "duration_sec": 20.56639528274536,
        "energy_kwh": 7.370480468278765e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.1616110801696777,
        "gpu_memory_reserved_gb": 2.490234375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 80,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00025396496535236285,
        "energy_idle_contribution_kwh": 0.0001802601606695752,
        "energy_net_kwh": 7.370480468278765e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.000727221952343722,
        "codecarbon_cpu_energy_kwh": 0.0002422299392208351,
        "codecarbon_gpu_energy_kwh": 0.00037103585238400065
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Latency, TTFT, bsz=1)"
    },
    "mmlu_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 816.057162535818,
      "std_ttft_ms": 332.2840349082657,
      "throughput_tokens_per_sec": 51.36379091807121,
      "total_loop_time_sec": 80.57960343360901,
      "total_new_tokens": 3982,
      "avg_new_tokens_per_prompt": 41.915789473684214,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 2.1616110801696777,
      "memory_reserved_gb": 2.48828125,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 2.1519203186035156,
      "energy_kwh": 0.0002948994673613721,
      "energy_raw_kwh": 0.0010099632628673473,
      "energy_idle_kwh": 0.0007150637955059752,
      "joules_per_token": 0.26660926230561016,
      "joules_per_token_calculation": {
        "total_joules": 1061.6380825009396,
        "total_tokens": 3982,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:16:10.571264",
        "project_name": "N/A",
        "duration_sec": 81.58366560935974,
        "energy_kwh": 0.0002948994673613721,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.1616110801696777,
        "gpu_memory_reserved_gb": 2.48828125,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 78,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0010099632628673473,
        "energy_idle_contribution_kwh": 0.0007150637955059752,
        "energy_net_kwh": 0.0002948994673613721,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.002892003055613571,
        "codecarbon_cpu_energy_kwh": 0.0009609194597944577,
        "codecarbon_gpu_energy_kwh": 0.001479018127657999
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Latency, TTFT, bsz=1)"
    },
    "ifeval_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 2051.6518211364746,
      "std_ttft_ms": 1211.1867011672305,
      "throughput_tokens_per_sec": 51.0612955476871,
      "total_loop_time_sec": 55.37626361846924,
      "total_new_tokens": 2619,
      "avg_new_tokens_per_prompt": 104.76,
      "num_measured_prompts": 25,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 2.1616110801696777,
      "memory_reserved_gb": 2.484375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 2.1519203186035156,
      "energy_kwh": 0.00020231721680779918,
      "energy_raw_kwh": 0.0006954858177082179,
      "energy_idle_kwh": 0.0004931686009004187,
      "joules_per_token": 0.27809926708975835,
      "joules_per_token_calculation": {
        "total_joules": 728.3419805080771,
        "total_tokens": 2619,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:17:08.509687",
        "project_name": "N/A",
        "duration_sec": 56.26701068878174,
        "energy_kwh": 0.00020231721680779918,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.1616110801696777,
        "gpu_memory_reserved_gb": 2.484375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0006954858177082179,
        "energy_idle_contribution_kwh": 0.0004931686009004187,
        "energy_net_kwh": 0.00020231721680779918,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0019915052199401114,
        "codecarbon_cpu_energy_kwh": 0.0006626848680250028,
        "codecarbon_gpu_energy_kwh": 0.0010170474803039998
      },
      "batch_size": 1,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Latency, TTFT, bsz=1)"
    },
    "hellaswag_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 636.8891325863925,
      "throughput_tokens_per_sec": 239.80197637703552,
      "total_loop_time_sec": 8.349255084991455,
      "total_new_tokens": 1680,
      "avg_new_tokens_per_prompt": 19.09090909090909,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 2.1616110801696777,
      "memory_reserved_gb": 2.51171875,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 2.1519203186035156,
      "energy_kwh": 3.485552792315524e-05,
      "energy_raw_kwh": 0.00011603380450064856,
      "energy_idle_kwh": 8.117827657749332e-05,
      "joules_per_token": 0.0746904169781898,
      "joules_per_token_calculation": {
        "total_joules": 125.47990052335886,
        "total_tokens": 1680,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:17:19.420276",
        "project_name": "N/A",
        "duration_sec": 9.261860847473145,
        "energy_kwh": 3.485552792315524e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.1616110801696777,
        "gpu_memory_reserved_gb": 2.51171875,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00011603380450064856,
        "energy_idle_contribution_kwh": 8.117827657749332e-05,
        "energy_net_kwh": 3.485552792315524e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0003322597261781971,
        "codecarbon_cpu_energy_kwh": 0.00010910999564930504,
        "codecarbon_gpu_energy_kwh": 0.00017183485969000066
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Throughput, bsz=8)"
    },
    "mmlu_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 1390.6500989740546,
      "throughput_tokens_per_sec": 274.56092808670303,
      "total_loop_time_sec": 18.20854616165161,
      "total_new_tokens": 4200,
      "avg_new_tokens_per_prompt": 47.72727272727273,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 2.1616110801696777,
      "memory_reserved_gb": 2.509765625,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 2.1519203186035156,
      "energy_kwh": 7.448320068584533e-05,
      "energy_raw_kwh": 0.00024282925408045979,
      "energy_idle_kwh": 0.00016834605339461446,
      "joules_per_token": 0.06384274344501029,
      "joules_per_token_calculation": {
        "total_joules": 268.1395224690432,
        "total_tokens": 4200,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:17:40.295760",
        "project_name": "N/A",
        "duration_sec": 19.207080841064453,
        "energy_kwh": 7.448320068584533e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.1616110801696777,
        "gpu_memory_reserved_gb": 2.509765625,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 77,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00024282925408045979,
        "energy_idle_contribution_kwh": 0.00016834605339461446,
        "energy_net_kwh": 7.448320068584533e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0006953351380319385,
        "codecarbon_cpu_energy_kwh": 0.0002261131227486048,
        "codecarbon_gpu_energy_kwh": 0.000362868345850002
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Throughput, bsz=8)"
    },
    "ifeval_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 4012.903610865275,
      "throughput_tokens_per_sec": 274.1157293241874,
      "total_loop_time_sec": 16.09793782234192,
      "total_new_tokens": 3300,
      "avg_new_tokens_per_prompt": 137.5,
      "num_measured_prompts": 24,
      "num_warmup_batches": 1,
      "memory_allocated_gb": 2.1616110801696777,
      "memory_reserved_gb": 2.51171875,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 2.1519203186035156,
      "energy_kwh": 6.311657152102638e-05,
      "energy_raw_kwh": 0.00021181862822013464,
      "energy_idle_kwh": 0.00014870205669910826,
      "joules_per_token": 0.0688544416593015,
      "joules_per_token_calculation": {
        "total_joules": 227.219657475695,
        "total_tokens": 3300,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:17:58.905087",
        "project_name": "N/A",
        "duration_sec": 16.96584129333496,
        "energy_kwh": 6.311657152102638e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 2.1616110801696777,
        "gpu_memory_reserved_gb": 2.51171875,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00021181862822013464,
        "energy_idle_contribution_kwh": 0.00014870205669910826,
        "energy_net_kwh": 6.311657152102638e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0006065370321583302,
        "codecarbon_cpu_energy_kwh": 0.00019962283060277712,
        "codecarbon_gpu_energy_kwh": 0.0003130210837500001
      },
      "batch_size": 8,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Throughput, bsz=8)"
    }
  },
  "completed_workloads": [
    "hellaswag_latency_b1",
    "mmlu_latency_b1",
    "ifeval_latency_b1",
    "hellaswag_throughput_b8",
    "mmlu_throughput_b8",
    "ifeval_throughput_b8"
  ],
  "failed_workloads": []
}