{
  "metadata": {
    "model_name": "Llama-3.2-1B-pruned-30%-run2",
    "started_at": "2025-11-07T12:41:51.714986",
    "last_updated": "2025-11-07T12:46:22.428200",
    "mode": "carbon_profiling"
  },
  "results": {
    "hellaswag_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 396.14219665527344,
      "std_ttft_ms": 9.49458463365364,
      "throughput_tokens_per_sec": 50.486921536925244,
      "total_loop_time_sec": 39.77420520782471,
      "total_new_tokens": 1900,
      "avg_new_tokens_per_prompt": 20.0,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.8609061241149902,
      "memory_reserved_gb": 2.505859375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.8519935607910156,
      "energy_kwh": 0.00013359815101337077,
      "energy_raw_kwh": 0.0004891113516609284,
      "energy_idle_kwh": 0.00035551320064755766,
      "joules_per_token": 0.2531333387621762,
      "joules_per_token_calculation": {
        "total_joules": 480.95334364813476,
        "total_tokens": 1900,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:42:33.983105",
        "project_name": "N/A",
        "duration_sec": 40.561513900756836,
        "energy_kwh": 0.00013359815101337077,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.8609061241149902,
        "gpu_memory_reserved_gb": 2.505859375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 78,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0004891113516609284,
        "energy_idle_contribution_kwh": 0.00035551320064755766,
        "energy_net_kwh": 0.00013359815101337077,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0014005574019819338,
        "codecarbon_cpu_energy_kwh": 0.00047774190119860826,
        "codecarbon_gpu_energy_kwh": 0.0006980769473500029
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Latency, TTFT, bsz=1)"
    },
    "mmlu_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 923.5881956000077,
      "std_ttft_ms": 213.47506767626297,
      "throughput_tokens_per_sec": 51.218999315125444,
      "total_loop_time_sec": 91.86077904701233,
      "total_new_tokens": 4494,
      "avg_new_tokens_per_prompt": 47.305263157894736,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.8609061241149902,
      "memory_reserved_gb": 2.515625,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.8519935607910156,
      "energy_kwh": 0.00030504490078558737,
      "energy_raw_kwh": 0.0011180592498885836,
      "energy_idle_kwh": 0.0008130143491029962,
      "joules_per_token": 0.24436173627683902,
      "joules_per_token_calculation": {
        "total_joules": 1098.1616428281145,
        "total_tokens": 4494,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:44:08.400560",
        "project_name": "N/A",
        "duration_sec": 92.7591233253479,
        "energy_kwh": 0.00030504490078558737,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.8609061241149902,
        "gpu_memory_reserved_gb": 2.515625,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 76,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0011180592498885836,
        "energy_idle_contribution_kwh": 0.0008130143491029962,
        "energy_net_kwh": 0.00030504490078558737,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.003201533051662586,
        "codecarbon_cpu_energy_kwh": 0.0010925400063763707,
        "codecarbon_gpu_energy_kwh": 0.001595025720463994
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Latency, TTFT, bsz=1)"
    },
    "ifeval_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 2739.192657470703,
      "std_ttft_ms": 645.4336222594725,
      "throughput_tokens_per_sec": 51.2851842008497,
      "total_loop_time_sec": 81.4323616027832,
      "total_new_tokens": 3512,
      "avg_new_tokens_per_prompt": 140.48,
      "num_measured_prompts": 25,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.8609061241149902,
      "memory_reserved_gb": 2.5,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.8519935607910156,
      "energy_kwh": 0.0002699917864104188,
      "energy_raw_kwh": 0.000991863070044513,
      "energy_idle_kwh": 0.0007218712836340943,
      "joules_per_token": 0.2767569564571491,
      "joules_per_token_calculation": {
        "total_joules": 971.9704310775077,
        "total_tokens": 3512,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:45:32.430872",
        "project_name": "N/A",
        "duration_sec": 82.36035132408142,
        "energy_kwh": 0.0002699917864104188,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.8609061241149902,
        "gpu_memory_reserved_gb": 2.5,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 74,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.000991863070044513,
        "energy_idle_contribution_kwh": 0.0007218712836340943,
        "energy_net_kwh": 0.0002699917864104188,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0028401736328262325,
        "codecarbon_cpu_energy_kwh": 0.000970128197656915,
        "codecarbon_gpu_energy_kwh": 0.0014136433531359918
      },
      "batch_size": 1,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Latency, TTFT, bsz=1)"
    },
    "hellaswag_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 609.5275011929599,
      "throughput_tokens_per_sec": 250.566664224923,
      "total_loop_time_sec": 7.969801664352417,
      "total_new_tokens": 1680,
      "avg_new_tokens_per_prompt": 19.09090909090909,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 1.8609061241149902,
      "memory_reserved_gb": 2.525390625,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.8519935607910156,
      "energy_kwh": 3.0843144179984374e-05,
      "energy_raw_kwh": 0.00010765288396857316,
      "energy_idle_kwh": 7.680973978858879e-05,
      "joules_per_token": 0.06609245181425223,
      "joules_per_token_calculation": {
        "total_joules": 111.03531904794374,
        "total_tokens": 1680,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:45:42.856053",
        "project_name": "N/A",
        "duration_sec": 8.763442039489746,
        "energy_kwh": 3.0843144179984374e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.8609061241149902,
        "gpu_memory_reserved_gb": 2.525390625,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 74,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00010765288396857316,
        "energy_idle_contribution_kwh": 7.680973978858879e-05,
        "energy_net_kwh": 3.0843144179984374e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.00030826118219274125,
        "codecarbon_cpu_energy_kwh": 0.00010322154608541137,
        "codecarbon_gpu_energy_kwh": 0.00015648845852399978
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Throughput, bsz=8)"
    },
    "mmlu_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 1404.9822850660844,
      "throughput_tokens_per_sec": 271.7601395239106,
      "total_loop_time_sec": 18.502988576889038,
      "total_new_tokens": 4200,
      "avg_new_tokens_per_prompt": 47.72727272727273,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 1.8609061241149902,
      "memory_reserved_gb": 2.52734375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.8519935607910156,
      "energy_kwh": 7.077869181631077e-05,
      "energy_raw_kwh": 0.00024050208644111926,
      "energy_idle_kwh": 0.00016972339462480849,
      "joules_per_token": 0.060667450128266374,
      "joules_per_token_calculation": {
        "total_joules": 254.80329053871878,
        "total_tokens": 4200,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:46:03.876700",
        "project_name": "N/A",
        "duration_sec": 19.36422562599182,
        "energy_kwh": 7.077869181631077e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.8609061241149902,
        "gpu_memory_reserved_gb": 2.52734375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 73,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00024050208644111926,
        "energy_idle_contribution_kwh": 0.00016972339462480849,
        "energy_net_kwh": 7.077869181631077e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0006886713551287956,
        "codecarbon_cpu_energy_kwh": 0.00022794442374791593,
        "codecarbon_gpu_energy_kwh": 0.00035349722724199034
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Throughput, bsz=8)"
    },
    "ifeval_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 3984.3881924947104,
      "throughput_tokens_per_sec": 276.077517264016,
      "total_loop_time_sec": 15.967973470687866,
      "total_new_tokens": 3300,
      "avg_new_tokens_per_prompt": 137.5,
      "num_measured_prompts": 24,
      "num_warmup_batches": 1,
      "memory_allocated_gb": 1.8609061241149902,
      "memory_reserved_gb": 2.52734375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.8519935607910156,
      "energy_kwh": 5.535366012693606e-05,
      "energy_raw_kwh": 0.0002032536732651363,
      "energy_idle_kwh": 0.00014790001313820023,
      "joules_per_token": 0.060385811047566615,
      "joules_per_token_calculation": {
        "total_joules": 199.27317645696982,
        "total_tokens": 3300,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:46:22.426524",
        "project_name": "N/A",
        "duration_sec": 16.87433385848999,
        "energy_kwh": 5.535366012693606e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.8609061241149902,
        "gpu_memory_reserved_gb": 2.52734375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 73,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0002032536732651363,
        "energy_idle_contribution_kwh": 0.00014790001313820023,
        "energy_net_kwh": 5.535366012693606e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0005820115104767547,
        "codecarbon_cpu_energy_kwh": 0.00019870749581319685,
        "codecarbon_gpu_energy_kwh": 0.00028982967630800105
      },
      "batch_size": 8,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Throughput, bsz=8)"
    }
  },
  "completed_workloads": [
    "hellaswag_latency_b1",
    "mmlu_latency_b1",
    "ifeval_latency_b1",
    "hellaswag_throughput_b8",
    "mmlu_throughput_b8",
    "ifeval_throughput_b8"
  ],
  "failed_workloads": []
}