{
  "metadata": {
    "model_name": "Llama-3.2-1B-pruned-30%-run3",
    "started_at": "2025-11-07T12:46:29.116089",
    "last_updated": "2025-11-07T12:50:50.783452",
    "mode": "carbon_profiling"
  },
  "results": {
    "hellaswag_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 395.80268106962507,
      "std_ttft_ms": 10.833363954656546,
      "throughput_tokens_per_sec": 50.423849284970714,
      "total_loop_time_sec": 39.775572299957275,
      "total_new_tokens": 1896,
      "avg_new_tokens_per_prompt": 19.957894736842107,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.8609061241149902,
      "memory_reserved_gb": 2.505859375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.8519935607910156,
      "energy_kwh": 0.00012462737895174697,
      "energy_raw_kwh": 0.0004808741691257471,
      "energy_idle_kwh": 0.00035624679017400013,
      "joules_per_token": 0.23663426383243097,
      "joules_per_token_calculation": {
        "total_joules": 448.6585642262891,
        "total_tokens": 1896,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:47:11.464405",
        "project_name": "N/A",
        "duration_sec": 40.6452112197876,
        "energy_kwh": 0.00012462737895174697,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.8609061241149902,
        "gpu_memory_reserved_gb": 2.505859375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 70,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0004808741691257471,
        "energy_idle_contribution_kwh": 0.00035624679017400013,
        "energy_net_kwh": 0.00012462737895174697,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.001376970447943864,
        "codecarbon_cpu_energy_kwh": 0.00047861954184096647,
        "codecarbon_gpu_energy_kwh": 0.0006731847052140055
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Latency, TTFT, bsz=1)"
    },
    "mmlu_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 906.2059377369128,
      "std_ttft_ms": 268.33566185004986,
      "throughput_tokens_per_sec": 50.575235759130635,
      "total_loop_time_sec": 91.12439560890198,
      "total_new_tokens": 4354,
      "avg_new_tokens_per_prompt": 45.83157894736842,
      "num_measured_prompts": 95,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.8609061241149902,
      "memory_reserved_gb": 2.509765625,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.8519935607910156,
      "energy_kwh": 0.00027746444656975985,
      "energy_raw_kwh": 0.0010836262810143046,
      "energy_idle_kwh": 0.0008061618344445447,
      "joules_per_token": 0.2294147927540504,
      "joules_per_token_calculation": {
        "total_joules": 998.8720076511355,
        "total_tokens": 4354,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:48:45.108946",
        "project_name": "N/A",
        "duration_sec": 91.97730040550232,
        "energy_kwh": 0.00027746444656975985,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.8609061241149902,
        "gpu_memory_reserved_gb": 2.509765625,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 69,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0010836262810143046,
        "energy_idle_contribution_kwh": 0.0008061618344445447,
        "energy_net_kwh": 0.00027746444656975985,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.003102935157204972,
        "codecarbon_cpu_energy_kwh": 0.00108333729566318,
        "codecarbon_gpu_energy_kwh": 0.0015099353746140035
      },
      "batch_size": 1,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Latency, TTFT, bsz=1)"
    },
    "ifeval_latency_b1": {
      "mode": "latency",
      "avg_ttft_ms": 2329.82045173645,
      "std_ttft_ms": 1084.8481123517156,
      "throughput_tokens_per_sec": 51.30009049020063,
      "total_loop_time_sec": 72.97719717025757,
      "total_new_tokens": 2988,
      "avg_new_tokens_per_prompt": 119.52,
      "num_measured_prompts": 25,
      "num_warmup_prompts": 5,
      "memory_allocated_gb": 1.8609061241149902,
      "memory_reserved_gb": 2.5,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.8519935607910156,
      "energy_kwh": 0.0002196282508495084,
      "energy_raw_kwh": 0.0008672098197009712,
      "energy_idle_kwh": 0.0006475815688514628,
      "joules_per_token": 0.2646123504210945,
      "joules_per_token_calculation": {
        "total_joules": 790.6617030582303,
        "total_tokens": 2988,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:50:00.670234",
        "project_name": "N/A",
        "duration_sec": 73.88442611694336,
        "energy_kwh": 0.0002196282508495084,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.8609061241149902,
        "gpu_memory_reserved_gb": 2.5,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 69,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.0008672098197009712,
        "energy_idle_contribution_kwh": 0.0006475815688514628,
        "energy_net_kwh": 0.0002196282508495084,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0024832323517520955,
        "codecarbon_cpu_energy_kwh": 0.0008703388067292074,
        "codecarbon_gpu_energy_kwh": 0.0012034392960839982
      },
      "batch_size": 1,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Latency, TTFT, bsz=1)"
    },
    "hellaswag_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 622.4344860423695,
      "throughput_tokens_per_sec": 245.37084006761876,
      "total_loop_time_sec": 8.17109489440918,
      "total_new_tokens": 1680,
      "avg_new_tokens_per_prompt": 19.09090909090909,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 1.8609061241149902,
      "memory_reserved_gb": 2.52734375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.8519935607910156,
      "energy_kwh": 3.0058038419913114e-05,
      "energy_raw_kwh": 0.00010837134710680426,
      "energy_idle_kwh": 7.831330868689115e-05,
      "joules_per_token": 0.06441008232838524,
      "joules_per_token_calculation": {
        "total_joules": 108.2089383116872,
        "total_tokens": 1680,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:50:11.272505",
        "project_name": "N/A",
        "duration_sec": 8.934988498687744,
        "energy_kwh": 3.0058038419913114e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.8609061241149902,
        "gpu_memory_reserved_gb": 2.52734375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 68,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00010837134710680426,
        "energy_idle_contribution_kwh": 7.831330868689115e-05,
        "energy_net_kwh": 3.0058038419913114e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0003103184823614732,
        "codecarbon_cpu_energy_kwh": 0.00010520193315347798,
        "codecarbon_gpu_energy_kwh": 0.00015562818005799423
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 20,
      "workload_description": "Short responses (Throughput, bsz=8)"
    },
    "mmlu_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 1408.1638292832808,
      "throughput_tokens_per_sec": 271.1461364637646,
      "total_loop_time_sec": 18.32211995124817,
      "total_new_tokens": 4200,
      "avg_new_tokens_per_prompt": 47.72727272727273,
      "num_measured_prompts": 88,
      "num_warmup_batches": 2,
      "memory_allocated_gb": 1.8609061241149902,
      "memory_reserved_gb": 2.5234375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.8519935607910156,
      "energy_kwh": 6.56581887872669e-05,
      "energy_raw_kwh": 0.00023384040428994036,
      "energy_idle_kwh": 0.00016818221550267347,
      "joules_per_token": 0.056278447531943054,
      "joules_per_token_calculation": {
        "total_joules": 236.36947963416083,
        "total_tokens": 4200,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:50:32.134244",
        "project_name": "N/A",
        "duration_sec": 19.188388109207153,
        "energy_kwh": 6.56581887872669e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.8609061241149902,
        "gpu_memory_reserved_gb": 2.5234375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 68,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00023384040428994036,
        "energy_idle_contribution_kwh": 0.00016818221550267347,
        "energy_net_kwh": 6.56581887872669e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0006695958047151701,
        "codecarbon_cpu_energy_kwh": 0.00022580832715139,
        "codecarbon_gpu_energy_kwh": 0.00033757999228600094
      },
      "batch_size": 8,
      "num_prompts": 100,
      "max_new_tokens": 50,
      "workload_description": "Knowledge QA (Throughput, bsz=8)"
    },
    "ifeval_throughput_b8": {
      "mode": "throughput",
      "avg_ttft_ms": null,
      "std_ttft_ms": null,
      "avg_batch_time_ms": 3999.8326301574707,
      "throughput_tokens_per_sec": 275.0115071581617,
      "total_loop_time_sec": 16.083502769470215,
      "total_new_tokens": 3300,
      "avg_new_tokens_per_prompt": 137.5,
      "num_measured_prompts": 24,
      "num_warmup_batches": 1,
      "memory_allocated_gb": 1.8609061241149902,
      "memory_reserved_gb": 2.52734375,
      "max_memory_allocated_gb": 5.055957794189453,
      "model_size_gb": 1.8519935607910156,
      "energy_kwh": 5.257456723829814e-05,
      "energy_raw_kwh": 0.00020135160616051808,
      "energy_idle_kwh": 0.00014877703892221994,
      "joules_per_token": 0.0573540733508707,
      "joules_per_token_calculation": {
        "total_joules": 189.2684420578733,
        "total_tokens": 3300,
        "formula": "total_joules / total_tokens"
      },
      "hardware_metadata": {
        "timestamp": "2025-11-07T12:50:50.781796",
        "project_name": "N/A",
        "duration_sec": 16.974396228790283,
        "energy_kwh": 5.257456723829814e-05,
        "co2_g": "N/A",
        "carbon_intensity_gCO2_kWh": "N/A",
        "country_name": "N/A",
        "country_iso_code": "N/A",
        "region": "N/A",
        "cloud_provider": "N/A",
        "cloud_region": "N/A",
        "os": "N/A",
        "python_version": "N/A",
        "codecarbon_version": "3.0.8",
        "cpu_model": "N/A",
        "cpu_count": 0,
        "cpu_power_usage_W": "N/A",
        "gpu_model": "N/A",
        "gpu_count": 0,
        "gpu_power_usage_W": "N/A",
        "gpu_name_torch": "NVIDIA L4",
        "gpu_total_memory_gb": 22.1610107421875,
        "gpu_compute_capability": "8.9",
        "gpu_memory_allocated_gb": 1.8609061241149902,
        "gpu_memory_reserved_gb": 2.52734375,
        "cuda_version": "12.6",
        "torch_version": "2.8.0+cu126",
        "gpu_temperature_celsius": 68,
        "idle_power_watts": 31.553248368950225,
        "idle_power_applied": true,
        "energy_raw_kwh": 0.00020135160616051808,
        "energy_idle_contribution_kwh": 0.00014877703892221994,
        "energy_net_kwh": 5.257456723829814e-05,
        "idle_correction_note": "Net energy = Raw energy - (Idle power \u00d7 Duration)",
        "codecarbon_overhead_estimated_pct": 2.5,
        "pue_assumption": "unknown (datacenter PUE not measured)",
        "codecarbon_total_energy_internal_kwh": 0.0005765649916965361,
        "codecarbon_cpu_energy_kwh": 0.00019997464842430693,
        "codecarbon_gpu_energy_kwh": 0.0002825141149000038
      },
      "batch_size": 8,
      "num_prompts": 30,
      "max_new_tokens": 150,
      "workload_description": "Instruction (Throughput, bsz=8)"
    }
  },
  "completed_workloads": [
    "hellaswag_latency_b1",
    "mmlu_latency_b1",
    "ifeval_latency_b1",
    "hellaswag_throughput_b8",
    "mmlu_throughput_b8",
    "ifeval_throughput_b8"
  ],
  "failed_workloads": []
}